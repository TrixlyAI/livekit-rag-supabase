Content from https://docs.livekit.io/agents/v0/overview:

On this page
What is LiveKit Agents?
What you can do with agents
How agents connect to LiveKit
How to create an agent
Agents framework features
Copy page
See more page options
Agents 1.0 available for Python
This documentation is for v0.x of the LiveKit Agents framework.
See updated documentation here:
LiveKit Agents 1.0
.
v1.0 for Node.js is coming soon.
What is LiveKit Agents?
LiveKit Agents is a framework for building programmable, multimodal AI agents that orchestrate LLMs and other AI models to accomplish tasks. This framework allows you to build agents using Python or Node.js.
Unlike traditional HTTP servers, agents operate as stateful, long-running processes. They connect to the LiveKit network via WebRTC, enabling low-latency, realtime media and data exchange with frontend applications.
The Agents framework overcomes several key limitations of traditional architectures:
Multimodal
: Agents can exchange voice, video, and text with users.
Simpler frontend
: Frontend applications use LiveKit’s SDKs to handle the complexities of WebRTC transport, media device management, and audio/video encoding and decoding.
Low-latency
: The
LiveKit Cloud
global mesh network connects each user to their nearest edge server, minimizing transport latency.
Centralized business logic
: Keeping business logic within the agent process allows it to support clients across platforms, including telephony integrations.
Stateful
: End-user interactions are inherently stateful. Rather than synchronizing client-side state through request/response cycles, agents provide a more intuitive way to manage these interactions.
What you can do with agents
The LiveKit Agents framework is designed to give you flexibility when building server side, programmable participants. You can create multiple frontends that all connect to the same backend agent.
Some great use cases for agents include:
AI voice agents
: An agent that has natural voice conversations with users.
Call center
: Answer incoming calls, or make outbound calls with AI agents.
Transcription
: Realtime voice-to-text transcription.
Object detection/recognition
: Identify objects over realtime video.
AI-driven avatars
: Generated avatars using prompts.
Translation
: Realtime translation.
Video manipulation
: Realtime video filters and transforms.
How agents connect to LiveKit
When you start running your agent code, it registers itself with a LiveKit server (either
self hosted
or
LiveKit Cloud
and runs as a background "worker" process. The worker waits on standby for users to connect. Once a end-user session is initiated (that is, a room is created for the user), an available worker dispatches an agent to the room.
Users connect to a LiveKit room using a frontend application. Each user is a
participant
in the room and the agent is an AI participant. How the agent interacts with end-user participants depends on the custom code you write.
How to create an agent
To create an agent using the framework, you’ll need to write a Python or Node.js application (your agent) and a frontend for your users:
Write the application code for your agent. The configuration, functions, and plugin options are all part of your agent code. You can use plugins included in the framework for LLM, STT, TTS, VAD, and utilities for working with text, or write your own custom plugins. Define the entrypoint function that executes when a connection is made. You can also define optional functions to preprocess connections and set connection thresholds or permissions for the worker process.
To learn more, see
Integrations
and
Worker options
.
Create a frontend for users to connect to your agent in a LiveKit room. For development and testing, you can use the
Agents Playground
.
Agents framework features
LiveKit audio/video transport
: Use the same
LiveKit API primitives
to transport voice and video from your frontend to your application server in realtime.
Abstractions over common tasks
: Tasks such as speech-to-text, text-to-speech, and using LLMs are simplified so you can focus on your core application logic.
Extensive and extensible plugins
: Prebuilt integrations with OpenAI, Deepgram, Google, ElevenLabs, and more. You can create a plugin to integrate any other provider.
End-to-end dev experience
: Compatible with
LiveKit server
and
LiveKit Cloud
. Develop locally and deploy to production without changing a single line of code.
Orchestration and scaling
: Built-in worker service for agent orchestration and load balancing. To scale, just add more servers.
Open Source
: Like the rest of LiveKit, the Agents framework is Apache 2.0.
Edge optimized
: When using LiveKit Cloud, your agents transmit voice and video over LiveKit's global edge network, ensuring minimal latency for users worldwide.
On this page
What is LiveKit Agents?
What you can do with agents
How agents connect to LiveKit
How to create an agent
Agents framework features


Content from https://docs.livekit.io/home/get-started/intro-to-livekit:

On this page
Why choose LiveKit?
What is WebRTC?
LiveKit ecosystem
Deployment options
What can you build with LiveKit?
Copy page
See more page options
LiveKit is an open source platform for developers building realtime media applications. It makes it easy to integrate audio, video, text, data, and AI models while offering scalable realtime infrastructure built on top of WebRTC.
Why choose LiveKit?
LiveKit provides a complete solution for realtime applications with several key advantages:
Developer-friendly
: Consistent APIs across platforms with comprehensive and well-documented SDKs.
Open source
: No vendor lock-in with complete transparency and flexibility.
AI-native
: First-class support for integrating AI models into realtime experiences.
Scalable
: Can support anywhere from a handful of users to thousands of concurrent participants, or more.
Deployment flexibility
: Choose between fully-managed cloud or self-hosted options.
Private and secure
: End-to-end encryption, HIPAA-compliance, and more.
Built on WebRTC
: The most robust realtime media protocol for peak performance in any network condition.
What is WebRTC?
WebRTC
provides significant advantages over other options for building realtime applications such as websockets.
Optimized for media
: Purpose-built for audio and video with advanced codecs and compression algorithms.
Network resilient
: Performs reliably even in challenging network conditions due to UDP, adaptive bitrate, and more.
Broad compatibility
: Natively supported in all modern browsers.
LiveKit handles all of the complexity of running production-grade WebRTC infrastructure while extending support to mobile apps, backends, and telephony.
LiveKit ecosystem
The LiveKit platform consists of these core components:
LiveKit Server
: An open-source media server that enables realtime communication between participants. Use LiveKit's fully-managed global cloud, or self-host your own.
LiveKit SDKs
: Full-featured web, native, and backend SDKs that make it easy to join rooms and publish and consume realtime media and data.
LiveKit Agents
: A framework for building realtime multimodal AI agents, with an extensive collection of plugins for nearly every AI provider.
Telephony
: A flexible SIP integration for inbound or outbound calling into any LiveKit room or agent session.
Egress
: Record and export realtime media from LiveKit rooms.
Ingress
: Ingest external streams (such as RTMP and WHIP) into LiveKit rooms.
Server APIs
: A REST API for managing rooms, and more. Includes SDKs and a CLI.
Deployment options
LiveKit offers two deployment options for LiveKit Server to fit your needs:
LiveKit Cloud
: A fully-managed, globally distributed service with automatic scaling and high reliability. Trusted by companies of all sizes, from startups to enterprises.
Self-hosted
: Run the open source LiveKit server on your own infrastructure for maximum control and customization.
Both options provide the same core platform features and use the same SDKs.
What can you build with LiveKit?
AI assistants
: Voice and video agents powered by any AI model.
Video conferencing
: Secure, private meetings for teams of any size.
Interactive livestreaming
: Broadcast to audiences with realtime engagement.
Robotics
: Integrate realtime video and powerful AI models into real-world devices.
Healthcare
: HIPAA-compliant telehealth with AI and humans in the loop.
Customer service
: Flexible and observable web, mobile, and telephone support options.
Whatever your use case, LiveKit makes it easy to build innovative, intelligent realtime applications without worrying about scaling media infrastructure.
Get started with LiveKit today
.
On this page
Why choose LiveKit?
What is WebRTC?
LiveKit ecosystem
Deployment options
What can you build with LiveKit?


Content from https://docs.livekit.io/home/get-started/api-primitives:

On this page
Overview
Room
Participant
Participant fields
Types of participants
Track
TrackPublication fields
Track subscription
Copy page
See more page options
Overview
LiveKit has only three core constructs: a room, participant, and track. A room is simply a realtime session
between one or more participants. A participant can publish one or more tracks and/or subscribe to one or more
tracks from another participant.
Room
A
Room
is a container object representing a LiveKit session.
Each participant in a room receives updates about changes to other participants in the same room. For example, when a participant adds, removes, or modifies the state (for example, mute) of a track, other participants are notified of this change. This is a powerful mechanism for synchronizing state and fundamental to building any realtime experience.
A room can be created manually via
server API
, or automatically, when the first participant joins it. Once the last participant leaves a room, it closes after a short delay.
Participant
A
Participant
is a user or process that is participating in a realtime session. They are represented by a unique developer-provided
identity
and a server-generated
sid
. A participant object also contains metadata about its state and tracks they've published.
Important
A participant's identity is unique per room. Thus, if participants with the same identity join a room, only the most recent one to join will remain; the server automatically disconnects other participants using that identity.
There are two kinds of participant objects in the SDKs:
A
LocalParticipant
represents the current user who, by default, can publish tracks in a room.
A
RemoteParticipant
represents a remote user. The local participant, by default, can subscribe to any tracks published by a remote participant.
A participant may also
exchange data
with one or many other participants.
Participant fields
Field
Type
Description
sid
string
A UID for this particular participant, generated by LiveKit server.
identity
string
Unique identity of the participant, as specified when connecting.
name
string
Optional display name.
state
ParticipantInfo.State
JOINING, JOINED, ACTIVE, or DISCONNECTED.
kind
ParticipantInfo.Kind
The type of participant; more below.
attributes
string
User-specified
attributes
for the participant.
permission
ParticipantInfo.Permission
Permissions granted to the participant.
Types of participants
In a realtime session, a participant could represent an end-user, as well as a server-side process. It's possible to distinguish between them with the
kind
field:
STANDARD
: A regular participant, typically an end-user in your application.
AGENT
: An agent spawned with the
Agents framework
.
SIP
: A telephony user connected via
SIP
.
EGRESS
: A server-side process that is recording the session using
LiveKit Egress
.
INGRESS
: A server-side process that is ingesting media into the session using
LiveKit Ingress
.
Track
A
Track
represents a stream of information, be it audio, video or custom data. By default, a participant in a room may publish tracks, such as their camera or microphone streams and subscribe to one or more tracks published by other participants. In order to model a track which may not be subscribed to by the local participant, all track objects have a corresponding
TrackPublication
object:
Track
: a wrapper around the native WebRTC
MediaStreamTrack
, representing a playable track.
TrackPublication
: a track that's been published to the server. If the track is subscribed to by the local participant and available for playback locally, it will have a
.track
attribute representing the associated
Track
object.
We can now list and manipulate tracks (via track publications) published by other participants, even if the local participant is not subscribed to them.
TrackPublication fields
A
TrackPublication
contains information about its associated track:
Field
Type
Description
sid
string
A UID for this particular track, generated by LiveKit server.
kind
Track.Kind
The type of track, whether it be audio, video or arbitrary data.
source
Track.Source
Source of media: Camera, Microphone, ScreenShare, or ScreenShareAudio.
name
string
The name given to this particular track when initially published.
subscribed
boolean
Indicates whether or not this track has been subscribed to by the local participant.
track
Track
If the local participant is subscribed, the associated
Track
object representing a WebRTC track.
muted
boolean
Whether this track is muted or not by the local participant. While muted, it won't receive new bytes from the server.
Track subscription
When a participant is subscribed to a track (which hasn't been muted by the publishing participant), they continuously receive its data. If the participant unsubscribes, they stop receiving media for that track and may resubscribe to it at any time.
When a participant creates or joins a room, the
autoSubscribe
option is set to
true
by default. This means the participant automatically subscribes to all existing tracks being published and any track published in the future. For more fine-grained control over track subscriptions, you can set
autoSubscribe
to
false
and instead use
selective subscriptions
.
Note
For most use cases, muting a track on the publisher side or unsubscribing from it on the subscriber side is typically recommended over unpublishing it. Publishing a track requires a negotiation phase and consequently has worse time-to-first-byte performance.
On this page
Overview
Room
Participant
Participant fields
Types of participants
Track
TrackPublication fields
Track subscription


Content from https://docs.livekit.io/home/get-started/authentication:

On this page
Overview
Creating a token
Token example
Video grant
Example: subscribe-only token
Example: camera-only
SIP grant
Creating a token with SIP grants
Room configuration
Creating a token with room configuration
Token refresh
Updating permissions
Copy page
See more page options
Overview
For a LiveKit SDK to successfully connect to the server, it must pass an access token with the request.
This token encodes the identity of a participant, name of the room, capabilities and permissions. Access tokens are JWT-based and signed with your API secret to prevent forgery.
Access tokens also carry an expiration time, after which the server will reject connections with that token. Note: expiration time only impacts the initial connection, and not subsequent reconnects.
Creating a token
LiveKit CLI
Node.js
Go
Ruby
Java
Python
Rust
Other
lk token create
\
--api-key
<
KEY
>
\
--api-secret
<
SECRET
>
\
--identity
<
NAME
>
\
--room
<
ROOM_NAME
>
\
--join
\
--valid-for 1h
Token example
Here's an example of the decoded body of a join token:
{
"exp"
:
1621657263
,
"iss"
:
"APIMmxiL8rquKztZEoZJV9Fb"
,
"sub"
:
"myidentity"
,
"nbf"
:
1619065263
,
"video"
:
{
"room"
:
"myroom"
,
"roomJoin"
:
true
}
,
"metadata"
:
""
}
field
description
exp
Expiration time of token
nbf
Start time that the token becomes valid
iss
API key used to issue this token
sub
Unique identity for the participant
metadata
Participant metadata
attributes
Participant attributes (key/value pairs of strings)
video
Video grant, including room permissions (see below)
sip
SIP grant
Video grant
Room permissions are specified in the
video
field of a decoded join token. It may contain one or more of the following properties:
field
type
description
roomCreate
bool
Permission to create or delete rooms
roomList
bool
Permission to list available rooms
roomJoin
bool
Permission to join a room
roomAdmin
bool
Permission to moderate a room
roomRecord
bool
Permissions to use Egress service
ingressAdmin
bool
Permissions to use Ingress service
room
string
Name of the room, required if join or admin is set
canPublish
bool
Allow participant to publish tracks
canPublishData
bool
Allow participant to publish data to the room
canPublishSources
string[]
Requires
canPublish
to be true. When set, only listed source can be published. (camera, microphone, screen_share, screen_share_audio)
canSubscribe
bool
Allow participant to subscribe to tracks
canUpdateOwnMetadata
bool
Allow participant to update its own metadata
hidden
bool
Hide participant from others in the room
kind
string
Type of participant (standard, ingress, egress, sip, or agent). this field is typically set by LiveKit internals.
Example: subscribe-only token
To create a token where the participant can only subscribe, and not publish into the room, you would use the following grant:
{
...
"video"
:
{
"room"
:
"myroom"
,
"roomJoin"
:
true
,
"canSubscribe"
:
true
,
"canPublish"
:
false
,
"canPublishData"
:
false
}
}
Example: camera-only
Allow the participant to publish camera, but disallow other sources
{
...
"video"
:
{
"room"
:
"myroom"
,
"roomJoin"
:
true
,
"canSubscribe"
:
true
,
"canPublish"
:
true
,
"canPublishSources"
:
[
"camera"
]
}
}
SIP grant
In order to interact with the SIP service, permission must be granted in the
sip
field
of the JWT. It may contain the following properties:
field
type
description
admin
bool
Permission to manage SIP trunks and dispatch rules.
call
bool
Permission to make SIP calls via
CreateSIPParticipant
.
Creating a token with SIP grants
Node.js
Go
Ruby
Java
Python
Rust
import
{
AccessToken
,
SIPGrant
,
VideoGrant
}
from
'livekit-server-sdk'
;
const
roomName
=
'name-of-room'
;
const
participantName
=
'user-name'
;
const
at
=
new
AccessToken
(
'api-key'
,
'secret-key'
,
{
identity
:
participantName
,
}
)
;
const
sipGrant
:
SIPGrant
=
{
admin
:
true
,
call
:
true
,
}
;
const
videoGrant
:
VideoGrant
=
{
room
:
roomName
,
roomJoin
:
true
,
}
;
at
.
addGrant
(
sipGrant
)
;
at
.
addGrant
(
videoGrant
)
;
const
token
=
await
at
.
toJwt
(
)
;
console
.
log
(
'access token'
,
token
)
;
Room configuration
You can create an access token for a user that includes room configuration options. When a room is created for a user, the room is created using the configuration
stored in the token. For example, you can use this to
explicitly dispatch an agent
when a user joins a room.
For the full list of
RoomConfiguration
fields, see
RoomConfiguration
.
Creating a token with room configuration
Node.js
Go
Ruby
Python
Rust
For a full example of explicit agent dispatch, see the
example
in GitHub.
import
{
AccessToken
,
SIPGrant
,
VideoGrant
}
from
'livekit-server-sdk'
;
import
{
RoomAgentDispatch
,
RoomConfiguration
}
from
'@livekit/protocol'
;
const
roomName
=
'name-of-room'
;
const
participantName
=
'user-name'
;
const
agentName
=
'my-agent'
;
const
at
=
new
AccessToken
(
'api-key'
,
'secret-key'
,
{
identity
:
participantName
,
}
)
;
const
videoGrant
:
VideoGrant
=
{
room
:
roomName
,
roomJoin
:
true
,
}
;
at
.
addGrant
(
videoGrant
)
;
at
.
roomConfig
=
new
RoomConfiguration
(
agents
:
[
new
RoomAgentDispatch
(
{
agentName
:
"test-agent"
,
metadata
:
"test-metadata"
}
)
]
)
;
const
token
=
await
at
.
toJwt
(
)
;
console
.
log
(
'access token'
,
token
)
;
Token refresh
LiveKit server proactively issues refreshed tokens to connected clients, ensuring they can reconnect if disconnected. These refreshed access tokens have a 10-minute expiration.
Additionally, tokens are refreshed when there are changes to a participant's name, permissions or metadata.
Updating permissions
A participant's permissions can be updated at any time, even after they've already connected. This is useful in applications where the participant's role could change during the session, such as in a participatory livestream.
It's possible to issue a token with
canPublish: false
initially, and then
updating it to
canPublish: true
during the session. Permissions can be changed
with the
UpdateParticipant
server API.
On this page
Overview
Creating a token
Token example
Video grant
Example: subscribe-only token
Example: camera-only
SIP grant
Creating a token with SIP grants
Room configuration
Creating a token with room configuration
Token refresh
Updating permissions


Content from https://docs.livekit.io/home/egress/overview:

On this page
Introduction
Egress types
Room composite egress
Web egress
Participant egress
Track composite egress
Track egress
Service architecture
Copy page
See more page options
Introduction
LiveKit Egress gives you a powerful and consistent set of APIs to export any room or individual tracks from a LiveKit session.
It supports recording to a MP4 file or HLS segments, as well as exporting to livestreaming services like YouTube Live, Twitch, and Facebook via RTMP(s).
For LiveKit Cloud customers, Egress is ready to use with your project without additional configuration.
When self-hosting LiveKit, Egress is a separate component that needs to be
deployed
.
Egress types
Room composite egress
Export an entire room's video and/or audio using a web layout rendered by Chrome. Room composites are tied to a room's lifecycle, and will stop automatically when the room ends. Composition templates are customizable web pages that can be hosted anywhere.
Example use case: recording a meeting for team members to watch later.
Web egress
Similar to Room Composite, but allows you to record and export any web page. Web Egress are not tied to LiveKit rooms, and can be used to record non-LiveKit content.
Example use case: restreaming content from a third-party source to YouTube and Twitch.
Participant egress
Export a participant's video and audio together. This is a newer API and is designed to be easier to use than Track Composite Egress.
Example use case: record the teacher's video in an online class.
Track composite egress
Sync and export up to one audio and one video track. Will transcode and mux.
Example use case: exporting audio+video from many cameras at once during a production, for use in additional post-production.
Track egress
Export individual tracks directly. Video tracks are not transcoded.
Example use case: streaming an audio track to a captioning service via websocket.
Service architecture
Depending on your request type, the egress service will either launch a web template in Chrome and connect to the room
(room composite requests), or it will use the sdk directly (track and track composite requests). It uses GStreamer to
encode, and can output to a file or to one or more streams.
On this page
Introduction
Egress types
Room composite egress
Web egress
Participant egress
Track composite egress
Track egress
Service architecture


Content from https://docs.livekit.io/home/ingress/overview:

On this page
Introduction
Supported Sources
Workflow
WHIP / RTMP
URL Input
API
CreateIngress
ListIngress
UpdateIngress
DeleteIngress
Using video presets
Custom settings
Enabling transcoding for WHIP sessions
Service architecture
Copy page
See more page options
Introduction
LiveKit Ingress lets you import video from another source into a LiveKit room. While WebRTC is a
versatile and scalable transport protocol for both media ingestion and delivery, some applications
require integrating with existing workflows or equipment that do not support WebRTC. Perhaps your
users want to publish video from OBS Studio or a dedicated hardware device, or maybe they want to stream
the content of media file hosted on a HTTP server to a room. LiveKit Ingress makes
these integrations easy.
LiveKit Ingress can automatically transcode the source media to ensure compatibility with LiveKit clients.
It can publish multiple layers with
Simulcast
.
The parameters of the different video layers can be defined at ingress creation time. Presets are provided to make encoding
settings configuration easy. The optional ability to provide custom encoding parameters enables more specialized use cases.
For LiveKit Cloud customers, Ingress is ready to use with your project without additional configuration.
When self-hosting LiveKit, Ingress is deployed as a separate service.
Supported Sources
RTMP/RTMPS
WHIP
Media files fetched from any HTTP server. The following media formats are supported:
HTTP Live Streaming (HLS)
ISO MPEG-4 (MP4)
Apple Quicktime (MOV)
Matroska (MKV/WEBM)
OGG audio
MP3 audio
M4A audio
Media served by a SRT server
Workflow
WHIP / RTMP
A typical push Ingress goes like this:
Your app creates an Ingress with
CreateIngress
API, which returns a URL and stream key of the Ingress
Your user copies and pastes the URL and key into your streaming workflow
Your user starts their stream
The Ingress Service starts transcoding their stream, or forwards media unchanged if transcoding is disabled.
The Ingress Service joins the LiveKit room and publishes the media for other Participants
When the stream source disconnects from the Ingress service, the Ingress Service participant leaves the room.
The Ingress remains valid, in a disconnected state, allowing it to be reused with the same stream key
URL Input
When pulling media from a HTTP or SRT server, Ingress has a slightly different lifecycle: it will start immediately after calling CreateIngress.
Your app creates an Ingress with
CreateIngress
API
The Ingress Service starts fetching the file or media and transcoding it
The Ingress Service joins the LiveKit room and publishes the transcoded media for other Participants
When the media is completely consumed, or if
DeleteIngress
is called, the Ingress Service participant leaves the room.
API
CreateIngress
WHIP / RTMP example
To provision an Ingress with the Ingress Service, use the CreateIngress API. It returns an
IngressInfo
object that describes the created Ingress, along with connection settings. These parameters can also be
queried at any time using the
ListIngress
API
LiveKit CLI
JavaScript
Go
Ruby
Create a file at
ingress.json
with the following content:
{
"input_type"
:
0
for RTMP
,
1
for WHIP
"name"
:
"Name of the Ingress goes here"
,
"room_name"
:
"Name of the room to connect to"
,
"participant_identity"
:
"Unique identity for the room participant the Ingress service will connect as"
,
"participant_name"
:
"Name displayed in the room for the participant"
,
"enable_transcoding"
:
true
// Transcode the input stream. Can only be false for WHIP,
}
Then create the Ingress using
lk
:
export
LIVEKIT_URL
=
https://my-livekit-host
export
LIVEKIT_API_KEY
=
livekit-api-key
export
LIVEKIT_API_SECRET
=
livekit-api-secret
lk ingress create ingress.json
URL Input example
With URL Input, Ingress will begin immediately after
CreateIngress
is called. URL_INPUT Ingress cannot be re-used.
LiveKit CLI
JavaScript
Go
Ruby
Create a file at
ingress.json
with the following content:
{
"input_type"
:
"URL_INPUT"
,
// or 2
"name"
:
"Name of the Ingress goes here"
,
"room_name"
:
"Name of the room to connect to"
,
"participant_identity"
:
"Unique identity for the room participant the Ingress service will connect as"
,
"participant_name"
:
"Name displayed in the room for the participant"
,
"url"
:
"HTTP(S) or SRT url to the file or stream"
}
Then create the Ingress using
lk
:
export
LIVEKIT_URL
=
https://my-livekit-host
export
LIVEKIT_API_KEY
=
livekit-api-key
export
LIVEKIT_API_SECRET
=
livekit-api-secret
lk ingress create ingress.json
ListIngress
LiveKit CLI
JavaScript
Go
Ruby
lk ingress list
The optional
--room
option allows to restrict the output to the Ingress associated to a given room. The
--id
option can check if a specific ingress is active.
UpdateIngress
The Ingress configuration can be updated using the
UpdateIngress
API. This enables
the ability to re-use the same Ingress URL to publish to different rooms. Only reusable Ingresses,
such as RTMP or WHIP, can be updated.
LiveKit CLI
JavaScript
Go
Ruby
Create a file at
ingress.json
with the fields to be updated.
{
"ingress_id"
:
"Ingress ID of the Ingress to update"
,
"name"
:
"Name of the Ingress goes here"
,
"room_name"
:
"Name of the room to connect to"
,
"participant_identity"
:
"Unique identity for the room participant the Ingress service will connect as"
,
"participant_name"
:
"Name displayed in the room for the participant"
}
The only required field is
ingress_id
. Non provided fields are left unchanged.
lk ingress update ingress.json
DeleteIngress
An Ingress can be reused multiple times. When not needed anymore, it can be deleted using the
DeleteIngress
API:
LiveKit CLI
JavaScript
Go
Ruby
lk ingress delete
<
INGRESS_ID
>
Using video presets
The Ingress service can transcode the media being received. This is the only supported behavior for RTMP and URL inputs. WHIP ingresses are not transcoded by default, but transcoding can be enabled by setting the
enable_transcoding
parameter. When transcoding is enabled, The default settings enable
video simulcast
to ensure media can be consumed by all viewers, and should be suitable for most use cases. In some situations however, adjusting these settings may be desirable to match source content or the viewer conditions better. For this purpose, LiveKit Ingress defines several presets, both for audio and video. Presets define both the characteristics of the media (codec, dimesions, framerate, channel count, sample rate) and the bitrate. For video, a single preset defines the full set of simulcast layers.
A preset can be chosen at Ingress creation time from the
constants in the Ingress protocol definition
:
LiveKit CLI
JavaScript
Go
Ruby
Create a file at
ingress.json
with the following content:
{
"name"
:
"Name of the egress goes here"
,
"room_name"
:
"Name of the room to connect to"
,
"participant_identity"
:
"Unique identity for the room participant the Ingress service will connect as"
,
"participant_name"
:
"Name displayed in the room for the participant"
"video"
:
{
"name"
:
"track name"
,
"source"
:
"SCREEN_SHARE"
,
"preset"
:
"Video preset enum value"
}
,
"audio"
:
{
"name"
:
"track name"
,
"source"
:
"SCREEN_SHARE_AUDIO"
,
"preset"
:
"Audio preset enum value"
}
}
Then create the Ingress using
lk
:
lk ingress create ingress.json
Custom settings
For specialized use cases, it is also possible to specify fully custom encoding parameters. In this case, all video layers need to be defined if simulcast is desired.
LiveKit CLI
JavaScript
Go
Ruby
Create a file at
ingress.json
with the following content:
{
"name"
:
"Name of the egress goes here"
,
"room_name"
:
"Name of the room to connect to"
,
"participant_identity"
:
"Unique identity for the room participant the Ingress service will connect as"
,
"participant_name"
:
"Name displayed in the room for the participant"
,
"video"
:
{
"options"
:
{
"video_codec"
:
"video codec ID from the [VideoCodec enum](https://github.com/livekit/protocol/blob/main/protobufs/livekit_models.proto)"
,
"frame_rate"
:
"desired framerate in frame per second"
,
"layers"
:
[
{
"quality"
:
"ID for one of the LOW, MEDIUM or HIGH VideoQualitu definitions"
,
"witdh"
:
"width of the layer in pixels"
,
"height"
:
"height of the layer in pixels"
,
"bitrate"
:
"video bitrate for the layer in bit per second"
}
]
}
}
,
"audio"
:
{
"options"
:
{
"audio_codec"
:
"audio codec ID from the [AudioCodec enum](https://github.com/livekit/protocol/blob/main/protobufs/livekit_models.proto)"
,
"bitrate"
:
"audio bitrate for the layer in bit per second"
,
"channels"
:
"audio channel count, 1 for mono, 2 for stereo"
,
"disable_dtx"
:
"wether to disable the [DTX feature](https://www.rfc-editor.org/rfc/rfc6716#section-2.1.9) for the OPUS codec"
}
}
}
Then create the Ingress using
lk
:
lk ingress create ingress.json
Enabling transcoding for WHIP sessions
By default, WHIP ingress sessions forward incoming audio and video media unmodified from the source to LiveKit clients. This behavior allows the lowest possible end to end latency between the media source and the viewers. This however requires the source encoder to be configured with settings that are compatible with all the subscribers, and ensure the right trade offs between quality and reach for clients with variable connection quality. This is best achieved when the source encoder is configured with simulcast enabled.
If the source encoder cannot be setup easily to achieve such tradeoffs, or if the available uplink bandwidth is insufficient to send all required simulcast layers, WHIP ingresses can be configured to transcode the source media similarly to other source types. This is done by setting the
enable_transcoding
option on the ingress. The encoder settings can then be configured in the
audio
and
video
settings in the same manner as for other inputs types.
LiveKit CLI
JavaScript
Go
Ruby
Create a file at
ingress.json
with the following content:
{
"input_type"
:
1
(WHIP only)
"name"
:
"Name of the egress goes here"
,
"room_name"
:
"Name of the room to connect to"
,
"participant_identity"
:
"Unique identity for the room participant the Ingress service will connect as"
,
"participant_name"
:
"Name displayed in the room for the participant"
,
"enable_transcoding"
:
true
"video"
:
{
"name"
:
"track name"
,
"source"
:
"SCREEN_SHARE"
,
"preset"
:
"Video preset enum value"
}
,
"audio"
:
{
"name"
:
"track name"
,
"source"
:
"SCREEN_SHARE_AUDIO"
,
"preset"
:
"Audio preset enum value"
}
}
Then create the Ingress using
lk
:
lk ingress create ingress.json
Service architecture
LiveKit Ingress exposes public RTMP and WHIP endpoints streamers can connect to. On initial handshake, the Ingress service validates the incoming request and retrieves the corresponding Ingress metadata, including what LiveKit room the stream belongs to. The Ingress server then sets up a GStreamer based media processing pipeline to transcode the incoming media to a format compatible with LiveKit WebRTC clients, publishes the resulting media to the LiveKit room.
On this page
Introduction
Supported Sources
Workflow
WHIP / RTMP
URL Input
API
CreateIngress
ListIngress
UpdateIngress
DeleteIngress
Using video presets
Custom settings
Enabling transcoding for WHIP sessions
Service architecture


Content from https://docs.livekit.io/agents/v0/integrations/overview:

On this page
Realtime API
LLM integrations
STT integrations
TTS integrations
Agents 1.0 available for Python
This documentation is for v0.x of the LiveKit Agents framework.
See updated documentation here:
Integration guides
.
v1.0 for Node.js is coming soon.
The Agents framework supports integrations for providers using plugins. This topic describes LLM, STT, and TTS provider plugins. Additional plugins include support for Retrieval-Augmented Generation (RAG), Natural Language Toolkit (NLTK), LlamaIndex, Silero VAD, turn detection, and more. To see the list of additional plugins and learn more about how LiveKit plugins work, see
Working with plugins
.
If you want to use a provider not listed in the following sections, contributions for plugins are always welcome. To learn more, see the the guidelines for contributions to the
Python repository
or the
Node.js repository
.
Realtime API integrations
Realtime APIs are designed for ulta-low-latency AI responses and use multimodal models. They can be better at understanding a user and their emotions resulting in more natural interactions.
OpenAI Realtime API
Support for the OpenAI Realtime API.
Gemini Live API
Support for Google's Gemini Live API.
Azure OpenAI Realtime API
OpenAI Realtime API hosted on Azure.
LLM integrations
You can create an instance of an LLM to use in a
VoicePipelineAgent
for the following providers using a plugin:
Anthropic
Azure
Amazon Web Services (AWS)
Cerebras
Deepseek
Fireworks
Gemini
Groq
LlamaIndex
Octo
Ollama
Openai
Perplexity
Telnyx
Together
xAI
STT integrations
LiveKit plugins support the following STT providers. Create an instance of an STT to use in a `VoicePipelineAgent` or as a standalone transcription service.
Provider
Supported frameworks
AssemblyAI
Python
Azure
Python, Node.js
AWS
Python
Clova
Python
Deepgram
Python, Node.js
fal
Python
Google
Python
Groq
Python, Node.js
OpenAI
Python, Node.js
Speechmatics
Python
TTS integrations
LiveKit plugins support the following TTS providers. Create an instance of a TTS to use in a `VoicePipelineAgent` or for speech generation in your apps.
Provider
Supported frameworks
Azure
Python
AWS
Python
Cartesia
Python, Node.js
Deepgram
Python, Node.js
Elevenlabs
Python, Node.js
Neuphonic
Python
OpenAI
Python, Node.js
PlayHT
Python
Rime
Python
On this page
Realtime API
LLM integrations
STT integrations
TTS integrations


Content from https://docs.livekit.io/agents/v0/integrations/openai/overview:

On this page
Overview
Quick reference
LLM
STT
TTS
Copy page
See more page options
Agents 1.0 available for Python
This documentation is for v0.x of the LiveKit Agents framework.
See updated documentation here:
OpenAI integration
.
v1.0 for Node.js is coming soon.
Overview
LiveKit's OpenAI integration provides support for LLM, STT, TTS, and the OpenAI Realtime API. The
Quick reference
section in this topic describes using OpenAI for LLM,
STT, and TTS. For a guide on using the Realtime API, see
the
OpenAI Realtime API integration guide
.
Use OpenAI STT, TTS, and LLM to create agents using the
VoicePipelineAgent
class. To use the Realtime API, you can create an agent using the
MultimodalAgent
class.
Note
The following quickstart guides are available to get you started creating an AI voice assistant with OpenAI:
Voice agent quickstart
using the
VoicePipelineAgent
class.
Speech-to-speech quickstart
using the
MultimodalAgent
class.
Quick reference
The following sections provide a quick reference for integrating OpenAI with LiveKit. For the complete
reference, see the links provided in each section.
LLM
LiveKit's OpenAI plugin provides support for creating an instance of an LLM class to be used in a
VoicePipelineAgent
.
LLM class usage
Create an instance of OpenAI LLM:
agent.py
.env.local
Python
Node.js
from
livekit
.
plugins
.
openai
import
llm
openai_llm
=
llm
.
LLM
(
model
=
"gpt-4o"
,
temperature
=
0.8
,
)
LLM parameters
This section describes some of the available parameters. For a complete reference of all available parameters,
see the
plugin reference
.
model
string | ChatModels
Optional
Default:
gpt-4o
#
ID of the model to use for inference. For a list of supported models, see
ChatModels
in the respective GitHub repository:
Python
,
Node.js
.
To learn more about available models, see
Models
.
api_key
string
Optional
Env:
OPENAI_API_KEY
#
OpenAI API key. Required if the environment variable is not set.
temperature
float
Optional
Default:
1.0
#
A measure of randomness of completions. A lower temperature is more deterministic. To learn more, see
chat completions
.
STT
LiveKit's OpenAI plugin allows you to create an instance of OpenAI STT that can be used as the first stage in a
VoicePipelineAgent
or as a standalone transcription service.
STT usage
Create an OpenAI STT:
agent.py
.env.local
Python
Node.js
from
livekit
.
plugins
.
openai
import
stt
openai_stt
=
stt
.
STT
(
language
=
"en"
,
model
=
"whisper-1"
,
)
STT parameters
This section describes some of the available parameters. For a complete reference of all available parameters, see
the
plugin reference
.
model
WhisperModels | string
Optional
Default:
whisper-1
#
ID of the model to use for speech recognition. To learn more, see
Whisper
.
language
string
Optional
Default:
en
#
Language of input audio in
ISO-639-1
format. See OpenAI's documentation for a list of
supported languages
.
TTS
LiveKit's OpenAI plugin allows you to create an instance of OpenAI TTS that can be used in a
VoicePipelineAgent
or as a standalone speech generator.
TTS usage
Create an OpenAI TTS:
agent.py
.env.local
Python
Node.js
from
livekit
.
plugins
.
openai
import
tts
openai_tts
=
tts
.
TTS
(
model
=
"tts-1"
,
voice
=
"nova"
,
)
TTS parameters
This section describes some of the available parameters. For a complete reference of all available parameters, see the
plugin reference
.
model
TTSModels | string
Optional
Default:
tts-1
#
ID of the model to use for speech generation. To learn more, see
TTS models
.
voice
TTSVoice | string
Optional
Default:
alloy
#
ID of the voice used for speech generation. To learn more, see
TTS voice options
.
On this page
Overview
Quick reference
LLM
STT
TTS


Content from https://docs.livekit.io/agents/:

On this page
Introduction
Use cases
Framework overview
How agents connect to LiveKit
Getting started
Copy page
See more page options
Introduction
The Agents framework allows you to add a Python or Node.js program to any LiveKit room as a full realtime participant. The SDK includes a complete set of tools and abstractions that make it easy to feed realtime media and data through an AI pipeline that works with any provider, and to publish realtime results back to the room.
If you want to get your hands on the code right away, follow this quickstart guide. It takes just a few minutes to build your first voice agent.
Voice AI quickstart
Build a simple voice assistant with Python in less than 10 minutes.
Deeplearning.ai course
Learn to build and deploy voice agents with LiveKit in this free course from Deeplearning.ai.
GitHub
GitHub repository
Python source code and examples for the LiveKit Agents SDK.
SDK reference
Python reference docs for the LiveKit Agents SDK.
Use cases
Some applications for agents include:
Multimodal assistant
: Talk, text, or screen share with an AI assistant.
Telehealth
: Bring AI into realtime telemedicine consultations, with or without humans in the loop.
Call center
: Deploy AI to the front lines of customer service with inbound and outbound call support.
Realtime translation
: Translate conversations in realtime.
NPCs
: Add lifelike NPCs backed by language models instead of static scripts.
Robotics
: Put your robot's brain in the cloud, giving it access to the most powerful models.
The following
recipes
demonstrate some of these use cases:
Medical Office Triage
Agent that triages patients based on symptoms and medical history.
Restaurant Agent
A restaurant front-of-house agent that can take orders, add items to a shared cart, and checkout.
Company Directory
Build a AI company directory agent. The agent can respond to DTMF tones and voice prompts, then redirect callers.
Pipeline Translator
Implement translation in the processing pipeline.
Framework overview
Your agent code operates as a stateful, realtime bridge between powerful AI models and your users. While AI models typically run in data centers with reliable connectivity, users often connect from mobile networks with varying quality.
WebRTC ensures smooth communication between agents and users, even over unstable connections. LiveKit WebRTC is used between the frontend and the agent, while the agent communicates with your backend using HTTP and WebSockets. This setup provides the benefits of WebRTC without its typical complexity.
The agents SDK includes components for handling the core challenges of realtime voice AI, such as streaming audio through an STT-LLM-TTS pipeline, reliable turn detection, handling interruptions, and LLM orchestration. It supports plugins for most major AI providers, with more continually added. The framework is fully open source and supported by an active community.
Other framework features include:
Voice, video, and text
: Build agents that can process realtime input and produce output in any modality.
Tool use
: Define tools that are compatible with any LLM, and even forward tool calls to your frontend.
Multi-agent handoff
: Break down complex workflows into simpler tasks.
Extensive integrations
: Integrate with nearly every AI provider there is for LLMs, STT, TTS, and more.
State-of-the-art turn detection
: Use the custom turn detection model for lifelike conversation flow.
Made for developers
: Build your agents in code, not configuration.
Production ready
: Includes built-in worker orchestration, load balancing, and Kubernetes compatibility.
Open source
: The framework and entire LiveKit ecosystem are open source under the Apache 2.0 license.
How agents connect to LiveKit
When your agent code starts, it first registers with a LiveKit server (either
self hosted
or
LiveKit Cloud
) to run as a "worker" process. The worker waits until it receives a dispatch request. To fulfill this request, the worker boots a "job" subprocess which joins the room. By default, your workers are dispatched to each new room created in your LiveKit project. To learn more about workers, see the
Worker lifecycle
guide.
After your agent and user join a room, the agent and your frontend app can communicate using LiveKit WebRTC. This enables reliable and fast realtime communication in any network conditions. LiveKit also includes full support for telephony, so the user can join the call from a phone instead of a frontend app.
To learn more about how LiveKit works overall, see the
Intro to LiveKit
guide.
Getting started
Follow these guides to learn more and get started with LiveKit Agents.
Voice AI quickstart
Build a simple voice assistant with Python in less than 10 minutes.
Recipes
A comprehensive collection of examples, guides, and recipes for LiveKit Agents.
Intro to LiveKit
An overview of the LiveKit ecosystem.
Web and mobile frontends
Put your agent in your pocket with a custom web or mobile app.
Telephony integration
Your agent can place and receive calls with LiveKit's SIP integration.
Building voice agents
Comprehensive documentation to build advanced voice AI apps with LiveKit.
Worker lifecycle
Learn how to manage your agents with workers and jobs.
Deploying to production
Guide to deploying your voice agent in a production environment.
Integration guides
Explore the full list of AI providers available for LiveKit Agents.
On this page
Introduction
Use cases
Framework overview
How agents connect to LiveKit
Getting started


Content from https://docs.livekit.io/sip/:

On this page
Introduction
Concepts
SIP participant
Trunks
Dispatch rules
Service architecture
Using LiveKit SIP
SIP features
Noise cancellation for calls
Next steps
Copy page
See more page options
Introduction
LiveKit SIP bridges the gap between traditional telephony and modern digital communication.
It enables seamless interaction between traditional phone systems and LiveKit rooms. You can use
LiveKit SIP to accept calls and make calls. When you add LiveKit Agents, you can use an AI voice
agent to handle your inbound and outbound calls.
Concepts
LiveKit SIP extends the
core primitives
—participant, room,
and track—to include two additional concepts specific to SIP: trunks and dispatch rules.
These concepts are represented by objects created through the
API
and control how calls are handled.
SIP participant
Each caller, callee, and AI voice agent that participates in a call is a LiveKit participant. A SIP participant is like any other participant and can be managed using the
participant APIs
. They have the same
attributes and metadata
as any other participant, and have additional
SIP specific attributes
.
For inbound calls, a SIP participant is automatically created for each caller.
To make an outbound call, you create a SIP participant using the
CreateSIPParticipant
API to make the call.
Trunks
LiveKit SIP trunks bridge your SIP provider and LiveKit. To use LiveKit, you must configure a SIP trunk with your telephony provider. The setup depends on your use case—whether you're handling incoming calls, making outgoing calls, or both.
Inbound trunks
handle incoming calls and can be restricted to specific IP addresses or phone numbers.
Outbound trunks
are used to place outgoing calls.
Trunks can be region restricted to meet local telephony regulations.
Note
The same SIP provider trunk can be associated with both an inbound and an outbound trunk in LiveKit.
You only need to create an inbound or outbound trunk
once
.
Dispatch rules
Dispatch Rules
are associated with a specific trunk and control how inbound calls are
dispatched to LiveKit rooms. All callers can be placed in the same room or different rooms based on the dispatch
rules. Multiple dispatch rules can be associated with the same trunk as long as each rule has a different pin.
Dispatch rules can also be used to add custom participant attributes to
SIP participants
.
Service architecture
LiveKit SIP relies on the following services:
SIP trunking provider for your phone number. LiveKit SIP supports most SIP providers out of the box.
LiveKit server (part of LiveKit Cloud) for API requests, managing and verifying SIP trunks and
dispatch rules, and creating participants and rooms for calls.
LiveKit SIP (part of LiveKit Cloud) to respond to SIP requests, mediate trunk authentication, and
match dispatch rules.
If you use LiveKit Cloud, LiveKit SIP is ready to use with your project without any
additional configuration. If you're self hosting LiveKit, the SIP service needs to be deployed
separately. To learn more about self hosting, see
SIP server
.
Using LiveKit SIP
The LiveKit SIP SDK is available in multiple languages. To learn more, see
SIP API
.
LiveKit SIP has been tested with the following SIP providers:
Note
LiveKit SIP is designed to work with all SIP providers. However, compatibility testing is
limited to the providers below.
Twilio
Telnyx
Exotel
Plivo
SIP features
LiveKit SIP supports the following functionality.
Feature
Description
DTMF
You can configure DTMF when making outbound calls by adding them to the
CreateSIPParticipant
request.
To learn more, see
Making a call with extension codes (DTMF)
.
SIP REFER
You can transfer calls using the
TransferSIPParticipant
API. Calls can be transferred to any valid
telephone number or SIP URI. To learn more, see
Cold transfer
.
SIP headers
You can map custom
X-*
SIP headers to participant attributes. For example, custom headers can be used
to route calls to different workflows. To learn more, see
Custom attributes
.
Noise cancellation
You can enable noise cancellation for callers and callees using Krisp. To learn more,
see
Noise cancellation for calls
.
Region pinning
You can restrict incoming and outgoing calls to a specific region to comply with local telephony regulations. To learn more, see
Region pinning for SIP
.
Noise cancellation for calls
Krisp
noise cancellation uses AI models to identify and remove background noise in realtime. This improves
the quality of calls that occur in noisy environments. For LiveKit SIP applications that use agents, noise cancellation improves the
quality and clarity of user speech for turn detection, transcriptions, and recordings.
For incoming calls, see the
inbound trunks documentation
for the
krisp_enabled
attribute.
For outgoing calls, see the
CreateSIPParticipant
documentation for the
krisp_enabled
attribute used during
outbound call creation
.
Next steps
See the following guides to get started with LiveKit SIP:
SIP trunk setup
Purchase a phone number and configure your SIP trunking provider for LiveKit SIP.
Accepting inbound calls
Learn how to accept inbound calls with LiveKit SIP.
Making outbound calls
Learn how to make outbound calls with LiveKit SIP.
Voice AI telephony guide
Create an AI agent integrated with telephony.
On this page
Introduction
Concepts
SIP participant
Trunks
Dispatch rules
Service architecture
Using LiveKit SIP
SIP features
Noise cancellation for calls
Next steps


Content from https://docs.livekit.io/agents/build/:

On this page
Overview
Agent sessions
RoomIO
Voice AI providers
Capabilities
Copy page
See more page options
Overview
Building a great voice AI app requires careful orchestration of multiple components. LiveKit Agents is built on top of the
Realtime SDK
to provide dedicated abstractions that simplify development while giving you full control over the underlying code.
Agent sessions
The
AgentSession
is the main orchestrator for your voice AI app. The session is responsible for collecting user input, managing the voice pipeline, invoking the LLM, and sending the output back to the user.
Each session requires at least one
Agent
to orchestrate. The agent is responsible for defining the core AI logic - instructions, tools, etc - of your app. The framework supports the design of custom
workflows
to orchestrate handoff and delegation between multiple agents.
The following example shows how to begin a simple single-agent session:
from
livekit
.
agents
import
AgentSession
,
Agent
,
RoomInputOptions
from
livekit
.
plugins
import
openai
,
cartesia
,
deepgram
,
noise_cancellation
,
silero
from
livekit
.
plugins
.
turn_detector
.
multilingual
import
MultilingualModel
session
=
AgentSession
(
stt
=
deepgram
.
STT
(
)
,
llm
=
openai
.
LLM
(
)
,
tts
=
cartesia
.
TTS
(
)
,
vad
=
silero
.
VAD
.
load
(
)
,
turn_detection
=
turn_detector
.
MultilingualModel
(
)
,
)
await
session
.
start
(
room
=
ctx
.
room
,
agent
=
Agent
(
instructions
=
"You are a helpful voice AI assistant."
)
,
room_input_options
=
RoomInputOptions
(
noise_cancellation
=
noise_cancellation
.
BVC
(
)
,
)
,
)
RoomIO
Communication between agent and user participants happens using media streams, also known as tracks. For voice AI apps, this is primarily audio, but can include vision. By default, track management is handled by
RoomIO
, a utility class that serves as a bridge between the agent session and the LiveKit room. When an AgentSession is initiated, it automatically creates a
RoomIO
object that enables all room participants to subscribe to available audio tracks.
To learn more about publishing audio and video, see the following topics:
Agent speech and audio
Add speech, audio, and background audio to your agent.
Vision
Give your agent the ability to see images and live video.
Text and transcription
Send and receive text messages and transcription to and from your agent.
Realtime media
Tracks are a core LiveKit concept. Learn more about publishing and subscribing to media.
Camera and microphone
Use the LiveKit SDKs to publish audio and video tracks from your user's device.
Custom RoomIO
For greater control over media sharing in a room,  you can create a custom
RoomIO
object. For example, you might want to manually control which input and output devices are used, or to control which participants an agent listens to or responds to.
To replace the default one created in
AgentSession
, create a
RoomIO
object in your entrypoint function and pass it an instance of the
AgentSession
in the constructor. For examples, see the following in the GitHub repository:
Toggling audio
Create a push-to-talk interface to toggle audio input and output.
Toggling input and output
Toggle both audio and text input and output.
Voice AI providers
You can choose from a variety of providers for each part of the voice pipeline to fit your needs. The framework supports both high-performance STT-LLM-TTS pipelines and speech-to-speech models. In either case, it automatically manages interruptions, transcription forwarding, turn detection, and more.
You may add these components to the
AgentSession
, where they act as global defaults within the app, or to each individual
Agent
if needed.
TTS
Text-to-speech integrations
STT
Speech-to-text integrations
LLM
Language model integrations
Multimodal
Realtime multimodal APIs
Capabilities
The following guides, in addition to others in this section, cover the core capabilities of the
AgentSession
and how to leverage them in your app.
Workflows
Orchestrate complex tasks among multiple agents.
Tool definition & use
Use tools to call external services, inject custom logic, and more.
Pipeline nodes
Add custom behavior to any component of the voice pipeline.
On this page
Overview
Agent sessions
RoomIO
Voice AI providers
Capabilities


Content from https://docs.livekit.io/agents/worker/:

On this page
Overview
Worker options
Copy page
See more page options
Overview
When you start your app with
python agent.py dev
, it registers itself as a
worker
with LiveKit server. LiveKit server manages dispatching your agents to rooms with users by sending requests to available workers.
A
LiveKit session
is one or more participants in a
room
. A LiveKit session is often referred to simply as a "room." When a user connects to a room, a worker fulfills the request to dispatch an agent to the room.
An overview of the worker lifecycle is as follows:
Worker registration
: Your agent code registers itself as a "worker" with LiveKit server, then waits on standby for requests.
Job request
: When a user connects to a room, LiveKit server sends a request to an available worker. A worker accepts and starts a new process to handle the job. This is also known as
agent dispatch
.
Job
: The job initiated by your
entrypoint
function. This is the bulk of the code and logic you write. To learn more, see
Job lifecycle
.
LiveKit session close
: By default, a room is automatically closed when the last non-agent participant leaves. Any remaining agents disconnect. You can also
end the session
manually.
The following diagram shows the worker lifecycle:
Some additional features of workers include the following:
Workers automatically exchange availability and capacity information with the LiveKit server, enabling load balancing of incoming requests.
Each worker can run multiple jobs simultaneously, running each in its own process for isolation. If one crashes, it won’t affect others running on the same worker.
When you deploy updates, workers gracefully drain active LiveKit sessions before shutting down, ensuring no sessions are interrupted mid-call.
Worker options
You can change the permissions, dispatch rules, add prewarm functions, and more through
WorkerOptions
.
On this page
Overview
Worker options


Content from https://docs.livekit.io/agents/integrations/:

On this page
Overview
Installing plugins
Using plugins
OpenAI API compatibility
Core plugins
Additional plugins
Build your own plugin
Copy page
See more page options
Overview
LiveKit Agents includes support for a wide variety of AI providers, from the largest research companies to emerging startups.
The open source plugin interface makes it easy to adopt the best AI providers for your app, without needing customized code for each.
Installing plugins
Each provider is available as a separate plugin package, included as an optional dependency on the base SDK for Python.
For example, to install the SDK with the Cartesia, Deepgram, and OpenAI plugins, run the following command:
pip
install
"livekit-agents[cartesia,deepgram,openai]~=1.0"
You may also install plugins as individual packages. For example, this is equivalent to the previous command:
pip
install
\
"livekit-agents~=1.0"
\
"livekit-plugins-cartesia~=1.0"
\
"livekit-plugins-deepgram~=1.0"
\
"livekit-plugins-openai~=1.0"
Using plugins
The AgentSession class accepts plugins as arguments using a standard interface. Each plugin loads its own associated API key from environment variables. For instance, the following code creates an AgentSession that uses the OpenAI, Cartesia, and Deepgram plugins installed in the preceding section:
agent.py
.env
from
livekit
.
plugins
import
openai
,
cartesia
,
deepgram
session
=
AgentSession
(
llm
=
openai
.
LLM
(
model
=
"gpt-4o"
)
,
tts
=
cartesia
.
TTS
(
model
=
"sonic-english"
)
,
stt
=
deepgram
.
STT
(
model
=
"nova-2"
)
,
)
OpenAI API compatibility
Many providers have standardized around the OpenAI API format for chat completions and more. The LiveKit Agents OpenAI plugin provides easy compatibility with many of these providers through special methods which load the correct API key from environment variables. For instance, to use Cerebras instead of OpenAI, you can use the following code:
agent.py
.env
from
livekit
.
plugins
import
openai
session
=
AgentSession
(
llm
=
openai
.
LLM
.
with_cerebras
(
model
=
"llama-3.1-70b-versatile"
)
,
# ... stt, tts, etc ..
)
Core plugins
The following are the core plugin types used in LiveKit Agents, to handle the primary voice AI tasks. Many providers are available for most functions.
Realtime models
Plugins for multimodal speech-to-speech models like the OpenAI Realtime API.
Large language models (LLM)
Plugins for AI models from OpenAI, Anthropic, and more.
Speech-to-text (STT)
Plugins for speech-to-text solutions like Deepgram, Whisper, and more.
Text-to-speech (TTS)
Plugins for text-to-speech solutions like Cartesia, ElevenLabs, and more.
Additional plugins
LiveKit Agents also includes the following additional specialized plugins, which are recommended for most voice AI use cases. Each runs locally and requires no additional API keys.
Silero VAD
Voice activity detection with Silero VAD.
LiveKit turn detector
A custom LiveKit model for improved end-of-turn detection.
Enhanced noise cancellation
LiveKit Cloud enhanced noise cancellation to improve voice AI performance.
Build your own plugin
The LiveKit Agents plugin framework is extensible and community-driven. Your plugin can integrate with new providers or directly load models for local inference. LiveKit especially welcomes new TTS, STT, and LLM plugins.
To learn more, see the guidelines for contributions to
the
Python
and
Node.js
SDKs.
On this page
Overview
Installing plugins
Using plugins
OpenAI API compatibility
Core plugins
Additional plugins
Build your own plugin


Content from https://docs.livekit.io/home/quickstarts/:

Platform-specific quickstart guides
LiveKit has SDKs for most major platforms and languages. Quickly integrate realtime AI, audio, or video into your app by selecting your platform below.
Web SDKs
For browser-based applications.
Next.js
React
JavaScript
Unity (WebGL)
Native SDKs
For native applications on mobile, desktop, and more.
Swift
Android (Compose)
Android
Flutter
React Native
Expo
Other SDKs
Don’t see your platform listed?
View the full
list of supported SDKs
.
Integrate with a
telephone system using SIP
.
Join the
LiveKit Slack community
to share what you’re building.
On this page
Web SDKs
Native SDKs
Other platforms


Content from https://docs.livekit.io/home/cloud/:

On this page
Why Choose LiveKit Cloud?
Comparing Open Source and Cloud
Copy page
See more page options
LiveKit Cloud is a fully-managed, globally distributed mesh network of LiveKit servers that provides all the power of the open-source platform with none of the operational complexity. It allows you to focus on building your application while LiveKit handles deployment, scaling, and maintenance.
Dashboard
Sign up for LiveKit Cloud to manage your projects, view analytics, and configure your LiveKit Cloud deployment.
Pricing
View LiveKit Cloud pricing plans and choose the right option for your application's needs.
Why Choose LiveKit Cloud?
Zero operational overhead
: No need to manage servers, scaling, or infrastructure.
Global edge network
: Users connect to the closest server for minimal latency.
Unlimited scale
: Support for rooms with unlimited participants through our mesh architecture.
Enterprise-grade reliability
: 99.99% uptime guarantee with redundant infrastructure.
Comprehensive analytics
: Monitor usage, performance, and quality metrics through the Cloud dashboard.
Same APIs and SDKs
: Use the exact same code whether you're on Cloud or self-hosted.
LiveKit Cloud runs the same open-source servers that you can find on GitHub. It provides the same APIs and supports all of the same SDKs. An open source user can migrate to Cloud, and a Cloud customer can switch to self-hosted at any moment. As far as your code is concerned, the only difference is the URL that it connects to.
For more details on LiveKit Cloud's architecture, see
Cloud Architecture
.
Comparing Open Source and Cloud
When building with LiveKit, you can either self-host the open-source server or use the managed LiveKit Cloud service:
Open Source
Cloud
Realtime features
Full support
Full support
Egress (recording, streaming)
Full support
Full support
Ingress (RTMP, WHIP, SRT ingest)
Full support
Full support
SIP (telephony integration)
Full support
Full support
Agents framework
Full support
Full support
Who manages it
You
LiveKit
Architecture
Single-home SFU
Mesh SFU
Connection model
Users in the same room connect to the same server
Each user connects to the closest server
Max users per room
Up to ~3,000
No limit
Analytics & telemetry
N/A
Cloud dashboard
Uptime guarantees
N/A
99.99%
On this page
Why Choose LiveKit Cloud?
Comparing Open Source and Cloud


Content from https://docs.livekit.io/sip/cloud:

On this page
Overview
Region pinning
Inbound calls
Outbound calls
Considerations
Available regions
Copy page
See more page options
Overview
LiveKit SIP is part of
LiveKit Cloud
and runs as a globally distributed service, providing redundancy and high availability. By default, SIP endpoints are global, and calls are routed through the region closest to the origination point. Incoming calls are routed to the region closest to the SIP trunking provider's endpoint. Outgoing calls originate from the same region where the
CreateSIPParticipant
API call is made.
In most cases, using the global endpoint is the recommended approach. However, if you need to exercise more control over call routing—for example, to comply with local telephony regulations—LiveKit SIP supports region pinning. This allows you to restrict both incoming and outgoing calls to a specific region.
Region pinning
Region pinning allows you to restrict incoming and outgoing calls to a specific region to comply with local telephony regulations. The following sections describe how to enable region pinning.
Inbound calls
To enable region pinning for incoming calls, configure your SIP trunking provider to use a region-based endpoint. A region-based endpoint is configured to direct traffic only to nodes within a specific region.
Region-based endpoint format
The endpoint format is as follows:
{
sip_subdomain
}
.
{
region_name
}
.
sip
.
livekit
.
cloud
Where:
{sip_subdomain}
is your LiveKit SIP URI subdomain. This is also your project ID without the
p_
prefix. You can find your SIP URI on the
Project settings
page.
For example, if your SIP URI is
sip:bwwn08a2m4o.sip.livekit.cloud
, your SIP subdomain is
bwwn08a2m4o
.
{region_name}
is one of the following
regions
:
eu
,
india
,
sa
,
us
For example to create a SIP endpoint for India, see the following:
Tip
Sign in to LiveKit Cloud to automatically include the subdomain for your project in the example.
<
your SIP subdomain
>
.india.sip.livekit.cloud
Use the region-based endpoint to configure your SIP trunking provider. Follow the instructions for external provider setup in
SIP trunk setup
.
Outbound calls
To originate calls from the same region as the destination phone number, set the
destination_country
parameter for an outbound trunk. This applies region pinning to all calls made through the trunk. When
destination_country
is enabled, outbound calls are routed based on location:
For countries that LiveKit operates data centers in, calls originate from a server within the country.
For other countries, calls originate from a server that is closest to that country.
In the unlikely event that the preferred region is non-operational or offline, calls originate from another region nearby. For a full list of supported regions, see
Available regions
.
The
destination_country
parameter accepts a two-letter
country code
. To learn more, see
CreateSIPOutboundTrunk
.
Example outbound trunk
Create an outbound trunk with the
destination_country
parameter set to India,
india
.
Create a file named
outbound-trunk.json
, replacing the phone number with your SIP provider phone number and username and password:
{
"trunk"
:
{
"name"
:
"My outbound trunk"
,
"phone_number"
:
"+15105550100"
,
"username"
:
"myusername"
,
"password"
:
"mypassword"
,
"destination_country"
:
"in"
}
}
Create the outbound trunk using the CLI:
lk sip outbound create outbound-trunk.json
To learn more, see
Outbound trunks
.
Considerations
When you enable region pinning, you turn off automatic failover to the nearest region in the case of an outage.
Available regions
The following regions are available for region pinning for SIP:
Region name
Region locations
eu
France, Germany, Zurich
india
India
sa
Saudi Arabia
us
US Central, US East B, US West B
Note
This list of regions is subject to change. Last updated 2025-07-23.
On this page
Overview
Region pinning
Inbound calls
Outbound calls
Considerations
Available regions


Content from https://docs.livekit.io/sip/accepting-calls:

On this page
Inbound call workflow
Setup for accepting calls
SIP trunking provider setup
LiveKit SIP configuration
Next steps
Copy page
See more page options
Inbound call workflow
When an inbound call is received, your SIP trunking provider sends a text-based
INVITE request to LiveKit SIP. The SIP service checks authorization credentials configured for
the LiveKit trunk with the credentials configured on your provider's SIP trunk and looks for a
matching dispatch rule. If there's a matching dispatch rule, a SIP participant is created for the
caller and put into a LiveKit room.
Depending on the dispatch rule, other participants (for example, a voice agent or other users) might
join the room.
User dials the SIP trunking provider phone number.
SIP trunking provider connects caller to LiveKit SIP.
LiveKit SIP authenticates the trunk credentials and finds a matching dispatch rule.
LiveKit server creates a SIP participant for the caller and places them in a LiveKit room
(per the dispatch rule).
User hears dial tone until LiveKit SIP responds to the call:
If the dispatch rule has a pin, prompts the user with
"Please enter room pin and press hash to confirm."
Incorrect pin: "No room matched the pin you entered." Call is disconnected with a tone.
Correct pin: "Entering room now."
User continues to hear a dial tone until another participant publishes tracks to the room.
Setup for accepting calls
The following are required to accept an inbound SIP call.
SIP trunking provider setup
Purchase a phone number from a SIP provider.
For a list of tested providers, see the table in
Using LiveKit SIP
.
Configure SIP trunking with the provider to send SIP traffic to your LiveKit SIP instance.
For instructions for setting up a SIP trunk,
see
Configuring a SIP provider trunk
.
LiveKit SIP configuration
Create an
inbound trunk
associated with your SIP provider phone number.
You only need to create one inbound trunk for each SIP provider phone number.
Create a
dispatch rule
. The dispatch rules dictate how SIP participants and
LiveKit rooms are created for incoming calls. The rules can include whether a caller needs to enter
a pin code to join a room and any custom metadata or attributes to be added to SIP participants.
Next steps
See the following guide to create an AI agent to receive inbound calls.
Voice AI telephony guide
Create an AI agent to receive inbound calls.
On this page
Inbound call workflow
Setup for accepting calls
SIP trunking provider setup
LiveKit SIP configuration
Next steps


Content from https://docs.livekit.io/sip/trunk-inbound:

On this page
Overview
Restricting calls to a region
Inbound trunk example
Accepting calls from specific phone numbers
List inbound trunks
Update inbound trunk
Update specific fields of an inbound trunk
Replace inbound trunk
Copy page
See more page options
Overview
After you purchase a phone number and
configure your SIP trunking provider
,
you must create an inbound trunk and
dispatch rule
to accept incoming calls. The inbound trunk allows
you to limit incoming calls to those coming from your SIP trunking provider.
You can also configure additional properties for all incoming calls that match the trunk including SIP headers,
participant metadata and attributes, and session properties. For a full list of available parameters, see
CreateSIPInboundTrunk
.
Note
LiveKit supports username and password authentication for inbound trunks, but your SIP trunking provider must also support it. Support varies by provider—for example, Twilio Elastic SIP Trunking doesn’t support it, though you can use username and password authentication with
TwiML
. Check with your provider to confirm.
To learn more about LiveKit SIP, see
SIP overview
. To learn more about SIP API endpoints and types, see
SIP API
.
Restricting calls to a region
When you configure your SIP trunking provider for inbound calls, you need to specify the LiveKit SIP endpoint to use. By default, this is a global endpoint and incoming calls are routed to the region closest to the call's origination point—typically the region where your telephony provider initiated the call. You can limit calls to a specific region using
region pinning
.
Inbound trunk example
The following examples create an inbound trunk that accepts calls made to the number
+1-510-555-0100
and enables Krisp
noise cancellation
. This phone number is the number purchased from your SIP
trunking provider.
LiveKit CLI
Node.js
Python
Ruby
Go
LiveKit Cloud
Create a file named
inbound-trunk.json
with the following content:
{
"trunk"
:
{
"name"
:
"My trunk"
,
"numbers"
:
[
"+15105550100"
]
,
"krispEnabled"
:
true
}
}
Important
If you're using Telnyx, the leading
+
in the phone number assumes the
Destination Number Format
is set to
+E.164
for your number.
Create the inbound trunk using
lk
:
lk sip inbound create inbound-trunk.json
Accepting calls from specific phone numbers
The configuration for inbound trunk accepts inbound calls to number
+1-510-555-0100
from caller numbers
+1-310-555-1100
and
+1-714-555-0100
.
Important
Remember to replace the numbers in the example with actual phone numbers when creating your trunks.
Tip
You can also filter allowed caller numbers with a
Dispatch Rule
.
LiveKit CLI
Node.js
Python
Ruby
Go
LiveKit Cloud
Create a file named
inbound-trunk.json
with the following content:
{
"trunk"
:
{
"name"
:
"My trunk"
,
"numbers"
:
[
"+15105550100"
]
,
"allowedNumbers"
:
[
"+13105550100"
,
"+17145550100"
]
}
}
Important
If you're using Telnyx, the leading
+
in the phone number assumes the
Destination Number Format
is set to
+E.164
for your number.
Create the inbound trunk using
lk
:
lk sip inbound create inbound-trunk.json
List inbound trunks
Use the
ListSIPInboundTrunk
API to list all inbound trunks and trunk parameters.
LiveKit CLI
Node.js
Python
Ruby
Go
LiveKit Cloud
lk sip inbound list
Update inbound trunk
Use the
UpdateSIPInboundTrunk
API to update specific fields of an inbound trunk or
replace
an inbound trunk with a new one.
Update specific fields of an inbound trunk
The
UpdateSIPInboundTrunkFields
API allows you to update specific fields of an inbound trunk without affecting other fields.
LiveKit CLI
Node.js
Python
Ruby
Go
LiveKit Cloud
Create a file named
inbound-trunk.json
with the following content:
{
"name"
:
"My trunk"
,
"numbers"
:
[
"+15105550100"
]
}
Important
If you're using Telnyx, the leading
+
in the phone number assumes the
Destination Number Format
is set to
+E.164
for your number.
Update the inbound trunk using
lk
:
lk sip inbound update
--id
<
trunk-id
>
inbound-trunk.json
Replace inbound trunk
The
UpdateSIPInboundTrunk
API allows you to replace an existing inbound trunk with a new one using the same trunk ID.
LiveKit CLI
Node.js
Python
Ruby
Go
LiveKit Cloud
The CLI doesn't support replacing inbound trunks.
On this page
Overview
Restricting calls to a region
Inbound trunk example
Accepting calls from specific phone numbers
List inbound trunks
Update inbound trunk
Update specific fields of an inbound trunk
Replace inbound trunk


Content from https://docs.livekit.io/sip/dispatch-rule:

On this page
Caller dispatch rule (individual)
Direct dispatch rule
Pin-protected room
Callee dispatch rule
Setting custom attributes on inbound SIP participants
Setting custom metadata on inbound SIP participants
Update dispatch rule
Update specific fields of a dispatch rule
Replace dispatch rule
List dispatch rules
Copy page
See more page options
To create a dispatch rule with the SIP service, use the
CreateSIPDispatchRule
API.
It returns a
SIPDispatchRuleInfo
object that describes the created
SIPDispatchRule
.
By default, a dispatch rule is matched against all your trunks and makes a caller's phone number visible to others in the
room. You can change these default behaviors using dispatch rule options. See the
CreateSIPDispatchRule
API reference for full list of available options.
To learn more about SIP and dispatch rules, see
SIP overview
. To learn more about SIP API endpoints and types, see
SIP API
.
Caller dispatch rule (individual)
An
SIPDispatchRuleIndividual
rule creates a new room for each caller.
The name of the created room is the phone number of the caller plus a random suffix.
You can optionally add a specific prefix to the room name by using the
roomPrefix
option.
The following examples dispatch callers into individual rooms prefixed with
call-
,
and
dispatches an agent
named
inbound-agent
to newly created rooms:
LiveKit CLI
Node.js
Python
Ruby
Go
LiveKit Cloud
{
"dispatch_rule"
:
{
"rule"
:
{
"dispatchRuleIndividual"
:
{
"roomPrefix"
:
"call-"
}
}
,
"name"
:
"My dispatch rule"
,
"roomConfig"
:
{
"agents"
:
[
{
"agentName"
:
"inbound-agent"
,
"metadata"
:
"job dispatch metadata"
}
]
}
}
}
Note
When you omit the
trunk_ids
field, the dispatch rule matches calls from all inbound trunks.
Direct dispatch rule
A direct dispatch rule places all callers into a specified room.
You can optionally protect room access by adding a pin in the
pin
field:
In the following examples, all calls are immediately connected to room
open-room
on LiveKit.
LiveKit CLI
Node.js
Python
Ruby
Go
LiveKit Cloud
Create a file named
dispatch-rule.json
and add the following:
{
"dispatch_rule"
:
{
"rule"
:
{
"dispatchRuleDirect"
:
{
"roomName"
:
"open-room"
}
}
,
"name"
:
"My dispatch rule"
}
}
Create the dispatch rule using
lk
:
lk sip dispatch create dispatch-rule.json
Pin-protected room
Add a
pin
to a room to require callers to enter a pin to connect to a room in LiveKit. The following
example requires callers to enter
12345#
on the phone to enter
safe-room
:
{
"dispatch_rule"
:
{
"trunk_ids"
:
[
]
,
"rule"
:
{
"dispatchRuleDirect"
:
{
"roomName"
:
"safe-room"
,
"pin"
:
"12345"
}
}
,
"name"
:
"My dispatch rule"
}
}
Callee dispatch rule
This creates a dispatch rule that puts callers into rooms based on the called number.
The name of the room is the called phone number plus an optional prefix (if
roomPrefix
is set).
You can optionally add a random suffix for each caller by setting
randomize
to true, making a separate room per caller.
LiveKit CLI
Node.js
Python
Ruby
Go
LiveKit Cloud
{
"dispatch_rule"
:
{
"rule"
:
{
"dispatchRuleCallee"
:
{
"roomPrefix"
:
"number-"
,
"randomize"
:
false
}
}
,
"name"
:
"My dispatch rule"
}
}
Setting custom attributes on inbound SIP participants
LiveKit participants have an
attributes
field that stores key-value pairs. You can add custom attributes
for SIP participants in the dispatch rule. These attributes are inherited by all SIP participants created
by the dispatch rule.
To learn more, see
SIP participant attributes
.
The following examples add two attributes to SIP participants created by this dispatch rule:
LiveKit CLI
Node.js
Python
Ruby
Go
LiveKit Cloud
{
"dispatch_rule"
:
{
"attributes"
:
{
"<key_name1>"
:
"<value1>"
,
"<key_name2>"
:
"<value2>"
}
,
"rule"
:
{
"dispatchRuleIndividual"
:
{
"roomPrefix"
:
"call-"
}
}
,
"name"
:
"My dispatch rule"
}
}
Setting custom metadata on inbound SIP participants
LiveKit participants have a
metadata
field that can store arbitrary data for your application (typically JSON).
It can also be set on SIP participants created by a dispatch rule.
Specifically,
metadata
set on a dispatch rule will be inherited by all SIP participants created by it.
The following examples add the metadata,
{"is_internal": true}
, to all SIP participants created from an inbound
call by this dispatch rule:
LiveKit CLI
Node.js
Python
Ruby
Go
LiveKit Cloud
{
"dispatch_rule"
:
{
"metadata"
:
"{\"is_internal\": true}"
,
"rule"
:
{
"dispatchRuleIndividual"
:
{
"roomPrefix"
:
"call-"
}
}
,
"name"
:
"My dispatch rule"
}
}
Update dispatch rule
Use the
UpdateSIPDispatchRule
API to update specific fields of a dispatch rule or
replace
a dispatch rule with a new one.
Update specific fields of a dispatch rule
The
UpdateSIPDispatchRuleFields
API allows you to update specific fields of a dispatch rule without affecting other fields.
LiveKit CLI
Node.js
Python
Ruby
Go
LiveKit Cloud
Create a file named
dispatch-rule.json
with the following content:
{
"name"
:
"My updated dispatch rule"
,
"rule"
:
{
"dispatchRuleCallee"
:
{
"roomPrefix"
:
"number-"
,
"randomize"
:
false
,
"pin"
:
"1234"
}
}
}
Update the dispatch rule using
lk
. You can update the
trunks
parameter to a comma-separated string of trunks IDs if the rule matches specific trunks.
lk sip dispatch update
--id
<
dispatch-rule-id
>
\
--trunks
"[]"
\
dispatch-rule.json
Replace dispatch rule
The
UpdateSIPDispatchRule
API allows you to replace an existing dispatch rule with a new one using the same dispatch rule ID.
LiveKit CLI
Node.js
Python
Ruby
Go
LiveKit Cloud
The instructions for replacing a dispatch rule are the same as for
updating a dispatch rule
.
List dispatch rules
Use the
ListSIPDispatchRule
API to list all dispatch rules.
LiveKit CLI
Node.js
Python
Ruby
Go
LiveKit Cloud
lk sip dispatch list
On this page
Caller dispatch rule (individual)
Direct dispatch rule
Pin-protected room
Callee dispatch rule
Setting custom attributes on inbound SIP participants
Setting custom metadata on inbound SIP participants
Update dispatch rule
Update specific fields of a dispatch rule
Replace dispatch rule
List dispatch rules


Content from https://docs.livekit.io/sip/accepting-calls-twilio-voice:

On this page
Inbound calls with Twilio programmable voice
Step 1. Purchase a phone number from Twilio
Step 2. Set up a TwiML Bin
Step 3. Direct phone number to the TwiML Bin
Step 4. Create a LiveKit inbound trunk
Step 5. Create a dispatch rule to place each caller into their own room.
Testing with an agent
Connecting to a Twilio phone conference
Step 1. Set Twilio environment variables
Step 2. Bridge a Twilio conference and LiveKit SIP
Step 3.  Execute the file
Copy page
See more page options
Inbound calls with Twilio programmable voice
Accept inbound calls using Twilio programmable voice. All you need is an inbound trunk and a dispatch rule
created using the LiveKit CLI (or SDK) to accept calls and route callers to LiveKit rooms. The following
steps guide you through the process.
Note
This method doesn't support
SIP REFER
. To set up Elastic SIP Trunking, see the
Configuring Twilio SIP trunks
quickstart.
Step 1. Purchase a phone number from Twilio
If you don't already have a phone number, see
How to Search for and Buy a Twilio Phone Number From Console
.
Step 2. Set up a TwiML Bin
TwiML Bins are a simple way to test TwiML responses. Use a TwiML Bin to redirect an inbound call to LiveKit.
To create a TwiML Bin, follow these steps:
Navigate to your
TwiML Bins
page.
Create a TwiML Bin and add the following contents:
<?xml version="1.0" encoding="UTF-8"?>
<
Response
>
<
Dial
>
<
Sip
username
=
"
<sip_trunk_username>
"
password
=
"
<sip_trunk_password>
"
>
sip:
<
your_phone_number
>
@
<
your
SIP
endpoint
>
</
Sip
>
</
Dial
>
</
Response
>
Step 3. Direct phone number to the TwiML Bin
Configure incoming calls to a specific phone number to use the TwiML Bin you just created:
Navigate to the
Manage numbers
page
and select the purchased phone number.
In the
Voice Configuration
section, edit the
A call comes in
fields. After you select
TwiML Bin
.
select the TwiML Bin created in the previous step.
Step 4. Create a LiveKit inbound trunk
Use the LiveKit CLI to create an
inbound trunk
for the purchased phone number.
Create an
inbound-trunk.json
file with the following contents. Replace the phone number
and add a
username
and
password
of your choosing:
{
"trunk"
:
{
"name"
:
"My inbound trunk"
,
"auth_username"
:
"<sip_trunk_username>"
,
"auth_password"
:
"<sip_trunk_password>"
}
}
Note
Be sure to use the same username and password that's specified in the TwiML Bin.
Use the CLI to create an inbound trunk:
lk sip inbound create inbound-trunk.json
Step 5. Create a dispatch rule to place each caller into their own room.
Use the LiveKit CLI to create a
dispatch rule
that places each caller into
individual rooms named with the prefix
call
.
Create a
dispatch-rule.json
file with the following contents:
{
"dispatch_rule"
:
{
"rule"
:
{
"dispatchRuleIndividual"
:
{
"roomPrefix"
:
"call-"
}
}
}
}
Create the dispatch rule using the CLI:
lk sip dispatch create dispatch-rule.json
Testing with an agent
Follow the
Voice AI quickstart
to create an agent that responds to incoming calls. Then call the phone number and your agent should pick up the call.
Connecting to a Twilio phone conference
You can bridge Twilio conferencing to LiveKit via SIP, allowing you to add agents and other LiveKit
clients to an existing Twilio conference. This requires the following setup:
Twilio conferencing
.
LiveKit
inbound trunk
.
LiveKit
voice AI agent
.
The example in this section uses
Node
and the
Twilio Node SDK
.
Step 1. Set Twilio environment variables
You can find these values in your
Twilio Console
:
export
TWILIO_ACCOUNT_SID
=
<
twilio_account_sid
>
export
TWILIO_AUTH_TOKEN
=
<
twilio_auth_token
>
Step 2. Bridge a Twilio conference and LiveKit SIP
Create a
bridge.js
file and update the
twilioPhoneNumber
,
conferenceSid
,
sipHost
,
and
from
field for the API call in the following code:
Note
If you're signed in to
LiveKit Cloud
, your sip host is filled in
below.
import
twilio
from
'twilio'
;
const
accountSid
=
process
.
env
.
TWILIO_ACCOUNT_SID
;
const
authToken
=
process
.
env
.
TWILIO_AUTH_TOKEN
;
const
twilioClient
=
twilio
(
accountSid
,
authToken
)
;
/**
* Phone number bought from Twilio that is associated with a LiveKit trunk.
* For example, +14155550100.
* See https://docs.livekit.io/sip/quickstarts/configuring-twilio-trunk/
*/
const
twilioPhoneNumber
=
'<sip_trunk_phone_number>'
;
/**
* SIP host is available in your LiveKit Cloud project settings.
* This is your project domain without the leading "sip:".
*/
const
sipHost
=
'<your SIP endpoint>'
;
/**
* The conference SID from Twilio that you want to add the agent to. You
* likely want to obtain this from your conference status callback webhook handler.
* The from field must contain the phone number, client identifier, or username
* portion of the SIP address that made this call.
* See https://www.twilio.com/docs/voice/api/conference-participant-resource#request-body-parameters
*/
const
conferenceSid
=
'<twilio_conference_sid>'
;
await
twilioClient
.
conferences
(
conferenceSid
)
.
participants
.
create
(
{
from
:
'<valid_from_value>'
,
to
:
`
sip:
${
twilioPhoneNumber
}
@
${
sipHost
}
`
,
}
)
;
Step 3.  Execute the file
When you run the file, it bridges the Twilio conference to a new LiveKit session using the previously
configured dispatch rule. This allows you to automatically
dispatch an agent
to the Twilio conference.
node
bridge.js
On this page
Inbound calls with Twilio programmable voice
Step 1. Purchase a phone number from Twilio
Step 2. Set up a TwiML Bin
Step 3. Direct phone number to the TwiML Bin
Step 4. Create a LiveKit inbound trunk
Step 5. Create a dispatch rule to place each caller into their own room.
Testing with an agent
Connecting to a Twilio phone conference
Step 1. Set Twilio environment variables
Step 2. Bridge a Twilio conference and LiveKit SIP
Step 3.  Execute the file


Content from https://docs.livekit.io/sip/making-calls:

On this page
Outbound call workflow
Setup for making calls
SIP trunking provider setup
LiveKit SIP configuration
Make an outbound call
Next steps
Copy page
See more page options
Outbound call workflow
To make an outbound call, you create a
SIP participant
with the user's phone
number. When you execute the
CreateSIPParticipant
request, LiveKit SIP sends an INVITE request to
your SIP provider. If the SIP provider accepts the call, the SIP participant is added to the
LiveKit room.
Call the
CreateSIPParticipant
API to create a SIP participant.
LiveKit SIP sends an INVITE request to the SIP trunking provider.
SIP trunking provider validates trunk credentials and accepts the call.
LiveKit server places SIP participant in the LiveKit room specified in the
CreateSIPParticipant
request.
Setup for making calls
The following sections outline the steps required to make an outbound SIP call.
SIP trunking provider setup
Purchase a phone number from a SIP Provider.
For a list of tested providers, see the table in
Using LiveKit SIP
.
Configure the SIP Trunk on the provider to send SIP traffic to accept SIP traffic from the
LiveKit SIP service.
For instructions for setting up a SIP trunk,
see
Configuring a SIP provider trunk
.
LiveKit SIP configuration
Create an
outbound trunk
associated with your SIP provider phone number.
This is the number that is used to dial out to the user. Include the authentication credentials
required by your SIP trunking provider to make calls.
Make an outbound call
Create a SIP participant. When the
CreateSIPParticipant
request is executed, a SIP call is initiated:
An INVITE request is sent to the SIP trunk provider. The provider checks authentication credentials
and returns a response to LiveKit.
If the call is accepted, LiveKit dials the user and creates a SIP participant in the LiveKit room.
If the call is not accepted by the SIP trunk provider, the
CreateSIPParticipant
request fails.
After the call starts ringing, you can check the call status by listening to
participant events
:
If the
sip.callStatus
participant attribute is updated to
active
, the call has connected.
If the call fails, the participant is disconnected and leaves the room.
Next steps
See the following guide to create an AI agent that makes outbound calls.
Voice AI telephony guide
Create an AI agent to make outbound calls.
On this page
Outbound call workflow
Setup for making calls
SIP trunking provider setup
LiveKit SIP configuration
Make an outbound call
Next steps


Content from https://docs.livekit.io/sip/trunk-outbound:

On this page
Restricting calls to a region
Create an outbound trunk
Configuring an outbound trunk for any phone number
List outbound trunks
Update an outbound trunk
Update specific fields of an outbound trunk
Replace an outbound trunk
IP address range for LiveKit Cloud SIP
Copy page
See more page options
To provision an outbound trunk with the SIP Service, use the
CreateSIPOutboundTrunk
API. It returns an
SIPOutboundTrunkInfo
object that describes the created SIP trunk. You can query these parameters any time using the
ListSIPOutboundTrunk
API.
Restricting calls to a region
To originate calls from the same region as the destination phone number, set the
destination_country
parameter for an outbound trunk. This applies region pinning to all calls made through the trunk. When
destination_country
is enabled, outbound calls are routed based on location:
For countries that LiveKit operates data centers in, calls originate from a server within the country.
For other countries, calls originate from a server that is closest to that country.
In the unlikely event that the preferred region is non-operational or offline, calls originate from another region nearby. For a full list of supported regions, see
Available regions
.
The
destination_country
parameter accepts a two-letter country code. To learn more, see
CreateSIPOutboundTrunk
.
Create an outbound trunk
The following creates a SIP outbound trunk with username and password authentication. It makes outbound calls from number
+15105550100
.
LiveKit CLI
Node.js
Python
Ruby
Go
LiveKit Cloud
Create a file named
outbound-trunk.json
using your phone number, trunk domain name, and
username
and
password
:
Twilio
Telnyx
{
"trunk"
:
{
"name"
:
"My outbound trunk"
,
"address"
:
"<my-trunk>.pstn.twilio.com"
,
"numbers"
:
[
"+15105550100"
]
,
"authUsername"
:
"<username>"
,
"authPassword"
:
"<password>"
}
}
Create the outbound trunk using the CLI:
lk sip outbound create outbound-trunk.json
The output of the command returns the trunk ID. Copy it for the next step:
SIPTrunkID: <your-trunk-id>
Configuring an outbound trunk for any phone number
The
numbers
parameter for outbound trunks is a required field. However, you can set the parameter to any string (for example,
*
) to use the outbound trunk for calls from any number. This is useful if you want to use the same outbound trunk for all calls or if you want to use a different phone number for each call.
Instead of setting the number on the trunk, you can set the phone number to call from using the
sip_number
parameter for the
CreateSIPParticipant
API.
The following example creates an outbound trunk that allows calling from any number, then initiates a call using the outbound trunk.
Create an outbound trunk using the CLI.
Create a file named
outbound-trunk.json
and copy and paste the following content:
{
"trunk"
:
{
"name"
:
"My outbound trunk"
,
"address"
:
"<my-trunk>.pstn.twilio.com"
,
"numbers"
:
[
"*"
]
,
"auth_username"
:
"<username>"
,
"auth_password"
:
"<password>"
}
}
Create the outbound trunk using the CLI:
lk sip outbound create outbound-trunk.json
Initiate a call from the number
+15105550100
using the CLI. This number is the phone number configured with your SIP trunk provider. Use the <trunk-id> from the output of the previous step.
Create a file named
participant.json
and copy and paste the following content:
{
"sip_number"
:
"+15105550100"
,
"sip_trunk_id"
:
"<trunk-id>"
,
"sip_call_to"
:
"+12135550100"
,
"room_name"
:
"open-room"
,
"participant_identity"
:
"sip-test"
,
"participant_name"
:
"Test call participant"
,
"wait_until_answered"
:
true
}
Important
If you're using Telnyx, the leading
+
in the phone number assumes the
Destination Number Format
is set to
+E.164
for your number.
Initiate the call using the CLI:
lk sip participant create participant.json
After you run the command, a call from the number
+15105550100
to
+12135550100
is initiated. Output from the command returns when the call is answered.
List outbound trunks
Use the
ListSIPOutboundTrunk
API to list all outbound trunks and trunk parameters.
LiveKit CLI
Node.js
Python
Ruby
Go
LiveKit Cloud
lk sip outbound list
Update an outbound trunk
The
UpdateSIPOutboundTrunk
API allows you to update specific fields of an outbound trunk or
replace
an outbound trunk with a new one.
Update specific fields of an outbound trunk
The
UpdateSIPOutboundTrunkFields
API allows you to update specific fields of an outbound trunk without affecting other fields.
LiveKit CLI
Node.js
Python
Ruby
Go
LiveKit Cloud
Create a file named
outbound-trunk.json
with the fields you want to update. The following example updates the name and phone numbers for the trunk:
Twilio
Telnyx
{
"name"
:
"My updated outbound trunk"
,
"address"
:
"<my-trunk>.pstn.twilio.com"
,
"numbers"
:
[
"+15105550100"
]
}
Update the outbound trunk using the CLI:
lk sip outbound update
--id
<
sip-trunk-id
>
outbound-trunk.json
The output of the command returns the trunk ID:
SIPTrunkID: <your-trunk-id>
Replace an outbound trunk
The
UpdateSIPOutboundTrunk
API allows you to replace an existing outbound trunk with a new one using the same trunk ID.
LiveKit CLI
Node.js
Python
Ruby
Go
LiveKit Cloud
The CLI doesn't support replacing outbound trunks.
IP address range for LiveKit Cloud SIP
LiveKit Cloud nodes do not have a static IP address range, thus there's no way currently to use IP range for outbound authentication.
Thus, prefer setting user/password authentication on SIP trunk Provider.
If it's unavailable, or IP range is required in addition to user/password, set range(s) that include all IPs: e.g.
0.0.0.0/0
or
0.0.0.0/1
+
128.0.0.0/1
.
On this page
Restricting calls to a region
Create an outbound trunk
Configuring an outbound trunk for any phone number
List outbound trunks
Update an outbound trunk
Update specific fields of an outbound trunk
Replace an outbound trunk
IP address range for LiveKit Cloud SIP


Content from https://docs.livekit.io/sip/outbound-calls:

On this page
Creating a SIP participant
Making a call with extension codes (DTMF)
Playing dial tone while the call is dialing
Copy page
See more page options
The following sections include examples for making an outbound call by creating a LiveKit SIP
participant and configuring call settings for dialing out. To create an AI agent to make outbound
calls on your behalf, see the
Voice AI telephony guide
.
Creating a SIP participant
To make outbound calls with SIP Service, create a SIP participant with the
CreateSIPParticipant
API.
It returns an
SIPParticipantInfo
object that describes the participant.
Outbound calling requires at least one
Outbound Trunk
.
LiveKit CLI
Node.js
Python
Ruby
Go
Create a
sip-participant.json
file with the following participant details:
{
"sip_trunk_id"
:
"<your-trunk-id>"
,
"sip_call_to"
:
"<phone-number-to-dial>"
,
"room_name"
:
"my-sip-room"
,
"participant_identity"
:
"sip-test"
,
"participant_name"
:
"Test Caller"
,
"krisp_enabled"
:
true
,
"wait_until_answered"
:
true
}
Create the SIP Participant using the CLI. After you run this command, the participant makes a
call to the
sip_call_to
number configured in your outbound trunk. When you set
wait_until_answered
to
true
, the command waits until the callee picks up the call before returning. You can also monitor
the call status using the
SIP participant attributes
. When the
callee picks up the call, the
sip.callStatus
attribute is
active
.
lk sip participant create sip-participant.json
Once the user picks up, they will be connected to
my-sip-room
.
Making a call with extension codes (DTMF)
To make outbound calls with fixed extension codes (DTMF tones), set
dtmf
field in
CreateSIPParticipant
request:
LiveKit CLI
Node.js
Python
Ruby
Go
{
"sip_trunk_id"
:
"<your-trunk-id>"
,
"sip_call_to"
:
"<phone-number-to-dial>"
,
"dtmf"
:
"*123#ww456"
,
"room_name"
:
"my-sip-room"
,
"participant_identity"
:
"sip-test"
,
"participant_name"
:
"Test Caller"
}
Tip
Character
w
can be used to delay DTMF by 0.5 sec.
This example will dial a specified number and will send the following DTMF tones:
*123#
Wait 1 sec
456
Playing dial tone while the call is dialing
SIP participants emit no audio by default while the call connects.
This can be changed by setting
play_dialtone
field in
CreateSIPParticipant
request:
LiveKit CLI
Node.js
Python
Ruby
Go
{
"sip_trunk_id"
:
"<your-trunk-id>"
,
"sip_call_to"
:
"<phone-number-to-dial>"
,
"room_name"
:
"my-sip-room"
,
"participant_identity"
:
"sip-test"
,
"participant_name"
:
"Test Caller"
,
"play_dialtone"
:
true
}
If
play_dialtone
is enabled, the SIP Participant plays a dial tone to the room until the phone is picked up.
On this page
Creating a SIP participant
Making a call with extension codes (DTMF)
Playing dial tone while the call is dialing


Content from https://docs.livekit.io/sip/dtmf:

On this page
Sending DTMF
Receiving DTMF
Copy page
See more page options
LiveKit's Telephony stack fully supports DTMF tones, enabling integration
with legacy IVR systems. It also enables agents to receive DTMF tones from telephone users.
Sending DTMF
To send DTMF tones, use the
publishDtmf
API on the
localParticipant
.
This API transmits DTMF tones to the room; tones can be sent by any participant in the room.
SIP participants in the room receive the tones and relay them to the telephone user.
Node.js
Python
Go
// publishes 123# in DTMF
await
localParticipant
.
publishDtmf
(
1
,
'1'
)
;
await
localParticipant
.
publishDtmf
(
2
,
'2'
)
;
await
localParticipant
.
publishDtmf
(
3
,
'3'
)
;
await
localParticipant
.
publishDtmf
(
11
,
'#'
)
;
Tip
Sending DTMF tones requires both a numeric code and a string representation to
ensure compatibility with various SIP implementations.
Special characters like
*
and
#
are mapped to their respective numeric codes.
See
RFC 4733
for details.
Receiving DTMF
When SIP receives DTMF tones, they are relayed to the room as events that participants can listen for.
Node.js
Python
Go
room
.
on
(
RoomEvent
.
DtmfReceived
,
(
code
,
digit
,
participant
)
=>
{
console
.
log
(
'DTMF received from participant'
,
participant
.
identity
,
code
,
digit
)
;
}
)
;
On this page
Sending DTMF
Receiving DTMF


Content from https://docs.livekit.io/sip/transfer-cold:

On this page
Transferring a SIP participant using SIP REFER
Enable call transfers for your Twilio SIP trunk
TransferSIPParticipant server API parameters
Usage
Copy page
See more page options
A "cold transfer" refers to transferring a caller (SIP participant) to another number or SIP endpoint without a hand off. A cold
transfer shuts down the room (that is, the session) of the original call.
Transferring a SIP participant using SIP REFER
REFER is a SIP method that allows you to move an active session to another endpoint (that is, transfer a call). For LiveKit
telephony apps, you can use the
TransferSIPParticipant
server API to transfer a caller to another phone number or SIP endpoint.
In order to successfully transfer calls, you must configure your provider trunks to allow call transfers.
Enable call transfers for your Twilio SIP trunk
Enable call transfer and PSTN transfers for your Twilio SIP trunk. To learn more, see Twilio's
Call Transfer via SIP REFER
documentation.
When you transfer a call, you have the option to set the caller ID to display the phone number of the transferee (the caller)
or the transferor (the phone number associated with your LiveKit trunk).
CLI
Console
The following command enables call transfers and sets the caller ID to display the number of the transferee:
Note
To list trunks, execute
twilio api trunking v1 trunks list
.
To set the caller ID to the transferor, set
transfer-caller-id
to
from-transferor
.
twilio api trunking v1 trunks update
--sid
<
twilio-trunk-sid
>
\
--transfer-mode enable-all
\
--transfer-caller-id from-transferee
TransferSIPParticipant server API parameters
transfer_to
string
Required
#
The
transfer_to
value can either be a valid telephone number or a SIP URI.
The following examples are valid values:
tel:+15105550100
sip:+15105550100@sip.telnyx.com
sip:+15105550100@my-livekit-demo.pstn.twilio.com
participant_identity
string
Required
#
Identity of the SIP participant that should be transferred.
room_name
string
Required
#
Source room name for the transfer.
play_dialtone
bool
Required
#
Play dial tone to the user being transferred when a transfer is initiated.
Usage
Set up the following environment variables:
export
LIVEKIT_URL
=
<
your LiveKit server URL
>
export
LIVEKIT_API_KEY
=
<
your API Key
>
export
LIVEKIT_API_SECRET
=
<
your API Secret
>
Reveal API Key and Secret
Node.js
Python
Ruby
Go
This example uses the LiveKit URL, API key, and secret set as environment variables.
import
{
SipClient
}
from
'livekit-server-sdk'
;
// ...
async
function
transferParticipant
(
participant
)
{
console
.
log
(
"transfer participant initiated"
)
;
const
sipTransferOptions
=
{
playDialtone
:
false
}
;
const
sipClient
=
new
SipClient
(
process
.
env
.
LIVEKIT_URL
,
process
.
env
.
LIVEKIT_API_KEY
,
process
.
env
.
LIVEKIT_API_SECRET
)
;
const
transferTo
=
"tel:+15105550100"
;
await
sipClient
.
transferSipParticipant
(
'open-room'
,
participant
.
identity
,
transferTo
,
sipTransferOptions
)
;
console
.
log
(
'transfer participant'
)
;
}
On this page
Transferring a SIP participant using SIP REFER
Enable call transfers for your Twilio SIP trunk
TransferSIPParticipant server API parameters
Usage


Content from https://docs.livekit.io/sip/hd-voice:

On this page
Configuring Telnyx
Other Providers
Copy page
See more page options
Telephone calls have traditionally been routed through the Public Switched Telephone Network (PSTN), a technology for landlines dating back over a century. PSTN calls are limited to an 8kHz sample rate using a narrowband audio codec, resulting in audio that typically sounds muffled or lacks range.
Modern cell phones can use VoIP for calls when connected via Wi-Fi or mobile data. VoIP can leverage wideband audio codecs that transmit audio at a higher sample rate, resulting in much higher quality audio, often referred to as HD Voice.
LiveKit SIP supports wideband audio codecs such as G.722 out of the box, providing higher quality audio when used with HD Voice-capable SIP trunks or endpoints.
Configuring Telnyx
Telnyx supports HD Voice for customers in the US. To enable HD Voice with Telnyx, ensure the following are configured in your Telnyx portal:
HD Voice feature
is enabled on the phone number you are trying to use (under Number -> Voice)
G.722
codec is enabled on your SIP Trunk (under SIP Connection -> Inbound)
We recommend leaving G.711U enabled for compatibility.
Other Providers
Currently, Twilio does not support HD voice. If you find other providers that support HD voice, please let us know so we can update this guide.
On this page
Configuring Telnyx
Other Providers


Content from https://docs.livekit.io/sip/sip-participant:

On this page
SIP participant attributes
SIP attributes
Twilio attributes
Custom attributes
Examples
Basic example
Modify voice AI agent based on caller attributes
Creating a SIP participant to make outbound calls
Copy page
See more page options
Note
To create a SIP participant to make outbound calls, see
Make outbound calls
.
Each user in a LiveKit telephony app is a
LiveKit participant
.
This includes end users who call in using your inbound trunk, the participant you use to make outbound calls,
and if you're using an agent, the AI voice agent that interacts with callers.
SIP participants are managed like any other participant using the
participant management commands
.
SIP participant attributes
SIP participants can be identified using the
kind
field for participants, which identifies the
type of participant
in a LiveKit room (i.e. session).
For SIP participants, this is
Participant.Kind == SIP
.
The participant
attributes
field contains SIP specific attributes that identify the caller and call details. You can use SIP participant attributes to create different workflows based on the caller.
For example, look up customer information in a database to identify the caller.
SIP attributes
All SIP participants have the following attributes:
Attribute
Description
sip.callID
LiveKit's SIP call ID. A unique ID used as a SIP call tag to identify a conversation
(i.e. match requests and responses).
sip.callIDFull
Trunk provider SIP call ID. A globally unique ID to identify a specific SIP call.
sip.callStatus
Current call status for the SIP call associated with this participant. Valid values are:
active
: Participant is connected and the call is active.
automation
: For outbound calls using Dual-Tone Multi-Frequency (DTMF), this status indicates the call
has successfully connected, but is still dialing DTMF numbers. After all the numbers are dialed,
the status changes to
active
.
dialing
: Call is dialing and waiting to be picked up.
hangup
: Call has been ended by a participant.
ringing
: Inbound call is ringing for the caller. Status changes to
active
when the SIP participant subscribes to any remote audio tracks.
sip.phoneNumber
User's phone number. For inbound trunks, this is the phone number the call originates from.
For outbound SIP, this is the number dialed by the SIP participant.
Note
This attribute isn't available if
HidePhoneNumber
is set in the dispatch rule.
sip.ruleID
SIP
DispatchRule
ID used for the inbound call. This field is empty for outbound calls.
sip.trunkID
The inbound or outbound SIP trunk ID used for the call.
sip.trunkPhoneNumber
Phone number associated with SIP trunk. For inbound trunks, this is the number
dialed in to by an end user. For outbound trunks, this is the number a call originates from.
Twilio attributes
If you're using Twilio SIP trunks, the following additional attributes are included:
Attribute
Description
sip.twilio.accountSid
Twilio account SID.
sip.twilio.callSid
Twilio call SID.
Custom attributes
You can add custom SIP participant attributes in one of two ways:
Adding attributes to the dispatch rule. To learn more, see
Setting custom attributes on inbound SIP participants
.
Using SIP headers: For any
X-*
SIP headers, you can configure your trunk with
headers_to_attributes
and a key/value pair mapping.
For example:
Telnyx
Twilio
{
"trunk"
:
{
"name"
:
"Demo inbound trunk"
,
"numbers"
:
[
"+15105550100"
]
,
"headers_to_attributes"
:
{
"X-<custom_key_value>"
:
"<custom_attribute_name>"
,
}
}
}
Caution
Note the leading
+
assumes the
Destination Number Format
is set to
+E.164
for your Telnyx number.
Examples
The following examples use SIP participant attributes.
Basic example
Node.js
Python
This example logs the Twilio call SID if the user is a SIP participant.
if
(
participant
.
kind
==
ParticipantKind
.
SIP
)
{
console
.
log
(
participant
.
attributes
[
'sip.twilio.callSid'
]
)
;
}
;
Modify voice AI agent based on caller attributes
Follow the
Voice AI quickstart
to create an agent that responds to incoming calls. Then modify the agent to use SIP participant attributes.
Python
Node.js
Before starting your
AgentSession
, select the best Deepgram STT model for the participant. Add this code
to your
entrypoint
function:
# Add this import to the top of your file
from
livekit
import
rtc
participant
=
await
ctx
.
wait_for_participant
(
)
dg_model
=
"nova-2-general"
# Check if the participant is a SIP participant
if
participant
.
kind
==
rtc
.
ParticipantKind
.
PARTICIPANT_KIND_SIP
:
# Use a Deepgram model better suited for phone calls
dg_model
=
"nova-2-phonecall"
if
participant
.
attributes
[
'sip.phoneNumber'
]
==
'+15105550100'
:
logger
.
info
(
"Caller phone number is +1-510-555-0100"
)
# Add other logic here to modify the agent based on the caller's phone number
session
=
AgentSession
(
stt
=
deepgram
.
STT
(
model
=
dg_model
)
,
# ... llm, vad, tts, etc.
)
# ... rest of your entrypoint, including `await session.start(...)`
Creating a SIP participant to make outbound calls
To make outbound calls, create a SIP participant. To learn more, see
Make outbound calls
.
On this page
SIP participant attributes
SIP attributes
Twilio attributes
Custom attributes
Examples
Basic example
Modify voice AI agent based on caller attributes
Creating a SIP participant to make outbound calls


Content from https://docs.livekit.io/sip/api:

On this page
Overview
Using endpoints
SIPService APIs
CreateSIPInboundTrunk
CreateSIPOutboundTrunk
CreateSIPDispatchRule
CreateSIPParticipant
DeleteSIPDispatchRule
DeleteSIPTrunk
GetSIPInboundTrunk
GetSIPOutboundTrunk
ListSIPDispatchRule
ListSIPInboundTrunk
ListSIPOutboundTrunk
TransferSIPParticipant
UpdateSIPDispatchRule
UpdateSIPInboundTrunk
UpdateSIPOutboundTrunk
Types
GetSIPInboundTrunkResponse
GetSIPOutboundTrunkResponse
SIPDispatchRule
SIPHeaderOptions
SIPDispatchRuleInfo
SIPDispatchRuleUpdate
SIPInboundTrunkInfo
SIPInboundTrunkUpdate
SIPOutboundTrunkInfo
SIPOutboundTrunkUpdate
SIPParticipantInfo
SIPMediaEncryption
SIPTransport
SIPTrunkInfo
TrunkKind
UpdateSIPDispatchRuleRequest
UpdateSIPInboundTrunkRequest
UpdateSIPOutboundTrunkRequest
Copy page
See more page options
Overview
LiveKit has built-in APIs that let you to manage SIP trunks, dispatch rules, and SIP participants. The SIP API is
available with LiveKit server SDKs and CLI:
Go SIP client
JS SIP client
Ruby SIP client
Python SIP client
Java SIP client
CLI
Important
Requests to the SIP API require the SIP
admin
permission unless otherwise noted. To create a token with the appropriate
grant, see
SIP grant
.
To learn more about additional APIs, see
Server APIs
.
Using endpoints
The SIP API is accessible via
/twirp/livekit.SIP/<MethodName>
. For example, if you're using LiveKit Cloud the
following URL is for the
CreateSIPInboundTrunk
API endpoint:
https://
<
your LiveKit URL domain
>
/twirp/livekit.SIP/CreateSIPInboundTrunk
Authorization header
All endpoints require a signed access token. This token should be set via HTTP header:
Authorization
:
Bearer
<
token
>
LiveKit server SDKs automatically include the above header.
Post body
Twirp expects an HTTP POST request. The body of the request must be
a JSON object (
application/json
) containing parameters specific to that request. Use an empty
{}
body for requests that don't require parameters.
Examples
For example, create an inbound trunk using
CreateSIPInboundTrunk
:
curl
-X
POST https://
<
your LiveKit URL domain
>
/twirp/livekit.SIP/CreateSIPInboundTrunk
\
-H
"Authorization: Bearer <token-with-sip-admin>"
\
-H
'Content-Type: application/json'
\
-d
'{ "name": "My trunk", "numbers": ["+15105550100"] }'
List inbound trunks using
ListSIPInboundTrunk
API endpoint to list inbound trunks:
curl
-X
POST https://
<
your LiveKit URL domain
>
/twirp/livekit.SIP/ListSIPInboundTrunk
\
-H
"Authorization: Bearer <token-with-sip-admin>"
\
-H
'Content-Type: application/json'
\
-d
'{}'
SIPService APIs
The SIPService APIs allow you to manage trunks, dispatch rules, and SIP participants.
Tip
All RPC definitions and options can be found
here
.
CreateSIPInboundTrunk
Create an inbound trunk with the specified settings.
Returns
SIPInboundTrunkInfo
.
Parameter
Type
Required
Description
name
string
yes
name of the trunk.
metadata
string
Initial metadata to assign to the trunk. This metadata is added to every SIP participant that uses the trunk.
numbers
array<string>
yes
Array of provider phone numbers associated with the trunk.
allowed_addresses
array<string>
List of IP addresses that are allowed to use the trunk. Each item in the list can be an individual
IP address or a Classless Inter-Domain Routing notation representing a CIDR block.
allowed_numbers
array<string>
List of phone numbers that are allowed to use the trunk.
auth_username
string
If configured, the username for authorized use of the provider's SIP trunk.
auth_password
string
If configured, the password for authorized use of the provider's SIP trunk.
headers
map<string, string>
SIP X-* headers for INVITE request. These headers are sent as-is and may help identify
this call as coming from LiveKit for the other SIP endpoint.
headers_to_attributes
map<string, string>
Key-value mapping of SIP X-* header names to participant attribute names.
attributes_to_headers
map<string, string>
Map SIP headers from INVITE request to
sip.h.*
participant attributes. If the names of the required headers is known,
use
headers_to_attributes
instead.
include_headers
SIPHeaderOptions
Specify how SIP headers should be mapped to attributes.
ringing_timeout
google.protobuf.Duration
Maximum time for the call to ring.
max_call_duration
google.protobuf.Duration
Maximum call duration.
krisp_enabled
bool
True to enable
Krisp noise cancellation
for the caller.
media_encryption
SIPMediaEncryption
Whether or not to encrypt media.
CreateSIPOutboundTrunk
Create an outbound trunk with the specified settings.
Returns
SIPOutboundTrunkInfo
.
Parameter
Type
Required
Description
name
string
yes
name of the trunk.
metadata
string
User-defined metadata for the trunk. This metadata is added to every SIP participant that uses the trunk.
address
string
yes
Hostname or IP the SIP INVITE is sent to. This is
not
a SIP URI and shouldn't contain the
sip:
protocol.
destination_country
string
yes
Two letter
country code
for the country the call terminates in. LiveKit uses the country code to route calls. To learn more, see
Restricting calls to a region
.
numbers
array<string>
yes
List of provider phone numbers associated with the trunk that can be used as a caller id.
transport
SIPTransport
Protocol to use for SIP transport: auto, TCP, or UDP.
auth_username
string
If configured, the username for authorized use of the provider's SIP trunk.
auth_password
string
If configured, the password for authorized use of the provider's SIP trunk.
headers
map<string, string>
SIP X-* headers for INVITE request. These headers are sent as-is and may help identify
this call as coming from LiveKit for the other SIP endpoint.
headers_to_attributes
map<string, string>
Key-value mapping of SIP X-* header names to participant attribute names.
media_encryption
SIPMediaEncryption
Whether or not to encrypt media.
CreateSIPDispatchRule
Create dispatch rule.
Returns
SIPDispatchRuleInfo
.
Parameter
Type
Required
Description
dispatch_rule
SIPDispatchRuleInfo
yes
Dispatch rule to create.
trunk_ids
array<string>
List of associated trunk IDs. If empty, all trunks match this dispatch rule.
hide_phone_number
bool
If true, use a random value for participant identity and phone number ommitted from attributes.
By default, the participant identity is created using the phone number (if the participant identity isn't explicitly set).
inbound_numbers
array<string>
If set, the dispatch rule only accepts calls made to numbers in the list.
name
string
yes
Human-readable name for the dispatch rule.
metadata
string
Optional metadata for the dispatch rule. If defined, participants created by the rule inherit this metadata.
attributes
map<string, string>
Key-value mapping of user-defined attributes. Participants created by this rule inherit these attributes.
room_preset
string
Only for LiveKit Cloud: Config preset to use.
room_config
RoomConfiguration
Room configuration to use if the participant initiates the room.
CreateSIPParticipant
Note
Requires SIP
call
grant on authorization token.
Create a SIP participant to make outgoing calls.
Returns
SIPParticipantInfo
Parameter
Type
Required
Description
sip_trunk_id
string
yes
ID for SIP trunk used to dial user.
sip_call_to
string
yes
Phone number to call.
sip_number
string
SIP number to call from. If empty, use trunk number.
room_name
string
yes
Name of the room to connect the participant to.
participant_identity
string
Identity of the participant.
participant_name
string
Name of the participant.
participant_metadata
string
User-defined metadata that is attached to created participant.
participant_attributes
map<string, string>
Key-value mapping of user-defined attributes to attach to created participant.
dtmf
string
DTMF digits (extension codes) to use when making the call. Use character
w
to add a 0.5 second delay.
play_dialtone
bool
Optionally play dial tone in the room in the room as an audible indicator for existing participants.
hide_phone_number
bool
If true, use a random value for participant identity and phone number ommitted from attributes.
By default, the participant identity is created using the phone number (if the participant identity isn't explicitly set).
headers
map<string, string>
SIP X-* headers for INVITE request. These headers are sent as-is and may help identify
this call as coming from LiveKit.
include_headers
SIPHeaderOptions
Specify how SIP headers should be mapped to attributes.
ringing_timeout
google.protobuf.Duration
Maximum time for the callee to answer the call.
max_call_duration
google.protobuf.Duration
Maximum call duration.
krisp_enabled
bool
True to enable
Krisp noise cancellation
for the callee.
media_encryption
SIPMediaEncryption
Whether or not to encrypt media.
wait_until_answered
bool
If true, return after the call is answered — including if it goes to voicemail.
DeleteSIPDispatchRule
Delete a dispatch rule.
Returns
SIPDispatchRuleInfo
.
Parameter
Type
Required
Description
sip_dispatch_rule_id
string
ID of dispatch rule.
DeleteSIPTrunk
Delete a trunk.
Returns
SIPTrunkInfo
.
Parameter
Type
Required
Description
sip_trunk_id
string
yes
ID of trunk.
GetSIPInboundTrunk
Get inbound trunk.
Returns
GetSIPInboundTrunkResponse
.
Parameter
Type
Required
Description
sip_trunk_id
string
yes
ID of trunk.
GetSIPOutboundTrunk
Get outbound trunk.
Returns
GetSIPOutboundTrunkResponse
.
Parameter
Type
Required
Description
sip_trunk_id
string
yes
ID of trunk.
ListSIPDispatchRule
List dispatch rules.
Returns array<
SIPDispatchRuleInfo
>.
ListSIPInboundTrunk
List inbound trunks.
Returns array<
SIPInboundTrunkInfo
>.
ListSIPOutboundTrunk
List outbound trunks.
Returns array<
SIPOutboundTrunkInfo
>.
TransferSIPParticipant
Note
Requires SIP
call
grant on authorization token.
Transfer call to another number or SIP endpoint.
Returns
google.protobuf.Empty
.
Parameter
Type
Required
Description
participant_identity
string
yes
Identity of the participant to transfer.
room_name
string
yes
Name of the room the participant is currently in.
transfer_to
string
yes
Phone number or SIP endpoint to transfer participant to.
play_dialtone
bool
Optionally play dial tone during the transfer. By default, the room audio is played during the transfer.
UpdateSIPDispatchRule
Update a dispatch rule.
Returns
SIPDispatchRuleInfo
.
Parameter
Type
Required
Description
req
UpdateSIPDispatchRuleRequest
yes
Update or replace request.
UpdateSIPInboundTrunk
Update an inbound trunk.
Returns
SIPInboundTrunkInfo
.
Parameter
Type
Required
Description
req
UpdateSIPInboundTrunkRequest
yes
Update or replace request.
UpdateSIPOutboundTrunk
Update an outbound trunk.
Returns
SIPOutboundTrunkInfo
.
Parameter
Type
Required
Description
req
UpdateSIPOutboundTrunkRequest
yes
Update or replace request.
Types
The SIP service includes the following types.
GetSIPInboundTrunkResponse
Field
Type
Description
trunk
SIPInboundTrunkInfo
Inbound trunk.
GetSIPOutboundTrunkResponse
Field
Type
Description
trunk
SIPOutboundTrunkInfo
Outbound trunk.
SIPDispatchRule
Valid values include:
Name
Type
Value
Description
dispatch_rule_direct
SIPDispatchRuleDirect
1
Dispatches callers into an existing room. You can optionally require a pin before caller enters the room.
dispatch_rule_individual
SIPDispatchRuleIndividual
2
Creates a new room for each caller.
dispatch_rule_callee
SIPDispatchRuleCallee
3
Creates a new room for each callee.
SIPHeaderOptions
Enum. Valid values are as follows:
Name
Value
Description
SIP_NO_HEADERS
0
Don't map any headers except those mapped explicitly.
SIP_X_HEADERS
1
Map all
X-*
headers to
sip.h.*
attributes.
SIP_ALL_HEADERS
2
Map all headers to
sip.h.*
attributes.
SIPDispatchRuleInfo
Field
Type
Description
sip_dispatch_rule_id
string
Dispatch rule ID.
rule
SIPDispatchRule
Type of dispatch rule.
trunk_ids
array<string>
List of associated trunk IDs.
hide_phone_number
bool
If true, hides phone number.
inbound_numbers
array<string>
If this list is included, the dispatch rule only accepts calls made to the numbers in the list.
name
string
Human-readable name for the dispatch rule.
metadata
string
User-defined metadata for the dispatch rule. Participants created by this rule inherit this metadata.
headers
map<string, string>
Custom SIP X-* headers to included in the 200 OK response.
attributes
map<string, string>
Key-value mapping of user-defined attributes. Participants created by this rule inherit these attributes.
room_preset
string
Only for LiveKit Cloud: Config preset to use.
room_config
RoomConfiguration
Room configuration object associated with the dispatch rule.
SIPDispatchRuleUpdate
Field
Type
Description
trunk_ids
array<string>
List of trunk IDs to associate with the dispatch rule.
rule
SIPDispatchRule
Type of dispatch rule.
name
string
Human-readable name for the dispatch rule.
metadata
string
User-defined metadata for the dispatch rule. Participants created by this rule inherit this metadata.
attributes
map<string, string>
Key-value mapping of user-defined attributes. Participants created by this rule inherit these attributes.
media_encryption
SIPMediaEncryption
Whether or not to encrypt media.
SIPInboundTrunkInfo
Field
Type
Description
sip_trunk_id
string
Trunk ID
name
string
Human-readable name for the trunk.
numbers
array<string>
Phone numbers associated with the trunk. The trunk only accepts calls made to the phone numbers in the list.
allowed_addresses
array<string>
IP addresses or CIDR blocks that are allowed to use the trunk. If this list is populated, the trunk only
accepts traffic from the IP addresses in the list.
allowed_numbers
array<string>
Phone numbers that are allowed to dial in. If this list is populated, the trunk only accepts calls from
the numbers in the list.
auth_username
string
Username used to authenticate inbound SIP invites.
auth_password
string
Password used to authenticate inbound SIP invites.
headers
map<string, string>
Custom SIP X-* headers to included in the 200 OK response.
headers_to_attributes
map<string, string>
Custom SIP X-* headers that map to SIP participant attributes.
ringing_timeout
google.protobuf.Duration
Maximum time for the caller to wait for track subscription (that is, for the call to be picked up).
max_call_duration
google.protobuf.Duration
Maximum call duration.
krisp_enabled
Boolean
True if Krisp noise cancellation is enabled for the call.
SIPInboundTrunkUpdate
Field
Type
Description
numbers
list[<string>]
List of phone numbers associated with the trunk.
allowed_addresses
list[<string>]
List of IP addresses or CIDR blocks that are allowed to use the trunk.
allowed_numbers
list[<string>]
List of phone numbers that are allowed to use the trunk.
auth_username
string
Username used to authenticate inbound SIP invites.
auth_password
string
Password used to authenticate inbound SIP invites.
name
string
Human-readable name for the trunk.
metadata
string
User-defined metadata for the trunk.
media_encryption
SIPMediaEncryption
Whether or not to encrypt media.
SIPOutboundTrunkInfo
Field
Type
Description
sip_trunk_id
string
Trunk ID.
name
string
Trunk name.
metadata
string
User-defined metadata for trunk.
address
string
Hostname or IP address the SIP request message (SIP INVITE) is sent to.
destination_country
string
Two letter
country code
for the country the call terminates in. LiveKit uses the country code to route calls. To learn more, see
Restricting calls to a region
.
transport
SIPTransport
Protocol to use for SIP transport: auto, TCP, or UDP.
numbers
array<string>
Phone numbers used to make calls. A random number in the list is selected whenever a call is made.
auth_username
string
Username used to authenticate with the SIP server.
auth_password
string
Password used to authenticate with the SIP server.
headers
map<string, string>
Custom SIP X-* headers to included in the 200 OK response.
headers_to_attributes
map<string, string>
Custom SIP X-* headers that map to SIP participant attributes.
SIPOutboundTrunkUpdate
Field
Type
Description
address
string
Hostname or IP address the SIP request message (SIP INVITE) is sent to.
transport
SIPTransport
Protocol to use for SIP transport: auto, TCP, or UDP.
destination_country
string
Two letter
country code
for the country the call terminates in. LiveKit uses the country code to route calls. To learn more, see
Restricting calls to a region
.
numbers
array<string>
Phone numbers used to make calls. A random number in the list is selected whenever a call is made.
auth_username
string
Username used to authenticate with the SIP server.
auth_password
string
Password used to authenticate with the SIP server.
name
string
Human-readable name for the trunk.
metadata
string
User-defined metadata for the trunk.
media_encryption
SIPMediaEncryption
Whether or not to encrypt media.
SIPParticipantInfo
Field
Type
Description
participant_id
string
Participant ID.
participant_identity
string
Participant name.
room_name
string
Name of the room.
sip_call_id
string
SIP call ID.
SIPMediaEncryption
Enum. Valid values are as follows:
Name
Value
Description
SIP_MEDIA_ENCRYPT_DISABLE
0
Don't turn on encryption.
SIP_MEDIA_ENCRYPT_ALLOW
1
Use encryption if available.
SIP_MEDIA_ENCRYPT_REQUIRE
2
Require encryption.
SIPTransport
Enum. Valid values are as follows:
Name
Value
Description
SIP_TRANSPORT_AUTO
0
Detect automatically.
SIP_TRANSPORT_UDP
1
UDP
SIP_TRANSPORT_TCP
2
TCP
SIP_TRANSPORT_TLS
3
TLS
SIPTrunkInfo
Note
This type is deprecated. See
SIPInboundTrunkInfo
and
SIPOutboundTrunkInfo
.
Field
Type
Description
sip_trunk_id
string
Trunk ID.
kind
TrunkKind
Type of trunk.
inbound_addresses
array<string>
IP addresses or CIDR blocks that are allowed to use the trunk. If this list is populated, the trunk only
accepts traffic from the IP addresses in the list.
outbound_address
string
IP address that the SIP INVITE is sent to.
outbound_number
string
Phone number used to make outbound calls.
transport
SIPTransport
Protocol to use for SIP transport: auto, TCP, or UDP.
inbound_numbers
array<string>
If this list is populated, the trunk only accepts calls to the numbers in this list.
inbound_username
string
Username used to authenticate inbound SIP invites.
inbound_password
string
Password used to authenticate inbound SIP invites.
outbound_username
string
Username used to authenticate outbound SIP invites.
outbound_password
string
Password used to authenticate outbound SIP invites.
name
string
Trunk name.
metadata
string
Initial metadata to assign to the trunk. This metadata is added to every SIP participant that uses the trunk.
TrunkKind
Enum. Valid values are as follows:
Name
Value
Description
TRUNK_LEGACY
0
Legacy trunk.
TRUNK_INBOUND
1
Inbound trunk
.
TRUNK_OUTBOUND
2
Outbound trunk
.
UpdateSIPDispatchRuleRequest
Field
Type
Description
sip_dispatch_rule_id
string
Dispatch rule ID.
action
SIPDispatchRule
|
SIPDispatchRuleUpdate
Dispatch rule for replacement or update.
UpdateSIPInboundTrunkRequest
Field
Type
Description
sip_trunk_id
string
Trunk ID.
action
SIPInboundTrunkInfo
|
SIPInboundTrunkUpdate
Trunk info for replacement or update.
UpdateSIPOutboundTrunkRequest
Field
Type
Description
sip_trunk_id
string
Trunk ID.
action
SIPOutboundTrunkInfo
|
SIPOutboundTrunkUpdate
Trunk info for replacement or update.
On this page
Overview
Using endpoints
SIPService APIs
CreateSIPInboundTrunk
CreateSIPOutboundTrunk
CreateSIPDispatchRule
CreateSIPParticipant
DeleteSIPDispatchRule
DeleteSIPTrunk
GetSIPInboundTrunk
GetSIPOutboundTrunk
ListSIPDispatchRule
ListSIPInboundTrunk
ListSIPOutboundTrunk
TransferSIPParticipant
UpdateSIPDispatchRule
UpdateSIPInboundTrunk
UpdateSIPOutboundTrunk
Types
GetSIPInboundTrunkResponse
GetSIPOutboundTrunkResponse
SIPDispatchRule
SIPHeaderOptions
SIPDispatchRuleInfo
SIPDispatchRuleUpdate
SIPInboundTrunkInfo
SIPInboundTrunkUpdate
SIPOutboundTrunkInfo
SIPOutboundTrunkUpdate
SIPParticipantInfo
SIPMediaEncryption
SIPTransport
SIPTrunkInfo
TrunkKind
UpdateSIPDispatchRuleRequest
UpdateSIPInboundTrunkRequest
UpdateSIPOutboundTrunkRequest


Content from https://docs.livekit.io/agents/v0/voice-agent/:

On this page
Features
Multimodal or voice pipeline
Handling background noise
Turn detection
Voice activity detection (VAD)
Turn detection model
Agent state
Transcriptions
Copy page
See more page options
Agents 1.0 available for Python
This documentation is for v0.x of the LiveKit Agents framework.
See updated documentation here:
Building voice agents
.
v1.0 for Node.js is coming soon.
Companies like OpenAI, Character.ai, Retell, and Speak have built their conversational AI products on the LiveKit platform. AI voice agents are one of the primary use cases for LiveKit's Agents framework.
Features
Programmable conversation flows
Integrated LLM function calls
Provide context to the conversation via RAG
Leverage connectors from an open-source plugin ecosystem
Send synchronized transcriptions to your frontend
Multimodal or voice pipeline
LiveKit offers two types of voice agents:
MultimodalAgent
and
VoicePipelineAgent
.
MultimodalAgent
uses OpenAI’s multimodal model and realtime API to directly process user audio and generate audio responses, similar to OpenAI’s advanced voice mode, producing more natural-sounding speech.
VoicePipelineAgent
uses a pipeline of STT, LLM, and TTS models, providing greater control over the conversation flow by allowing applications to modify the text returned by the LLM.
Multimodal
Voice pipeline
Python
✅
✅
Node.JS
✅
✅
Model type
single multimodal
STT, LLM, TTS
Function calling
✅
✅
RAG
via function calling
✅
Natural speech
more natural
Modify LLM response
✅
Model vendors
OpenAI
various
Turn detection
VAD
VAD and turn detection model
Handling background noise
While humans can easily ignore background noise, AI models often struggle, leading to misinterpretations or unnecessary pauses when detecting non-speech sounds. Although WebRTC includes built-in noise suppression, it often falls short in real-world environments.
To address this, LiveKit has partnered with
Krisp
to bring best-in-class noise suppression technology to AI agents.
For instructions on enabling Krisp, see
Krisp integration guide
Turn detection
Endpointing
is the process of detecting the start and end of speech in an audio stream. This is crucial for conversational AI agents to understand when a user has finished speaking and when to start responding to user input.
Determining the end of a turn is particularly challenging for AI agents. Humans rely on multiple cues, such as pauses, speech tone, and content, to recognize when someone has finished speaking.
LiveKit employs two primary strategies to approximate how humans determine turn boundaries:
Voice activity detection (VAD)
LiveKit Agents uses VAD to detect when the user has finished speaking. The agent waits for a minimum duration of silence before considering the turn complete.
Both VoicePipelineAgent and MultimodalAgent use VAD for turn detection.
For OpenAI Multimodal configuration, refer to the
MultimodalAgent turn detection
docs.
VoicePipelineAgent uses Silero VAD to detect end of speech. The
min_endpointing_delay
parameter in the agent constructor specifies the minimum silence duration to consider the end of a turn.
Turn detection model
While VAD provides a simple approximation of turn completion, it lacks contextual awareness. In natural conversations, pauses often occur as people think or formulate responses.
To address this, LiveKit has developed a custom, open-weights language model to incorporate conversational context as an additional signal to VAD.
The
turn-detector
plugin uses this model to predict whether a user is done speaking.
When the model predicts that the user is
not done
with their turn, the agent will wait for a significantly longer period of silence before responding. This helps to prevent unwanted interruptions during natural pauses in speech.
Here's
a demo
of the model in action.
Benchmarks
In our testing, the turn detector model demonstrated the following performance:
85% true positive rate
: avoids early interruptions by correctly identifying when the user is not done speaking.
97% true negative rate
: accurately determines the end of a turn when the user has finished speaking.
Using turn detector
Currently, this model is supported for
VoicePipelineAgent
in Python. To use it, install the
livekit-plugins-turn-detector
package.
Then, initialize the agent with the turn detector:
from
livekit
.
plugins
import
turn_detector
agent
=
VoicePipelineAgent
(
.
.
.
turn_detector
=
turn_detector
.
EOUModel
(
)
,
)
Before running the agent for the first time, download the model weights:
python my_agent.py download-files
Agent state
Voice agents automatically publish their current state to your frontend, making it easy to build UI that reflects the agent’s status.
The state is passed to your frontend as a
participant attribute
on the agent participant. Components like
useVoiceAssistant
expose the following states:
disconnected
: either agent or user is disconnected
connecting
: agent is being connected with the user
initializing
: agent is connected, but not yet ready
listening
: agent is listening for user input
thinking
: agent is performing inference on user input
speaking
: agent is playing out a response
Transcriptions
LiveKit provides realtime transcriptions for both the agent and the user, which are sent to your frontend via the
transcription protocol
.
User speech transcriptions are delivered as soon as they are processed by STT. Since the agent’s text response is available before speech synthesis, we manually synchronize the text transcription with audio playback.
On this page
Features
Multimodal or voice pipeline
Handling background noise
Turn detection
Voice activity detection (VAD)
Turn detection model
Agent state
Transcriptions


Content from https://docs.livekit.io/agents/v0/deployment:

On this page
Deployment best practices
Networking
Environment variables
Storage
Memory and CPU
Rollout
Load balancing
Worker availability
Autoscaling
Where to deploy
Recommendations
Copy page
See more page options
Agents 1.0 available for Python
This documentation is for v0.x of the LiveKit Agents framework.
See updated documentation here:
Deploying to production
.
v1.0 for Node.js is coming soon.
Agent workers are designed to be deployable and scalable by default.
When you start your worker locally,
you are adding it to a pool (of one). LiveKit automatically load balances across
available workers in the pool.
This works the same way in production,
the only difference is the worker is deployed somewhere with more than
one running instance.
Deployment best practices
Networking
Workers communicate with LiveKit and accept incoming jobs via a WebSocket connection to a LiveKit server.
This means that workers are not web servers and do not need to be exposed to the public internet.
No inbound hosts or ports need to be exposed.
Workers can optionally expose a health check endpoint for monitoring purposes. This is not required for normal operation.
The default health check server listens on
http://0.0.0.0:8081/
.
Environment variables
Your production worker will need certain environment variables configured.
A minimal worker requires the LiveKit URL, API key and secret:
LIVEKIT_URL
LIVEKIT_API_KEY
LIVEKIT_API_SECRET
Depending on the plugins your agent uses, you might need additional environment variables:
DEEPGRAM_API_KEY
CARTESIA_API_KEY
OPENAI_API_KEY
etc.
Important
If you use a
LIVEKIT_URL
,
LIVEKIT_API_KEY
, and
LIVEKIT_API_SECRET
from the same project that you use
for local development, your local worker will join the same pool as your production workers.
This means real users could be connected to your local worker.
This is usually not what you want so make sure to use
a different project for local development.
Storage
Workers are stateless and do not require persistent storage. The minimal docker image is about 1GB in size.
For ephemeral storage, 10GB should be more than enough to account for the docker image size and
any temporary files that are created.
Memory and CPU
Different agents will have different memory and CPU requirements. To help guide your scaling decisions, we ran a load test that approximates the load of a
voice-to-voice session on a 4-Core, 8GB machine.
Tip
During the automated load test we also added one human participant interacting with a
voice assistant agent to make sure quality of service was maintained.
This test created 30 agents corresponding to 30 users (so 60 participants in total).
The users published looping speech audio. The agents were subscribed to their corresponding
user's audio and running the Silero voice activity detection plugin against that audio.
The agents also published their own audio which was a simple sine wave.
In short, this test was designed to evaluate a voice assistant use case where the agent is
listening to user speech, running VAD, and publishing audio back to the user.
The results of running the above test on a 4-Core, 8GB machine are:
CPU Usage: ~3.8 cores
Memory usage: ~2.8GB
To be safe and account for spikes, we recommend 4 cores for every 25 voice agents.
Rollout
Workers stop accepting jobs when they receive a SIGINT or SIGTERM. Agents that are still running
on the worker continue to run. It's important that you configure a large enough grace period
for your containers to allow agents to finish.
Voice agents could require a 10+ minute grace period to allow for conversations to finish.
Different deployment platforms have different ways of setting this grace period.
In Kubernetes, it's the
terminationGracePeriodSeconds
field in the pod spec.
Consult your deployment platform's documentation for more information.
Load balancing
Workers don't need an external load balancer.
They rely on a job distribution system embedded within LiveKit servers.
This system is responsible for ensuring that when a job becomes available
(e.g. a new room is created), it is dispatched to only one worker at a time.
If a worker fails to accept the job within a predetermined timeout period,
the job is routed to another available worker.
In the case of LiveKit Cloud, the system prioritizes available workers at the "edge"
or geographically closest to the end-user. Within a region, job distribution
is uniform across workers.
Worker availability
As mentioned in the Load Balancing section, LiveKit will automatically distribute load
across available workers. This means that LiveKit needs a way to know which workers are available.
This "worker availability" is defined by the
load_fnc
and
load_threshold
in the
WorkerOptions
configuration.
The
load_fnc
returns a value between 0 and 1, indicating how busy the worker is while
load_threshold
, a value between 0 and 1, is that load value at which the worker will stop
accepting new jobs.
By default, the
load_fnc
returns the CPU usage of the worker and the
load_threshold
is 0.75.
Autoscaling
Many voice agent use cases have non-uniform load patterns over a day/week/month so it's a good idea
to configure an autoscaler.
An autoscaler should be configured at a
lower
threshold than the worker's
load_threshold
. This allows for existing workers to continue to accept new jobs
while additional workers are still starting up.
Since voice agents are typically long running tasks (relative to typical web requests), rapid
increases in load are more likely to be sustained. In technical terms: spikes are less spikey. For your
autoscaling configuration, you should consider
reducing
cooldown/stabilization periods when scaling
up. When scaling down, consider
increasing
cooldown/stabilization periods because workers take time to
drain.
For example, if deploying on Kubernetes using a Horizontal Pod Autoscaler,
see
stabilizationWindowSeconds
.
Where to deploy
There are many ways to deploy software to a production environment.
We provide some platform-specific
deployment examples
.
All of the examples assume Docker containers are used for deployment.
Recommendations
Render.com
: We (and other builders in the LiveKit community) have found Render.com to
be the easiest way to deploy and autoscale workers. We provide an example
render.yaml
and
instructions in the
deployment examples
repo.
Kubernetes
: If you have a running Kubernetes cluster, it makes sense to deploy your workers there.
We provide an example manifest in the deployment example repo.
On this page
Deployment best practices
Networking
Environment variables
Storage
Memory and CPU
Rollout
Load balancing
Worker availability
Autoscaling
Where to deploy
Recommendations


Content from https://docs.livekit.io/agents/v0/playground:

On this page
Using the hosted playground
Self-hosted playground
1. Clone the project
2. Create environment variables
3. Customizing configuration
4. Run the playground
5. Deploying the playground
Copy page
See more page options
Agents 1.0 available for Python
This documentation is for v0.x of the LiveKit Agents framework.
See updated documentation here:
Agents playground
.
v1.0 for Node.js is coming soon.
To ease the process of building and testing an agent, we've developed a versatile web frontend called "playground". This app is available for use or customization according to your specific requirements. It can also serve as a starting point for
your application.
Playground source code:
https://github.com/livekit/agents-playground/
Built with Next.js and
LiveKit Components
.
Using the hosted playground
A hosted version of the playground is available at
https://agents-playground.livekit.io
. This version is compatible with any LiveKit server instance (including one running locally). To use it, you'll need to input your LiveKit server's URL and a corresponding access token.
Self-hosted playground
When you run your own version of the playground, it can be configured to automatically generate access tokens for your users. This setup streamlines the user experience, removing the need for manual token entry.
1. Clone the project
git
clone https://github.com/livekit/agents-playground.git
2. Create environment variables
Create an
.env.local
file in the root of your project folder with the following variables:
LIVEKIT_API_KEY
=
YOUR_API_KEY
LIVEKIT_API_SECRET
=
YOUR_API_SECRET
# Public configuration
NEXT_PUBLIC_LIVEKIT_URL
=
wss://YOUR_LIVEKIT_URL
NEXT_PUBLIC_APP_CONFIG
=
"
title: 'LiveKit Agent Playground'
description: 'LiveKit Agent Playground allows you to test your LiveKit Agent integration by connecting to your LiveKit Cloud or self-hosted instance.'
github_link: 'https://github.com/livekit/agents-playground'
video_fit: 'cover' # 'contain' or 'cover'
settings:
editable: true # Should the user be able to edit settings in-app
theme_color: 'cyan'
chat: true  # Enable or disable chat feature
outputs:
audio: true # Enable or disable audio output
video: true # Enable or disable video output
inputs:
mic: true    # Enable or disable microphone input
camera: true # Enable or disable camera input
sip: true    # Enable or disable SIP input
"
3. Customizing configuration
NEXT_PUBLIC_APP_CONFIG
is a configurable YAML string designed to tailor the playground's capabilities. By default, the playground is set up to publish both the
user's camera and microphone when they connect to a room. However if, for example, your agent doesn't need vision, you can disable the camera by setting
inputs.camera
to
false
.
Similarly, agents have varying output requirements. If your agent does not provide voice/audio feedback to the user, you can set
outputs.audio
to
false
. This adjustment will consequently remove the related audio component from the playground UI.
4. Run the playground
That's it! You're ready to run your own version of the playground.
npm
install
npm
run dev
5. Deploying the playground
To deploy your version of the playground, you can use any hosting provider that supports Node.js.
We're hosting the public version of it on
Vercel
.
Deploy with Vercel
On this page
Using the hosted playground
Self-hosted playground
1. Clone the project
2. Create environment variables
3. Customizing configuration
4. Run the playground
5. Deploying the playground


Content from https://docs.livekit.io/agents/start/voice-ai:

On this page
Overview
Requirements
Python
LiveKit server
AI providers
Setup
Packages
Environment variables
Agent code
Download model files
Speak to your agent
Connect to playground
Next steps
Copy page
See more page options
Overview
This guide walks you through the setup of your very first voice assistant using LiveKit Agents for Python. In less than 10 minutes, you'll have a voice assistant that you can speak to in your terminal, browser, telephone, or native app.
Python starter project
Prefer to just clone a repo? This repo is ready-to-go, will all the code you need to get started.
GitHub
livekit-examples/agent-starter-python
Deeplearning.ai course
Learn to build and deploy voice agents with LiveKit in this free course from Deeplearning.ai.
Requirements
The following sections describe the minimum requirements to get started with LiveKit Agents.
Python
LiveKit Agents requires Python 3.9 or later.
Looking for Node.js?
The Node.js beta is still in development and has not yet reached v1.0. See the
v0.x documentation
for Node.js reference and join the
LiveKit Community Slack
to be the first to know when the next release is available.
LiveKit server
You need a LiveKit server instance to transport realtime media between user and agent. The easiest way to get started is with a free
LiveKit Cloud
account. Create a project and use the API keys in the following steps. You may also
self-host LiveKit
if you prefer.
AI providers
LiveKit Agents
integrates with most AI model providers
and supports both high-performance STT-LLM-TTS voice pipelines, as well as lifelike multimodal models.
The rest of this guide assumes you use one of the following two starter packs, which provide the best combination of value, features, and ease of setup.
STT-LLM-TTS pipeline
Realtime model
Your agent strings together three specialized providers into a high-performance voice pipeline. You need accounts and API keys for each.
Component
Provider
Required Key
Alternatives
STT
Deepgram
DEEPGRAM_API_KEY
STT integrations
LLM
OpenAI
OPENAI_API_KEY
LLM integrations
TTS
Cartesia
CARTESIA_API_KEY
TTS integrations
Setup
Use the instructions in the following sections to set up your new project.
Packages
Noise cancellation
This example integrates LiveKit Cloud
enhanced background voice/noise cancellation
, powered by Krisp.
If you're not using LiveKit Cloud, omit the plugin and the
noise_cancellation
parameter from the following code.
For telephony applications, use the
BVCTelephony
model for the best results.
STT-LLM-TTS pipeline
Realtime model
Install the following packages to build a complete voice AI agent with your STT-LLM-TTS pipeline, noise cancellation, and
turn detection
:
pip
install
\
"livekit-agents[deepgram,openai,cartesia,silero,turn-detector]~=1.0"
\
"livekit-plugins-noise-cancellation~=0.2"
\
"python-dotenv"
Environment variables
Create a file named
.env
and add your LiveKit credentials along with the necessary API keys for your AI providers.
STT-LLM-TTS pipeline
Realtime model
.env
DEEPGRAM_API_KEY
=
<
Your Deepgram API Key
>
OPENAI_API_KEY
=
<
Your OpenAI API Key
>
CARTESIA_API_KEY
=
<
Your Cartesia API Key
>
LIVEKIT_API_KEY
=
<
your API Key
>
LIVEKIT_API_SECRET
=
<
your API Secret
>
LIVEKIT_URL
=
<
your LiveKit server URL
>
Reveal API Key and Secret
Agent code
Create a file named
agent.py
containing the following code for your first voice agent.
STT-LLM-TTS pipeline
Realtime model
agent.py
from
dotenv
import
load_dotenv
from
livekit
import
agents
from
livekit
.
agents
import
AgentSession
,
Agent
,
RoomInputOptions
from
livekit
.
plugins
import
(
openai
,
cartesia
,
deepgram
,
noise_cancellation
,
silero
,
)
from
livekit
.
plugins
.
turn_detector
.
multilingual
import
MultilingualModel
load_dotenv
(
)
class
Assistant
(
Agent
)
:
def
__init__
(
self
)
-
>
None
:
super
(
)
.
__init__
(
instructions
=
"You are a helpful voice AI assistant."
)
async
def
entrypoint
(
ctx
:
agents
.
JobContext
)
:
session
=
AgentSession
(
stt
=
deepgram
.
STT
(
model
=
"nova-3"
,
language
=
"multi"
)
,
llm
=
openai
.
LLM
(
model
=
"gpt-4o-mini"
)
,
tts
=
cartesia
.
TTS
(
model
=
"sonic-2"
,
voice
=
"f786b574-daa5-4673-aa0c-cbe3e8534c02"
)
,
vad
=
silero
.
VAD
.
load
(
)
,
turn_detection
=
MultilingualModel
(
)
,
)
await
session
.
start
(
room
=
ctx
.
room
,
agent
=
Assistant
(
)
,
room_input_options
=
RoomInputOptions
(
# LiveKit Cloud enhanced noise cancellation
# - If self-hosting, omit this parameter
# - For telephony applications, use `BVCTelephony` for best results
noise_cancellation
=
noise_cancellation
.
BVC
(
)
,
)
,
)
await
session
.
generate_reply
(
instructions
=
"Greet the user and offer your assistance."
)
if
__name__
==
"__main__"
:
agents
.
cli
.
run_app
(
agents
.
WorkerOptions
(
entrypoint_fnc
=
entrypoint
)
)
Download model files
To use the
turn-detector
,
silero
, or
noise-cancellation
plugins, you first need to download the model files:
python agent.py download-files
Speak to your agent
Start your agent in
console
mode to run inside your terminal:
python agent.py console
Your agent speaks to you in the terminal, and you can speak to it as well.
Connect to playground
Start your agent in
dev
mode to connect it to LiveKit and make it available from anywhere on the internet:
python agent.py dev
Use the
Agents playground
to speak with your agent and explore its full range of multimodal capabilities.
Congratulations, your agent is up and running. Continue to use the playground or the
console
mode as you build and test your agent.
Agent CLI modes
In the
console
mode, the agent runs locally and is only available within your terminal.
Run your agent in
dev
(development / debug) or
start
(production) mode to connect to LiveKit and join rooms.
Next steps
Follow these guides bring your voice AI app to life in the real world.
Web and mobile frontends
Put your agent in your pocket with a custom web or mobile app.
Telephony integration
Your agent can place and receive calls with LiveKit's SIP integration.
Testing your agent
Add behavioral tests to fine-tune your agent's behavior.
Building voice agents
Comprehensive documentation to build advanced voice AI apps with LiveKit.
Worker lifecycle
Learn how to manage your agents with workers and jobs.
Deploying to production
Guide to deploying your voice agent in a production environment.
Integration guides
Explore the full list of AI providers available for LiveKit Agents.
Recipes
A comprehensive collection of examples, guides, and recipes for LiveKit Agents.
On this page
Overview
Requirements
Python
LiveKit server
AI providers
Setup
Packages
Environment variables
Agent code
Download model files
Speak to your agent
Connect to playground
Next steps


Content from https://docs.livekit.io/agents/start/telephony:

On this page
Overview
Getting started
Agent dispatch
Inbound calls
Dispatch rules
Answering the phone
Call your agent
Outbound calls
Dialing a number
Make a call with your agent
Voicemail detection
Hangup
Transferring call to another number
Recipes
Further reading
Copy page
See more page options
Overview
It's easy to integrate LiveKit Agents with telephony systems using Session Initiation Protocol (SIP). You can choose to support inbound calls, outbound calls, or both. LiveKit also provides features including DTMF, SIP REFER, and more.
Telephony integration requires no significant changes to your existing agent code, as phone calls are simply bridged into LiveKit rooms using a special participant type.
Getting started
Follow the
Voice AI quickstart
to get a simple agent up and running.
Set up a SIP trunk for your project.
Return to this guide to enable inbound and outbound calls.
Voice AI quickstart
Follow the Voice AI quickstart to get your agent up and running.
SIP trunk setup
Configure your SIP trunk provider to route calls in LiveKit.
Agent dispatch
LiveKit recommends using explicit agent dispatch for telephony integrations to ensure no unexpected automatic dispatch occurs given the complexity of inbound and outbound calling.
To enable explicit dispatch, give your agent a name. This disables automatic dispatch.
agent.py
# ... your existing agent code ...
if
__name__
==
"__main__"
:
agents
.
cli
.
run_app
(
agents
.
WorkerOptions
(
entrypoint_fnc
=
entrypoint
,
# agent_name is required for explicit dispatch
agent_name
=
"my-telephony-agent"
)
)
Full examples
See the docs on
agent dispatch
for more complete examples.
Inbound calls
After you configure your
inbound trunk
follow these steps to enable inbound calling for your agent.
Dispatch rules
The following rule routes all inbound calls to a new room and dispatches your agent to that room:
dispatch-rule.json
{
"dispatch_rule"
:
{
"rule"
:
{
"dispatchRuleIndividual"
:
{
"roomPrefix"
:
"call-"
}
}
,
"roomConfig"
:
{
"agents"
:
[
{
"agentName"
:
"my-telephony-agent"
}
]
}
}
}
Create this rule with the following command:
lk sip dispatch create dispatch-rule.json
Answering the phone
Call the
generate_reply
method of your
AgentSession
to greet the caller after picking up. This code goes after
session.start
:
await
session
.
generate_reply
(
instructions
=
"Greet the user and offer your assistance."
)
Call your agent
After you start your agent with the following command, dial the number you set up earlier to hear your agent answer the phone.
python agent.py dev
Outbound calls
After setting up your
outbound trunk
, you may place outbound calls by dispatching an agent and then creating a SIP participant.
The following guide describes how to modify the
voice AI quickstart
for outbound calling. Alternatively, see the following complete example on GitHub:
Outbound caller example
Complete example of an outbound calling agent.
Dialing a number
Add the following code so your agent reads the phone number and places an outbound call by creating a SIP participant after connection.
You should also remove the initial greeting or place it behind an
if
statement to ensure the agent waits for the user to speak first when placing an outbound call.
SIP trunk ID
You must fill in the
sip_trunk_id
for this example to work. You can get this from LiveKit CLI with
lk sip outbound list
.
agent.py
# add these imports at the top of your file
from
livekit
import
api
import
json
# ... any existing code / imports ...
def
entrypoint
(
ctx
:
agents
.
JobContext
)
:
# If a phone number was provided, then place an outbound call
# By having a condition like this, you can use the same agent for inbound/outbound telephony as well as web/mobile/etc.
dial_info
=
json
.
loads
(
ctx
.
job
.
metadata
)
phone_number
=
dial_info
[
"phone_number"
]
# The participant's identity can be anything you want, but this example uses the phone number itself
sip_participant_identity
=
phone_number
if
phone_number
is
not
None
:
# The outbound call will be placed after this method is executed
try
:
await
ctx
.
api
.
sip
.
create_sip_participant
(
api
.
CreateSIPParticipantRequest
(
# This ensures the participant joins the correct room
room_name
=
ctx
.
room
.
name
,
# This is the outbound trunk ID to use (i.e. which phone number the call will come from)
# You can get this from LiveKit CLI with `lk sip outbound list`
sip_trunk_id
=
'ST_xxxx'
,
# The outbound phone number to dial and identity to use
sip_call_to
=
phone_number
,
participant_identity
=
sip_participant_identity
,
# This will wait until the call is answered before returning
wait_until_answered
=
True
,
)
)
print
(
"call picked up successfully"
)
except
api
.
TwirpError
as
e
:
print
(
f"error creating SIP participant:
{
e
.
message
}
, "
f"SIP status:
{
e
.
metadata
.
get
(
'sip_status_code'
)
}
"
f"
{
e
.
metadata
.
get
(
'sip_status'
)
}
"
)
ctx
.
shutdown
(
)
# .. create and start your AgentSession as normal ...
# Add this guard to ensure the agent only speaks first in an inbound scenario.
# When placing an outbound call, its more customary for the recipient to speak first
# The agent will automatically respond after the user's turn has ended.
if
phone_number
is
None
:
await
session
.
generate_reply
(
instructions
=
"Greet the user and offer your assistance."
)
Make a call with your agent
Use either the LiveKit CLI or the Python API to instruct your agent to place an outbound phone call.
In this example, the job's metadata includes the phone number to call. You can extend this to include more information if needed for your use case.
LiveKit CLI
Python
The following command creates a new room and dispatches your agent to it with the phone number to call. Ensure the agent name matches the name you set earlier in the
agent dispatch
section.
lk dispatch create
\
--new-room
\
--agent-name my-telephony-agent
\
--metadata
'{"phone_number": "+15105550123"}'
# insert your own phone number here
Voicemail detection
Your agent may still encounter an automated system such as an answering machine or voicemail. You can give your LLM the ability to detect a likely voicemail system via tool call, and then perform special actions such as leaving a message and
hanging up
.
agent.py
import
asyncio
# add this import at the top of your file
class
Assistant
(
Agent
)
:
## ... existing init code ...
@function_tool
async
def
detected_answering_machine
(
self
)
:
"""Call this tool if you have detected a voicemail system, AFTER hearing the voicemail greeting"""
await
self
.
session
.
generate_reply
(
instructions
=
"Leave a voicemail message letting the user know you'll call back later."
)
await
asyncio
.
sleep
(
0.5
)
# Add a natural gap to the end of the voicemail message
await
hangup_call
(
)
Hangup
To end a call for all participants, use the
delete_room
API. If only the agent session ends, the user will continue to hear silence until they hang up. The example below shows a basic
hangup_call
function you can use as a starting point.
agent.py
# Add these imports at the top of your file
from
livekit
import
api
,
rtc
from
livekit
.
agents
import
get_job_context
# Add this function definition anywhere
async
def
hangup_call
(
)
:
ctx
=
get_job_context
(
)
if
ctx
is
None
:
# Not running in a job context
return
await
ctx
.
api
.
room
.
delete_room
(
api
.
DeleteRoomRequest
(
room
=
ctx
.
room
.
name
,
)
)
class
MyAgent
(
Agent
)
:
.
.
.
# to hang up the call as part of a function call
@function_tool
async
def
end_call
(
self
,
ctx
:
RunContext
)
:
"""Called when the user wants to end the call"""
# let the agent finish speaking
current_speech
=
ctx
.
session
.
current_speech
if
current_speech
:
await
current_speech
.
wait_for_playout
(
)
await
hangup_call
(
)
Transferring call to another number
In case the agent needs to transfer the call to another number or SIP destination, you can use the
transfer_sip_participant
API.
This is a "cold" transfer, where the agent hands the call off to another party without staying on the line. The current session ends after the transfer is complete.
agent.py
class
Assistant
(
Agent
)
:
## ... existing init code ...
@function_tool
(
)
async
def
transfer_call
(
self
,
ctx
:
RunContext
)
:
"""Transfer the call to a human agent, called after confirming with the user"""
transfer_to
=
"+15105550123"
participant_identity
=
"+15105550123"
# let the message play fully before transferring
await
ctx
.
session
.
generate_reply
(
instructions
=
"Inform the user that you're transferring them to a different agent."
)
job_ctx
=
get_job_context
(
)
try
:
await
job_ctx
.
api
.
sip
.
transfer_sip_participant
(
api
.
TransferSIPParticipantRequest
(
room_name
=
job_ctx
.
room
.
name
,
participant_identity
=
participant_identity
,
# to use a sip destination, use `sip:user@host` format
transfer_to
=
f"tel:
{
transfer_to
}
"
,
)
)
except
Exception
as
e
:
print
(
f"error transferring call:
{
e
}
"
)
# give the LLM that context
return
"could not transfer call"
SIP REFER
You must enable SIP REFER on your SIP trunk provider to use
transfer_sip_participant
.
For Twilio, you must also enable
Enable PSTN Transfer
.
Recipes
The following recipes are particular helpful to learn more about telephony integration.
Company Directory
Build a AI company directory agent. The agent can respond to DTMF tones and voice prompts, then redirect callers.
SIP Warm Handoff
Transfer calls from an AI agent to a human operator seamlessly.
SIP Lifecycle
Complete lifecycle management for SIP calls.
Survey Caller
Automated survey calling system.
Further reading
The following guides provide more information on building voice agents for telephony.
Workflows
Orchestrate detailed workflows such as collecting credit card information over the phone.
Tool definition & use
Extend your agent's capabilities with tools.
Telephony documentation
Full documentation on the LiveKit SIP integration and features.
Agent speech
Customize and perfect your agent's verbal interactions.
On this page
Overview
Getting started
Agent dispatch
Inbound calls
Dispatch rules
Answering the phone
Call your agent
Outbound calls
Dialing a number
Make a call with your agent
Voicemail detection
Hangup
Transferring call to another number
Recipes
Further reading


Content from https://docs.livekit.io/agents/start/frontend:

On this page
Overview
Starter apps
Media and text
Data sharing
State and control
Audio visualizer
Authentication
Virtual avatars
Responsiveness tips
Minimize connection time
Connection indicators
Effects
Copy page
See more page options
Overview
LiveKit Agents is ready to integrate with your preferred frontend platform using the
LiveKit SDKs
for JavaScript, Swift, Android, Flutter, React Native, and more. Your agent can communicate with your frontend through LiveKit WebRTC, which provides fast and reliable realtime connectivity.
For example, a simple voice agent subscribes to the user's microphone track and publishes its own.
Text transcriptions
are also available as text streams. A more complex agent with vision capabilities can subscribe to a video track published from the user's camera or shared screen. An agent can also publish its own video to implement a virtual avatar or other features.
In all of these cases, the LiveKit SDKs are production grade and easy to use so you can build useful and advanced agents without worrying about the complexities of realtime media delivery. This topic contains resources and tips for building a high-quality frontend for your agent.
Starter apps
LiveKit recommends using one of the following starter apps to get up and running quickly on your preferred platform. Each app is open source under the MIT License so you can freely modify it to your own needs. The mobile apps require a hosted token server, but include a
LiveKit Cloud Sandbox
for development purposes.
http://localhost:3000
Swift
SwiftUI Voice Agent
A native iOS, macOS, and visionOS voice AI assistant built in SwiftUI.
GitHub
livekit-examples/agent-starter-swift
Next.js
Next.js Voice Agent
A web voice AI assistant built with React and Next.js.
GitHub
livekit-examples/agent-starter-react
Flutter
Flutter Voice Agent
A cross-platform voice AI assistant app built with Flutter.
GitHub
livekit-examples/agent-starter-flutter
React
React Native Voice Agent
A native voice AI assistant app built with React Native and Expo.
GitHub
livekit-examples/agent-starter-react-native
Android
Android Voice Agent
A native Android voice AI assistant app built with Kotlin and Jetpack Compose.
GitHub
livekit-examples/agent-starter-android
Web Embed Voice Agent
A voice AI agent that can be embedded in any web page.
GitHub
livekit-examples/agent-starter-embed
Media and text
To learn more about realtime media and text streams, see the following documentation.
Media tracks
Use the microphone, speaker, cameras, and screenshare with your agent.
Text streams
Send and receive realtime text and transcriptions.
Data sharing
To share images, files, or any other kind of data between your frontend and your agent, you can use the following features.
Byte streams
Send and receive images, files, or any other data.
Data packets
Low-level API for sending and receiving any kind of data.
State and control
In some cases, your agent and your frontend code might need a custom integration of state and configuration to meet your application's requirements. In these cases, the LiveKit realtime state and data features can be used to create a tightly-coupled and responsive experience.
AgentSession automatically manages the
lk.agent.state
participant attribute to contain the appropriate string value from among
initializing
,
listening
,
thinking
, or
speaking
.
State synchronization
Share custom state between your frontend and agent.
RPC
Define and call methods on your agent or your frontend from the other side.
Audio visualizer
The LiveKit component SDKs for React, SwiftUI, Android Compose, and Flutter include an audio visualizer component that can be used to give your voice agent a visual presence in your application.
For complete examples, see the sample apps listed above. The following documentation is a quick guide to using these components:
React
Swift
Android
Flutter
Install the
React components
and
styles
packages to use the
useVoiceAssistant
hook and the
BarVisualizer
. These components work automatically within a
LiveKitRoom
or
RoomContext.Provider
).
Also see
VoiceAssistantControlBar
, which provides a simple set of common UI controls for voice agent applications.
"use client"
;
import
"@livekit/components-styles"
;
import
{
useVoiceAssistant
,
BarVisualizer
,
}
from
"@livekit/components-react"
;
export
default
function
SimpleVoiceAssistant
(
)
{
// Get the agent's audio track and current state
const
{
state
,
audioTrack
}
=
useVoiceAssistant
(
)
;
return
(
<
div className
=
"h-80"
>
<
BarVisualizer state
=
{
state
}
barCount
=
{
5
}
trackRef
=
{
audioTrack
}
style
=
{
{
}
}
/
>
<
p className
=
"text-center"
>
{
state
}
<
/
p
>
<
/
div
>
)
;
}
Authentication
The LiveKit SDKs require a
token
to connect to a room. In web apps, you can typically include a simple token endpoint as part of the app. For mobile apps, you need a separate
token server
.
Virtual avatars
Your frontend can include a video representation of your agent using a virtual avatar from a supported provider. LiveKit includes full support for video rendering on all supported platforms. The
starter apps
include support for virtual avatars. For more information and a list of supported providers, consult the documentation:
Virtual avatars
Use a virtual avatar to give your agent a visual presence in your app.
Responsiveness tips
This section contains some suggestions to make your app feel more responsive to the user.
Minimize connection time
To connect your user to your agent, these steps must all occur:
Fetch an access token.
The user connects to the room.
Dispatch an agent process.
The agent connects to the room.
User and agent publish and subscribe to each other's media tracks.
If done in sequence, this takes up to a few seconds to complete. You can reduce this time by eliminating or parallelizing these steps.
Option 1: "Warm" token
In this case, your application will generate a token for the user at login with a long expiration time. When you need to connect to the room, the token is already available in your frontend.
Option 2: Dispatch agent during token generation
In this case, your application will optimistically create a room and dispatch the agent at the same time the token is generated, using
explicit agent dispatch
. This allows the user and the agent to connect to the room at the same time.
Connection indicators
Make your app feel more responsive, even when slow to connect, by linking various events into only one or two status indicators for the user rather than a number of discrete steps and UI changes.  Refer to the
event handling
documentation for more information on how to monitor the connection state and other events.
In the case that your agent fails to connect, you should notify the user and allow them to try again rather than leaving them to speak into an empty room.
Room connection
: The
room.connect
method can be awaited in most SDKs, and most also provide a
room.connectionState
property. Also monitor the
Disconnected
event to know when the connection is lost.
Agent presence
: Monitor
ParticipantConnected
events with
participant.kind === ParticipantKind.AGENT
Agent state
: Access the agent's state (
initializing
,
listening
,
thinking
, or
speaking
)
Track subscription
: Listen for
TrackSubscribed
events to know when your media has been subscribed to.
Effects
You should use sound effects, haptic feedback, and visual effects to make your agent feel more responsive. This is especially important during long thinking states (for instance, when performing external lookups or tool use). The
visualizer
includes basic "thinking" state indication and also allows the user to notice when their audio is not working. For more advanced effects, use the
state and control
features to trigger effects in your frontend.
On this page
Overview
Starter apps
Media and text
Data sharing
State and control
Audio visualizer
Authentication
Virtual avatars
Responsiveness tips
Minimize connection time
Connection indicators
Effects


Content from https://docs.livekit.io/agents/start/playground:

On this page
Overview
Links
Copy page
See more page options
Overview
The LiveKit Agents playground is a versatile web frontend that makes it easy to test your multimodal AI agent without having to worry about UI until you're happy with your AI.
To use the playground, you first need to have an agent running in
dev
or
start
mode. If you haven't done that yet, first follow the
Voice AI quickstart
.
Feature
Notes
Audio
Mic input and speaker output with visualizer
Text
Live transcription and chat input
Video
Live webcam input, live output
Links
Follow these links to get started with the playground.
Hosted playground
A hosted playground that seamlessly integrates with LiveKit Cloud.
Source code
Run the playground yourself or use it as a starting point for your own application.
On this page
Overview
Links


Content from https://docs.livekit.io/agents/start/v0-migration:

On this page
Unified agent interface
Customizing pipeline behavior
before_llm_cb -> llm_node
before_tts_cb -> tts_node
Tool definition and use
Chat context
Updating chat context
Transcriptions
Accepting text input
State change events
User state
Agent state
Other events
Removed features
Copy page
See more page options
Unified agent interface
Agents 1.0 introduces
AgentSession
, a single, unified
agent orchestrator
that serves as the foundation for all types of agents built using the framework.  With this change, the
VoicePipelineAgent
and
MultimodalAgent
classes have been deprecated and 0.x agents will need to be updated to use
AgentSession
in order to be compatible with 1.0 and later.
AgentSession
contains a superset of the functionality of
VoicePipelineAgent
and
MultimodalAgent
, allowing you to switch between pipelined and speech-to-speech models without changing your core application logic.
Note
The following code highlights the differences between Agents v0.x and Agents 1.0. For a full working example, see the
Voice AI quickstart
.
Version 0.x
Version 1.0
from
livekit
.
agents
import
JobContext
,
llm
from
livekit
.
agents
.
pipeline
import
VoicePipelineAgent
from
livekit
.
plugins
import
(
cartesia
,
deepgram
,
google
,
silero
,
)
async
def
entrypoint
(
ctx
:
JobContext
)
:
initial_ctx
=
llm
.
ChatContext
(
)
.
append
(
role
=
"system"
,
text
=
"You are a helpful voice AI assistant."
,
)
agent
=
VoicePipelineAgent
(
vad
=
silero
.
VAD
.
load
(
)
,
stt
=
deepgram
.
STT
(
)
,
llm
=
google
.
LLM
(
)
,
tts
=
cartesia
.
TTS
(
)
,
)
await
agent
.
start
(
room
,
participant
)
await
agent
.
say
(
"Hey, how can I help you today?"
,
allow_interruptions
=
True
)
Customizing pipeline behavior
We’ve introduced more flexibility for developers to customize the behavior of agents built on 1.0 through the new concept of
pipeline nodes
, which enable custom processing within the pipeline steps while also delegating to the default implementation of each node as needed.
Pipeline nodes replaces the
before_llm_cb
and
before_tts_cb
callbacks.
before_llm_cb -> llm_node
before_llm_cb
has been replaced by
llm_node
. This node can be used to modify the chat context before sending it to LLM, or integrate with custom LLM providers without having to create a plugin. As long as it returns AsyncIterable[llm.ChatChunk], the LLM node will forward the chunks to the next node in the pipeline.
Version 0.x
Version 1.0
async
def
add_rag_context
(
assistant
:
VoicePipelineAgent
,
chat_ctx
:
llm
.
ChatContext
)
:
rag_context
:
str
=
retrieve
(
chat_ctx
)
chat_ctx
.
append
(
text
=
rag_context
,
role
=
"system"
)
agent
=
VoicePipelineAgent
(
.
.
.
before_llm_cb
=
add_rag_context
,
)
before_tts_cb -> tts_node
before_tts_cb
has been replaced by
tts_node
. This node gives greater flexibility in customizing the TTS pipeline. It's possible to modify the text before synthesis, as well as the audio buffers after synthesis.
Version 0.x
Version 1.0
def
_before_tts_cb
(
agent
:
VoicePipelineAgent
,
text
:
str
|
AsyncIterable
[
str
]
)
:
# The TTS is incorrectly pronouncing "LiveKit", so we'll replace it with MFA-style IPA
# spelling for Cartesia
return
tokenize
.
utils
.
replace_words
(
text
=
text
,
replacements
=
{
"livekit"
:
r"<<l|aj|v|cʰ|ɪ|t|>>"
}
)
agent
=
VoicePipelineAgent
(
.
.
.
before_tts_cb
=
_before_tts_cb
,
)
Tool definition and use
Agents 1.0 streamlines the way in which
tools
are defined for use within your agents, making it easier to add and maintain agent tools.  When migrating from 0.x to 1.0, developers will need to make the following changes to existing use of functional calling within their agents in order to be compatible with versions 1.0 and later.
The
@llm.ai_callable
decorator for function definition has been replaced with the new
@function_tool
decorator.
If you define your functions within an
Agent
and use the
@function_tool
decorator, these tools are automatically accessible to the LLM. In this scenario, you no longer required to define your functions in a
llm.FunctionContext
class and pass them into the agent constructor.
Argument types are now inferred from the function signature and docstring. Annotated types are no longer supported.
Functions take in a
RunContext
object, which provides access to the current agent state.
Version 0.x
Version 1.0
from
livekit
.
agents
import
llm
from
livekit
.
agents
.
pipeline
import
VoicePipelineAgent
from
livekit
.
agents
.
multimodal
import
MultimodalAgent
class
AssistantFnc
(
llm
.
FunctionContext
)
:
@llm
.
ai_callable
(
)
async
def
get_weather
(
self
,
.
.
.
)
.
.
.
fnc_ctx
=
AssistantFnc
(
)
pipeline_agent
=
VoicePipelineAgent
(
.
.
.
fnc_ctx
=
fnc_ctx
,
)
multimodal_agent
=
MultimodalAgent
(
.
.
.
fnc_ctx
=
fnc_ctx
,
)
Chat context
ChatContext has been overhauled in 1.0 to provide a more powerful and flexible API for managing chat history. It now accounts for differences between LLM providers—such as stateless and stateful APIs—while exposing a unified interface.
Chat history can now include three types of items:
ChatMessage
: a message associated with a role (e.g., user, assistant). Each message includes a list of
content
items, which can contain text, images, or audio.
FunctionCall
: a function call initiated by the LLM.
FunctionCallOutput
: the result returned from a function call.
Updating chat context
In 0.x, updating the chat context required modifying chat_ctx.messages directly. This approach was error-prone and difficult to time correctly, especially with realtime APIs.
In v1.x, there are two supported ways to update the chat context:
Agent handoff
–
transferring control
to a new agent, which will have its own chat context.
Explicit update
- calling
agent.update_chat_ctx()
to modify the context directly.
Transcriptions
Agents 1.0 brings some new changes to how
transcriptions
are handled:
Transcriptions now use
text streams
with topic
lk.transcription
.
The
old transcription protocol
is deprecated and will be removed in v1.1.
for now both protocols are used for backwards compatibility.
Upcoming versions SDKs/components standardize on text streams for transcriptions.
Accepting text input
Agents 1.0 introduces
improved support for text input
. Previously, text had to be manually intercepted and injected into the agent via
ChatManager
.
In this version, agents automatically receive text input from a text stream on the
lk.chat
topic.
The
ChatManager
has been removed in Python SDK v1.0.
State change events
User state
user_started_speaking
and
user_stopped_speaking
events are no longer emitted. They've been combined into a single
user_state_changed
event.
Version 0.x
Version 1.0
@agent
.
on
(
"user_started_speaking"
)
def
on_user_started_speaking
(
)
:
print
(
"User started speaking"
)
Agent state
Version 0.x
Version 1.0
@agent
.
on
(
"agent_started_speaking"
)
def
on_agent_started_speaking
(
)
:
# Log transcribed message from user
print
(
"Agent started speaking"
)
Other events
Agent events were overhauled in version 1.0. For details, see the
events
page.
Removed features
OpenAI Assistants API support has been removed in 1.0.
The beta integration with the Assistants API in the OpenAI LLM plugin has been deprecated. Its stateful model made it difficult to manage state consistently between the API and agent.
On this page
Unified agent interface
Customizing pipeline behavior
before_llm_cb -> llm_node
before_tts_cb -> tts_node
Tool definition and use
Chat context
Updating chat context
Transcriptions
Accepting text input
State change events
User state
Agent state
Other events
Removed features


Content from https://docs.livekit.io/agents/build/workflows:

On this page
Overview
Agents
Defining an agent
Setting the active agent
Handing off from tool call
Passing state
Tasks
Defining a task
Running a task
Task results
Prebuilt tasks
Context preservation
Overriding plugins
Examples
Further reading
Copy page
See more page options
Overview
LiveKit Agents enables you to compose reliable workflows to tackle complex scenarios.
An
Agent
takes indefinite control of a session. It can include custom prompts, tools, and other logic. If needed, it can invoke tasks or hand off control to a different agent. This is useful for scenarios such as the following:
Including multiple personas with unique traits within a single session.
Moving through different predetermined conversation phases.
Offering multiple modes or functionality within a single voice agent.
The framework also includes experimental support for
Tasks
, which take temporary control of a session to complete a specific task and return a specific result. For more information, see
Tasks
.
Agents
Agents form the backbone of a session’s functionality and are responsible for overall orchestration.
Defining an agent
Extend the
Agent
class to define a custom agent.
from
livekit
.
agents
import
Agent
class
HelpfulAssistant
(
Agent
)
:
def
__init__
(
self
)
:
super
(
)
.
__init__
(
instructions
=
"You are a helpful voice AI assistant."
)
async
def
on_enter
(
self
)
-
>
None
:
await
self
.
session
.
generate_reply
(
instructions
=
"Greet the user and ask how you can help them."
)
You can also create an instance of
Agent
class directly:
agent
=
Agent
(
instructions
=
"You are a helpful voice AI assistant."
)
Setting the active agent
Specify the initial agent in the
AgentSession
constructor:
session
=
AgentSession
(
agent
=
CustomerServiceAgent
(
)
# ...
)
To set a new agent, use the
update_agent
method:
session
.
update_agent
(
CustomerServiceAgent
(
)
)
Handing off from tool call
Return a different agent from within a tool call to hand off control automatically. This allows the LLM to make decisions about when handoff should occur. For more information, see
tool return value
.
from
livekit
.
agents
import
Agent
,
function_tool
class
CustomerServiceAgent
(
Agent
)
:
def
__init__
(
self
)
:
super
(
)
.
__init__
(
instructions
=
"""You are a friendly customer service representative. Help customers with
general inquiries, account questions, and technical support. If a customer needs
specialized help, transfer them to the appropriate specialist."""
)
async
def
on_enter
(
self
)
-
>
None
:
await
self
.
session
.
generate_reply
(
instructions
=
"Greet the user warmly and offer your assistance."
)
@function_tool
(
)
async
def
transfer_to_billing
(
self
,
context
:
RunContext
)
:
"""Transfer the customer to a billing specialist for account and payment questions."""
return
"Transferring to billing"
,
BillingAgent
(
chat_ctx
=
self
.
chat_ctx
)
@function_tool
(
)
async
def
transfer_to_technical_support
(
self
,
context
:
RunContext
)
:
"""Transfer the customer to technical support for product issues and troubleshooting."""
return
"Transferring to technical support"
,
TechnicalSupportAgent
(
chat_ctx
=
self
.
chat_ctx
)
class
BillingAgent
(
Agent
)
:
def
__init__
(
self
)
:
super
(
)
.
__init__
(
instructions
=
"""You are a billing specialist. Help customers with account questions,
payments, refunds, and billing inquiries. Be thorough and empathetic."""
)
async
def
on_enter
(
self
)
-
>
None
:
await
self
.
session
.
generate_reply
(
instructions
=
"Introduce yourself as a billing specialist and ask how you can help with their account."
)
class
TechnicalSupportAgent
(
Agent
)
:
def
__init__
(
self
)
:
super
(
)
.
__init__
(
instructions
=
"""You are a technical support specialist. Help customers troubleshoot
product issues, setup problems, and technical questions. Ask clarifying questions
to diagnose problems effectively."""
)
async
def
on_enter
(
self
)
-
>
None
:
await
self
.
session
.
generate_reply
(
instructions
=
"Introduce yourself as a technical support specialist and offer to help with any technical issues."
)
Passing state
To store custom state within your session, use the
userdata
attribute. The type of userdata is up to you, but the recommended approach is to use a
dataclass
.
from
livekit
.
agents
import
AgentSession
from
dataclasses
import
dataclass
@dataclass
class
MySessionInfo
:
user_name
:
str
|
None
=
None
age
:
int
|
None
=
None
To add userdata to your session, pass it in the constructor. You must also specify the type of userdata on the
AgentSession
itself.
session
=
AgentSession
[
MySessionInfo
]
(
userdata
=
MySessionInfo
(
)
,
# ... tts, stt, llm, etc.
)
Userdata is available as
session.userdata
, and is also available within function tools on the
RunContext
. The following example shows how to use userdata in an agent workflow that starts with the
IntakeAgent
.
class
IntakeAgent
(
Agent
)
:
def
__init__
(
self
)
:
super
(
)
.
__init__
(
instructions
=
"""Your are an intake agent. Learn the user's name and age."""
)
@function_tool
(
)
async
def
record_name
(
self
,
context
:
RunContext
[
MySessionInfo
]
,
name
:
str
)
:
"""Use this tool to record the user's name."""
context
.
userdata
.
user_name
=
name
return
self
.
_handoff_if_done
(
)
@function_tool
(
)
async
def
record_age
(
self
,
context
:
RunContext
[
MySessionInfo
]
,
age
:
int
)
:
"""Use this tool to record the user's age."""
context
.
userdata
.
age
=
age
return
self
.
_handoff_if_done
(
)
def
_handoff_if_done
(
self
)
:
if
self
.
session
.
userdata
.
user_name
and
self
.
session
.
userdata
.
age
:
return
CustomerServiceAgent
(
)
else
:
return
None
class
CustomerServiceAgent
(
Agent
)
:
def
__init__
(
self
)
:
super
(
)
.
__init__
(
instructions
=
"You are a friendly customer service representative."
)
async
def
on_enter
(
self
)
-
>
None
:
userdata
:
MySessionInfo
=
self
.
session
.
userdata
await
self
.
session
.
generate_reply
(
instructions
=
f"Greet
{
userdata
.
user_name
}
personally and offer your assistance."
)
Tasks
Tasks allow you to create focused, reusable components that complete specific tasks and return typed results. Unlike regular agents that take indefinite control of a session, tasks are used within agents or other tasks, complete their objective, and return control along with their result.
Tasks are useful for scenarios such as:
Acquiring recording consent at the beginning of a call.
Collecting specific structured information, such as an address or a credit card number.
Moving through a series of questions, one at a time.
Any discrete task that should complete and return control to the caller.
Experimental feature
Tasks are currently experimental and the API might change in a future release.
Defining a task
Extend the
AgentTask
class and specify a result type using
generics
. Use the
on_enter
method to begin the task's interaction with the user, and call the
complete
method with a result when complete. The task has full support for tools, similar to an agent.
from
livekit
.
agents
import
AgentTask
,
function_tool
class
CollectConsent
(
AgentTask
[
bool
]
)
:
def
__init__
(
self
)
:
super
(
)
.
__init__
(
instructions
=
"Ask for recording consent and get a clear yes or no answer."
)
async
def
on_enter
(
self
)
-
>
None
:
await
self
.
session
.
generate_reply
(
instructions
=
"Ask for permission to record the call for quality assurance purposes."
)
@function_tool
async
def
consent_given
(
self
)
-
>
None
:
"""Use this when the user gives consent to record."""
self
.
complete
(
True
)
@function_tool
async
def
consent_denied
(
self
)
-
>
None
:
"""Use this when the user denies consent to record."""
self
.
complete
(
False
)
Running a task
The task runs automatically upon creation. It must be created within the context of an existing
Agent
which is active within an
AgentSession
. The task takes control of the session until it returns a result. Await the task to receive its result.
from
livekit
.
agents
import
Agent
,
function_tool
,
get_job_context
class
CustomerServiceAgent
(
Agent
)
:
def
__init__
(
self
)
:
super
(
)
.
__init__
(
instructions
=
"You are a friendly customer service representative."
)
async
def
on_enter
(
self
)
-
>
None
:
if
await
CollectConsent
(
chat_ctx
=
self
.
chat_ctx
)
:
await
self
.
session
.
generate_reply
(
instructions
=
"Offer your assistance to the user."
)
else
:
await
self
.
session
.
generate_reply
(
instructions
=
"Inform the user that you are unable to proceed and will end the call."
)
job_ctx
=
get_job_context
(
)
await
job_ctx
.
api
.
room
.
delete_room
(
api
.
DeleteRoomRequest
(
room
=
job_ctx
.
room
.
name
)
)
Task results
Use any result type you want. For complex results, use a custom dataclass.
from
dataclasses
import
dataclass
@dataclass
class
ContactInfoResult
:
name
:
str
email_address
:
str
phone_number
:
str
class
GetContactInfoTask
(
AgentTask
[
ContactInfoResult
]
)
:
# ....
Prebuilt tasks
The framework will include prebuilt tasks for common use cases within the module
livekit.agents.beta.workflows
. As of initial release, only the
GetEmailTask
is available.
GetEmailTask
Use
GetEmailTask
to reliably collect and validate an email address from the user.
from
livekit
.
agents
.
beta
.
workflows
import
GetEmailTask
# ... within your agent ...
email_result
=
await
GetEmailTask
(
chat_ctx
=
self
.
chat_ctx
)
print
(
f"Collected email:
{
email_result
.
email_address
}
"
)
Context preservation
By default, each new agent or task starts with a fresh conversation history for their LLM prompt. To include the prior conversation, set the
chat_ctx
parameter in the
Agent
or
AgentTask
constructor. You can either copy the prior agent's
chat_ctx
, or construct a new one based on custom business logic to provide the appropriate context.
from
livekit
.
agents
import
ChatContext
,
function_tool
,
Agent
class
TechnicalSupportAgent
(
Agent
)
:
def
__init__
(
self
,
chat_ctx
:
ChatContext
)
:
super
(
)
.
__init__
(
instructions
=
"""You are a technical support specialist. Help customers troubleshoot
product issues, setup problems, and technical questions."""
,
chat_ctx
=
chat_ctx
)
class
CustomerServiceAgent
(
Agent
)
:
# ...
@function_tool
(
)
async
def
transfer_to_technical_support
(
self
)
:
"""Transfer the customer to technical support for product issues and troubleshooting."""
await
self
.
session
.
generate_reply
(
instructions
=
"Inform the customer that you're transferring them to the technical support team."
)
# Pass the chat context during handoff
return
TechnicalSupportAgent
(
chat_ctx
=
self
.
session
.
chat_ctx
)
The complete conversation history for the session is always available in
session.history
.
Overriding plugins
You can override any of the plugins used in the session by setting the corresponding attributes in your
Agent
or
AgentTask
constructor. For instance, you can change the voice for a specific agent by overriding the
tts
attribute.
from
livekit
.
agents
import
Agent
from
livekit
.
plugins
import
cartesia
class
CustomerServiceManager
(
Agent
)
:
def
__init__
(
self
)
:
super
(
)
.
__init__
(
instructions
=
"You are a customer service manager who can handle escalated issues."
,
tts
=
cartesia
.
TTS
(
voice
=
"6f84f4b8-58a2-430c-8c79-688dad597532"
)
)
Examples
These examples show how to build more complex workflows with multiple agents:
Drive-thru agent
A complex food ordering agent with tasks, tools, and a complete evaluation suite.
Front-desk agent
A calendar booking agent with tasks, tools, and evaluations.
Medical Office Triage
Agent that triages patients based on symptoms and medical history.
Restaurant Agent
A restaurant front-of-house agent that can take orders, add items to a shared cart, and checkout.
Further reading
For more information on concepts touched on in this article, see the following related articles:
Tool definition and use
Complete guide to defining and using tools in your agents.
Nodes
Add custom behavior to any component of the voice pipeline.
Agent speech
Customize the speech output of your agents.
Testing & evaluation
Test every aspect of your agents with a custom test suite.
On this page
Overview
Agents
Defining an agent
Setting the active agent
Handing off from tool call
Passing state
Tasks
Defining a task
Running a task
Task results
Prebuilt tasks
Context preservation
Overriding plugins
Examples
Further reading


Content from https://docs.livekit.io/agents/build/audio:

On this page
Overview
Preemptive speech generation
Example
Initiating speech
session.say
generate_reply
Controlling agent speech
SpeechHandle
Getting the current speech handle
Interruptions
Customizing pronunciation
Adjusting speech volume
Adding background audio
Create the player
Start and stop the player
Play audio on-demand
Multiple audio clips
Supported audio sources
Additional resources
Copy page
See more page options
Overview
Speech capabilities are a core feature of LiveKit agents, enabling them to interact with users through voice.
This guide covers the various speech features and functionalities available for agents.
LiveKit Agents provide a unified interface for controlling agents using both the STT-LLM-TTS pipeline and realtime models.
To learn more and see usage examples, see the following topics:
Text-to-speech (TTS)
TTS is a synthesis process that converts text into audio, giving AI agents a "voice."
Speech-to-speech
Multimodal, realtime APIs can understand speech input and generate speech output directly.
Preemptive speech generation
Preemptive generation
allows the agent to begin generating a response before the user's end of turn is committed. The response is based on partial transcription or early signals from user input, helping reduce perceived response delay and improving conversational flow.
When enabled, the agent starts generating a response as soon as the final transcript is available. If the chat context or tools change in the
on_user_turn_completed
node
, the preemptive response is canceled and replaced with a new one based on the final transcript.
This feature reduces latency when the following are true:
STT node
returns the final transcript faster than
VAD
emits the
end_of_speech
event.
Turn detection model
is enabled.
You can enable this feature for STT-LLM-TTS pipeline agents using the
preemptive_generation
parameter for AgentSession:
session
=
AgentSession
(
preemptive_generation
=
True
,
.
.
.
# STT, LLM, TTS, etc.
)
Note
Preemptive generation doesn't guarantee reduced latency. Use
logging, metrics, and telemetry
to validate and fine tune agent performance.
Example
Preemptive generation example
An example of an agent using preemptive generation.
Initiating speech
By default, the agent waits for user input before responding—the Agents framework automatically handles response generation.
In some cases, though, the agent might need to initiate the conversation. For example, it might greet the user at the start of a session or check in after a period of silence.
session.say
To have the agent speak a predefined message, use
session.say()
. This triggers the configured TTS to synthesize speech and play it back to the user.
You can also optionally provide pre-synthesized audio for playback. This skips the TTS step and reduces response time.
Realtime models and TTS
The
say
method requires a TTS plugin. If you're using a realtime model, you need to add a TTS plugin to your session
or use the
generate_reply()
method instead.
await
session
.
say
(
"Hello. How can I help you today?"
,
allow_interruptions
=
False
,
)
Parameters
text
str | AsyncIterable[str]
Required
#
The text to speak.
audio
AsyncIterable[rtc.AudioFrame]
Optional
#
Pre-synthesized audio to play.
allow_interruptions
boolean
Optional
#
If
True
, allow the user to interrupt the agent while speaking. (default
True
)
add_to_chat_ctx
boolean
Optional
#
If
True
, add the text to the agent's chat context after playback. (default
True
)
Returns
Returns a
SpeechHandle
object.
Events
This method triggers a
speech_created
event.
generate_reply
To make conversations more dynamic, use
session.generate_reply()
to prompt the LLM to generate a response.
There are two ways to use
generate_reply
:
give the agent instructions to generate a response
session
.
generate_reply
(
instructions
=
"greet the user and ask where they are from"
,
)
provide the user's input via text
session
.
generate_reply
(
user_input
=
"how is the weather today?"
,
)
Impact to chat history
When using
generate_reply
with
instructions
, the agent uses the instructions to generate a response, which is added to the chat history. The instructions themselves are not recorded in the history.
In contrast,
user_input
is directly added to the chat history.
Parameters
user_input
string
Optional
#
The user input to respond to.
instructions
string
Optional
#
Instructions for the agent to use for the reply.
allow_interruptions
boolean
Optional
#
If
True
, allow the user to interrupt the agent while speaking. (default
True
)
Returns
Returns a
SpeechHandle
object.
Events
This method triggers a
speech_created
event.
Controlling agent speech
You can control agent speech using the
SpeechHandle
object returned by the
say()
and
generate_reply()
methods, and allowing user interruptions.
SpeechHandle
The
say()
and
generate_reply()
methods return a
SpeechHandle
object, which lets you track the state of the agent's speech. This can be useful for coordinating follow-up actions—for example, notifying the user before ending the call.
await
session
.
say
(
"Goodbye for now."
,
allow_interruptions
=
False
)
# the above is a shortcut for
# handle = session.say("Goodbye for now.", allow_interruptions=False)
# await handle.wait_for_playout()
You can wait for the agent to finish speaking before continuing:
handle
=
session
.
generate_reply
(
instructions
=
"Tell the user we're about to run some slow operations."
)
# perform an operation that takes time
.
.
.
await
handle
# finally wait for the speech
The following example makes a web request for the user, and cancels the request when the user interrupts:
async
with
aiohttp
.
ClientSession
(
)
as
client_session
:
web_request
=
client_session
.
get
(
'https://api.example.com/data'
)
handle
=
await
session
.
generate_reply
(
instructions
=
"Tell the user we're processing their request."
)
if
handle
.
interrupted
:
# if the user interrupts, cancel the web_request too
web_request
.
cancel
(
)
SpeechHandle
has an API similar to
ayncio.Future
, allowing you to add a callback:
handle
=
session
.
say
(
"Hello world"
)
handle
.
add_done_callback
(
lambda
_
:
print
(
"speech done"
)
)
Getting the current speech handle
The agent session's active speech handle, if any, is available with the
current_speech
property. If no speech is active, this property returns
None
. Otherwise, it returns the active
SpeechHandle
.
Use the active speech handle to coordinate with the speaking state. For instance, you can ensure that a hang up occurs only after the current speech has finished, rather than mid-speech:
# to hang up the call as part of a function call
@function_tool
async
def
end_call
(
self
,
ctx
:
RunContext
)
:
"""Use this tool when the user has signaled they wish to end the current call. The session ends automatically after invoking this tool."""
# let the agent finish speaking
current_speech
=
ctx
.
session
.
current_speech
if
current_speech
:
await
current_speech
.
wait_for_playout
(
)
# call API to delete_room
.
.
.
Interruptions
By default, the agent stops speaking when it detects that the user has started speaking. This behavior can be disabled by setting
allow_interruptions=False
when scheduling speech.
To explicitly interrupt the agent, call the
interrupt()
method on the handle or session at any time. This can be performed even when
allow_interruptions
is set to
False
.
handle
=
session
.
say
(
"Hello world"
)
handle
.
interrupt
(
)
# or from the session
session
.
interrupt
(
)
Customizing pronunciation
Most TTS providers allow you to customize pronunciation of words using Speech Synthesis Markup Language (SSML). The following example uses the
tts_node
to add custom pronunciation rules:
agent.py
Required imports
async
def
tts_node
(
self
,
text
:
AsyncIterable
[
str
]
,
model_settings
:
ModelSettings
)
-
>
AsyncIterable
[
rtc
.
AudioFrame
]
:
# Pronunciation replacements for common technical terms and abbreviations.
# Support for custom pronunciations depends on the TTS provider.
pronunciations
=
{
"API"
:
"A P I"
,
"REST"
:
"rest"
,
"SQL"
:
"sequel"
,
"kubectl"
:
"kube control"
,
"AWS"
:
"A W S"
,
"UI"
:
"U I"
,
"URL"
:
"U R L"
,
"npm"
:
"N P M"
,
"LiveKit"
:
"Live Kit"
,
"async"
:
"a sink"
,
"nginx"
:
"engine x"
,
}
async
def
adjust_pronunciation
(
input_text
:
AsyncIterable
[
str
]
)
-
>
AsyncIterable
[
str
]
:
async
for
chunk
in
input_text
:
modified_chunk
=
chunk
# Apply pronunciation rules
for
term
,
pronunciation
in
pronunciations
.
items
(
)
:
# Use word boundaries to avoid partial replacements
modified_chunk
=
re
.
sub
(
rf'\b
{
term
}
\b'
,
pronunciation
,
modified_chunk
,
flags
=
re
.
IGNORECASE
)
yield
modified_chunk
# Process with modified text through base TTS implementation
async
for
frame
in
Agent
.
default
.
tts_node
(
self
,
adjust_pronunciation
(
text
)
,
model_settings
)
:
yield
frame
The following table lists the SSML tags supported by most TTS providers:
SSML Tag
Description
phoneme
Used for phonetic pronunciation using a standard phonetic alphabet. These tags provide a phonetic pronunciation for the enclosed text.
say as
Specifies how to interpret the enclosed text. For example, use
character
to speak each character individually,
or
date
to specify a calendar date.
lexicon
A custom dictionary that defines the pronunciation of certain words using phonetic notation or text-to-pronunciation mappings.
emphasis
Speak text with an emphasis.
break
Add a manual pause.
prosody
Controls pitch, speaking rate, and volume of speech output.
Adjusting speech volume
To adjust the volume of the agent's speech, add a processor to the
tts_node
or the
realtime_audio_output_node
.  Alternative, you can also
adjust the volume of playback
in the frontend SDK.
The following example agent has an adjustable volume between 0 and 100, and offers a
tool call
to change it.
agent.py
Required imports
class
Assistant
(
Agent
)
:
def
__init__
(
self
)
-
>
None
:
self
.
volume
:
int
=
50
super
(
)
.
__init__
(
instructions
=
f"You are a helpful voice AI assistant. Your starting volume level is
{
self
.
volume
}
."
)
@function_tool
(
)
async
def
set_volume
(
self
,
volume
:
int
)
:
"""Set the volume of the audio output.
Args:
volume (int): The volume level to set. Must be between 0 and 100.
"""
self
.
volume
=
volume
# Audio node used by STT-LLM-TTS pipeline models
async
def
tts_node
(
self
,
text
:
AsyncIterable
[
str
]
,
model_settings
:
ModelSettings
)
:
return
self
.
_adjust_volume_in_stream
(
Agent
.
default
.
tts_node
(
self
,
text
,
model_settings
)
)
# Audio node used by realtime models
async
def
realtime_audio_output_node
(
self
,
audio
:
AsyncIterable
[
rtc
.
AudioFrame
]
,
model_settings
:
ModelSettings
)
-
>
AsyncIterable
[
rtc
.
AudioFrame
]
:
return
self
.
_adjust_volume_in_stream
(
Agent
.
default
.
realtime_audio_output_node
(
self
,
audio
,
model_settings
)
)
async
def
_adjust_volume_in_stream
(
self
,
audio
:
AsyncIterable
[
rtc
.
AudioFrame
]
)
-
>
AsyncIterable
[
rtc
.
AudioFrame
]
:
stream
:
utils
.
audio
.
AudioByteStream
|
None
=
None
async
for
frame
in
audio
:
if
stream
is
None
:
stream
=
utils
.
audio
.
AudioByteStream
(
sample_rate
=
frame
.
sample_rate
,
num_channels
=
frame
.
num_channels
,
samples_per_channel
=
frame
.
sample_rate
//
10
,
# 100ms
)
for
f
in
stream
.
push
(
frame
.
data
)
:
yield
self
.
_adjust_volume_in_frame
(
f
)
if
stream
is
not
None
:
for
f
in
stream
.
flush
(
)
:
yield
self
.
_adjust_volume_in_frame
(
f
)
def
_adjust_volume_in_frame
(
self
,
frame
:
rtc
.
AudioFrame
)
-
>
rtc
.
AudioFrame
:
audio_data
=
np
.
frombuffer
(
frame
.
data
,
dtype
=
np
.
int16
)
audio_float
=
audio_data
.
astype
(
np
.
float32
)
/
np
.
iinfo
(
np
.
int16
)
.
max
audio_float
=
audio_float
*
max
(
0
,
min
(
self
.
volume
,
100
)
)
/
100.0
processed
=
(
audio_float
*
np
.
iinfo
(
np
.
int16
)
.
max
)
.
astype
(
np
.
int16
)
return
rtc
.
AudioFrame
(
data
=
processed
.
tobytes
(
)
,
sample_rate
=
frame
.
sample_rate
,
num_channels
=
frame
.
num_channels
,
samples_per_channel
=
len
(
processed
)
//
frame
.
num_channels
,
)
Adding background audio
To add more realism to your agent, or add additional sound effects, publish background audio. This audio is played on a separate audio track. The
BackgroundAudioPlayer
class supports on-demand playback of custom audio as well as automatic ambient and thinking sounds synchronized to the agent lifecycle.
For a complete example, see the following recipe:
Background Audio
A voice AI agent with background audio for thinking states and ambiance.
Create the player
The
BackgroundAudioPlayer
class manages audio playback to a room. It can also play ambient and thinking sounds automatically during the lifecycle of the agent session, if desired.
ambient_sound
AudioSource | AudioConfig | list[AudioConfig]
Optional
#
Ambient sound plays on a loop in the background during the agent session. See
Supported audio sources
and
Multiple audio clips
for more details.
thinking_sound
AudioSource | AudioConfig | list[AudioConfig]
Optional
#
Thinking sound plays while the agent is in the "thinking" state. See
Supported audio sources
and
Multiple audio clips
for more details.
Create the player within your
entrypoint
function:
from
livekit
.
agents
import
BackgroundAudioPlayer
,
AudioConfig
,
BuiltinAudioClip
# An audio player with automated ambient and thinking sounds
background_audio
=
BackgroundAudioPlayer
(
ambient_sound
=
AudioConfig
(
BuiltinAudioClip
.
OFFICE_AMBIENCE
,
volume
=
0.8
)
,
thinking_sound
=
[
AudioConfig
(
BuiltinAudioClip
.
KEYBOARD_TYPING
,
volume
=
0.8
)
,
AudioConfig
(
BuiltinAudioClip
.
KEYBOARD_TYPING2
,
volume
=
0.7
)
,
]
,
)
# An audio player with a custom ambient sound played on a loop
background_audio
=
BackgroundAudioPlayer
(
ambient_sound
=
"/path/to/my-custom-sound.mp3"
,
)
# An audio player for on-demand playback only
background_audio
=
BackgroundAudioPlayer
(
)
Start and stop the player
Call the
start
method after room connection and after starting the agent session. Ambient sounds, if any, begin playback immediately.
room
: The room to publish the audio to.
agent_session
: The agent session to publish the audio to.
await
background_audio
.
start
(
room
=
ctx
.
room
,
agent_session
=
session
)
To stop and dispose the player, call the
aclose
method. You must create a new player instance if you want to start again.
await
background_audio
.
aclose
(
)
Play audio on-demand
You can play audio at any time, after starting the player, with the
play
method.
audio
AudioSource | AudioConfig | list[AudioConfig]
Required
#
The audio source or a probabilistic list of sources to play. To learn more, see
Supported audio sources
and
Multiple audio clips
.
loop
boolean
Optional
Default:
False
#
Set to
True
to continuously loop playback.
For example, if you created
background_audio
in the
previous example
, you can play an audio file like this:
background_audio
.
play
(
"/path/to/my-custom-sound.mp3"
)
The
play
method returns a
PlayHandle
which you can use to await or cancel the playback.
The following example uses the handle to await playback completion:
# Wait for playback to complete
await
background_audio
.
play
(
"/path/to/my-custom-sound.mp3"
)
The next example shows the handle's
stop
method, which stops playback early:
handle
=
background_audio
.
play
(
"/path/to/my-custom-sound.mp3"
)
await
(
asyncio
.
sleep
(
1
)
)
handle
.
stop
(
)
# Stop playback early
Multiple audio clips
You can pass a list of audio sources to any of
play
,
ambient_sound
, or
thinking_sound
. The player selects a single entry in the list based on the
probability
parameter. This is useful to avoid repetitive sound effects. To allow for the possibility of no audio at all, ensure the sum of the probabilities is less than 1.
AudioConfig
has the following properties:
source
AudioSource
Required
#
The audio source to play. See
Supported audio sources
for more details.
volume
float
Optional
Default:
1
#
The volume at which to play the given audio.
probability
float
Optional
Default:
1
#
The relative probability of selecting this audio source from the list.
# Play the KEYBOARD_TYPING sound with an 80% probability and the KEYBOARD_TYPING2 sound with a 20% probability
background_audio
.
play
(
[
AudioConfig
(
BuiltinAudioClip
.
KEYBOARD_TYPING
,
volume
=
0.8
,
probability
=
0.8
)
,
AudioConfig
(
BuiltinAudioClip
.
KEYBOARD_TYPING2
,
volume
=
0.7
,
probability
=
0.2
)
,
]
)
Supported audio sources
The following audio sources are supported:
Local audio file
Pass a string path to any local audio file. The player decodes files with FFmpeg via
PyAV
and supports all common audio formats including MP3, WAV, AAC, FLAC, OGG, Opus, WebM, and MP4.
WAV files
The player uses an optimized custom decoder to load WAV data directly to audio frames, without the overhead of FFmpeg. For small files, WAV is the highest-efficiency option.
Built-in audio clips
The following built-in audio clips are available by default for common sound effects:
BuiltinAudioClip.OFFICE_AMBIENCE
: Chatter and general background noise of a busy office.
BuiltinAudioClip.KEYBOARD_TYPING
: The sound of an operator typing on a keyboard, close to their microphone.
BuiltinAudioClip.KEYBOARD_TYPING2
: A shorter version of
KEYBOARD_TYPING
.
Raw audio frames
Pass an
AsyncIterator[rtc.AudioFrame]
to play raw audio frames from any source.
Additional resources
To learn more, see the following resources.
Voice AI quickstart
Use the quickstart as a starting base for adding audio code.
Speech related event
Learn more about the
speech_created
event, triggered when new agent speech is created.
LiveKit SDK
Learn how to use the LiveKit SDK to play audio tracks.
Background audio example
An example of using the
BackgroundAudioPlayer
class to play ambient office noise and thinking sounds.
Text-to-speech (TTS)
TTS usage and examples for pipeline agents.
Speech-to-speech
Multimodal, realtime APIs understand speech input and generate speech output directly.
On this page
Overview
Preemptive speech generation
Example
Initiating speech
session.say
generate_reply
Controlling agent speech
SpeechHandle
Getting the current speech handle
Interruptions
Customizing pronunciation
Adjusting speech volume
Adding background audio
Create the player
Start and stop the player
Play audio on-demand
Multiple audio clips
Supported audio sources
Additional resources


Content from https://docs.livekit.io/agents/build/vision/:

On this page
Overview
Images
Load into initial context
Upload from frontend
Sample video frames
Inference detail
Live video
Additional resources
Copy page
See more page options
Overview
If your
LLM
includes support for vision, you can add images to its chat context to leverage its full capabilities. LiveKit has tools for adding raw images from disk, the network, or uploaded directly from your frontend app. Additionally, you can use live video either with sampled frames in an STT-LLM-TTS pipeline model or true video input with a realtime model such as
Gemini Live
.
This guide includes an overview of the vision features and code samples for each use case.
Images
The agent's chat context supports images as well as text. You can add as many images as you want to the chat context, but keep in mind that larger context windows contribute to slow response times.
To add an image to the chat context, create an
ImageContent
object and include it in a chat message. The image content can be a base 64 data URL, an external URL, or a frame from a
video track
.
Load into initial context
The following example shows an agent initialized with an image at startup. This example uses an external URL, but you can modify it to load a local file using a base 64 data URL instead:
agent.py
Required imports
def
entrypoint
(
ctx
:
JobContext
)
:
# ctx.connect, etc.
session
=
AgentSession
(
# ... stt, tts, llm, etc.
)
initial_ctx
=
ChatContext
(
)
initial_ctx
.
add_message
(
role
=
"user"
,
content
=
[
"Here is a picture of me"
,
ImageContent
(
image
=
"https://example.com/image.jpg"
)
]
,
)
await
session
.
start
(
room
=
ctx
.
room
,
agent
=
Agent
(
chat_ctx
=
initial_ctx
,
)
,
# ... room_input_options, etc.
)
LLM provider support for external URLs
Not every provider supports external image URLs. Consult their documentation for details.
Upload from frontend
To upload an image from your frontend app, use the
sendFile method
of the LiveKit SDK. Add a byte stream handler to your agent to receive the image data and add it to the chat context. Here is a simple agent capable of receiving images from the user on the byte stream topic
"images"
:
agent.py
Required imports
class
Assistant
(
Agent
)
:
def
__init__
(
self
)
-
>
None
:
self
.
_tasks
=
[
]
# Prevent garbage collection of running tasks
super
(
)
.
__init__
(
instructions
=
"You are a helpful voice AI assistant."
)
async
def
on_enter
(
self
)
:
def
_image_received_handler
(
reader
,
participant_identity
)
:
task
=
asyncio
.
create_task
(
self
.
_image_received
(
reader
,
participant_identity
)
)
self
.
_tasks
.
append
(
task
)
task
.
add_done_callback
(
lambda
t
:
self
.
_tasks
.
remove
(
t
)
)
# Add the handler when the agent joins
get_job_context
(
)
.
room
.
register_byte_stream_handler
(
"images"
,
_image_received_handler
)
async
def
_image_received
(
self
,
reader
,
participant_identity
)
:
image_bytes
=
bytes
(
)
async
for
chunk
in
reader
:
image_bytes
+=
chunk
chat_ctx
=
self
.
chat_ctx
.
copy
(
)
# Encode the image to base64 and add it to the chat context
chat_ctx
.
add_message
(
role
=
"user"
,
content
=
[
ImageContent
(
image
=
f"data:image/png;base64,
{
base64
.
b64encode
(
image_bytes
)
.
decode
(
'utf-8'
)
}
"
)
]
,
)
await
self
.
update_chat_ctx
(
chat_ctx
)
Sample video frames
LLMs can process video in the form of still images, but many LLMs are not trained for this use case and can produce suboptimal results in understanding motion and other changes through a video feed. Realtime models, like
Gemini Live
, are trained on video and you can enable
live video input
for automatic support.
If you are using an STT-LLM-TTS pipeline, you can still work with video by sampling the video track at suitable times. For instance, in the following example the agent always includes the latest video frame on each conversation turn from the user. This provides the model with additional context without overwhelming it with data or expecting it to interpret many sequential frames at a time:
agent.py
Required imports
class
Assistant
(
Agent
)
:
def
__init__
(
self
)
-
>
None
:
self
.
_latest_frame
=
None
self
.
_video_stream
=
None
self
.
_tasks
=
[
]
super
(
)
.
__init__
(
instructions
=
"You are a helpful voice AI assistant."
)
async
def
on_enter
(
self
)
:
room
=
get_job_context
(
)
.
room
# Find the first video track (if any) from the remote participant
remote_participant
=
list
(
room
.
remote_participants
.
values
(
)
)
[
0
]
video_tracks
=
[
publication
.
track
for
publication
in
list
(
remote_participant
.
track_publications
.
values
(
)
)
if
publication
.
track
.
kind
==
rtc
.
TrackKind
.
KIND_VIDEO
]
if
video_tracks
:
self
.
_create_video_stream
(
video_tracks
[
0
]
)
# Watch for new video tracks not yet published
@room
.
on
(
"track_subscribed"
)
def
on_track_subscribed
(
track
:
rtc
.
Track
,
publication
:
rtc
.
RemoteTrackPublication
,
participant
:
rtc
.
RemoteParticipant
)
:
if
track
.
kind
==
rtc
.
TrackKind
.
KIND_VIDEO
:
self
.
_create_video_stream
(
track
)
async
def
on_user_turn_completed
(
self
,
turn_ctx
:
ChatContext
,
new_message
:
ChatMessage
)
-
>
None
:
# Add the latest video frame, if any, to the new message
if
self
.
_latest_frame
:
new_message
.
content
.
append
(
ImageContent
(
image
=
self
.
_latest_frame
)
)
self
.
_latest_frame
=
None
# Helper method to buffer the latest video frame from the user's track
def
_create_video_stream
(
self
,
track
:
rtc
.
Track
)
:
# Close any existing stream (we only want one at a time)
if
self
.
_video_stream
is
not
None
:
self
.
_video_stream
.
close
(
)
# Create a new stream to receive frames
self
.
_video_stream
=
rtc
.
VideoStream
(
track
)
async
def
read_stream
(
)
:
async
for
event
in
self
.
_video_stream
:
# Store the latest frame for use later
self
.
_latest_frame
=
event
.
frame
# Store the async task
task
=
asyncio
.
create_task
(
read_stream
(
)
)
task
.
add_done_callback
(
lambda
t
:
self
.
_tasks
.
remove
(
t
)
)
self
.
_tasks
.
append
(
task
)
Video frame encoding
By default, the
ImageContent
encodes video frames as JPEG at their native size. To adjust the size of the encoded frames, set the
inference_width
and
inference_height
parameters. Each frame is resized to fit within the provided dimensions while maintaining the original aspect ratio. For more control, use the
encode
method of the
livekit.agents.utils.images
module and pass the result as a data URL:
agent.py
Required imports
image_bytes
=
encode
(
event
.
frame
,
EncodeOptions
(
format
=
"PNG"
,
resize_options
=
ResizeOptions
(
width
=
512
,
height
=
512
,
strategy
=
"scale_aspect_fit"
)
)
)
image_content
=
ImageContent
(
image
=
f"data:image/png;base64,
{
base64
.
b64encode
(
image_bytes
)
.
decode
(
'utf-8'
)
}
"
)
Inference detail
If your LLM provider supports it, you can set the
inference_detail
parameter to
"high"
or
"low"
to control the token usage and inference quality applied. The default is
"auto"
, which uses the provider's default.
Live video
Limited support
Live video input requires a realtime model with video support. Currently, only
Gemini Live
has this feature.
Set the
video_enabled
parameter to
True
in
RoomInputOptions
to enable live video input. Your agent automatically receives frames from the user's
camera
or
screen sharing
tracks, if available. Only the single most recently published video track is used.
By default the agent samples one frame per second while the user speaks, and one frame every three seconds otherwise. Each frame is fit into 1024x1024 and encoded to JPEG. To override the frame rate, set
video_sampler
on the
AgentSession
with a custom instance.
Video input is passive and has no effect on
turn detection
. To leverage live video input in a non-conversational context, use
manual turn control
and trigger LLM responses or tool calls on a timer or other schedule.
The following example shows how to add Gemini Live vision to your
voice AI quickstart
agent:
agent.py
Required imports
class
VideoAssistant
(
Agent
)
:
def
__init__
(
self
)
-
>
None
:
super
(
)
.
__init__
(
instructions
=
"You are a helpful voice assistant with live video input from your user."
,
llm
=
google
.
beta
.
realtime
.
RealtimeModel
(
voice
=
"Puck"
,
temperature
=
0.8
,
)
,
)
async
def
entrypoint
(
ctx
:
JobContext
)
:
session
=
AgentSession
(
)
await
session
.
start
(
agent
=
VideoAssistant
(
)
,
room
=
ctx
.
room
,
room_input_options
=
RoomInputOptions
(
video_enabled
=
True
,
# ... noise_cancellation, etc.
)
,
)
Additional resources
The following documentation and examples can help you get started with vision in LiveKit Agents.
Voice AI quickstart
Use the quickstart as a starting base for adding vision code.
Byte streams
Send images from your frontend to your agent with byte streams.
RoomIO
Learn more about
RoomIO
and how it manages tracks.
Vision Assistant
A voice AI agent with video input powered by Gemini Live.
Camera and microphone
Publish camera and microphone tracks from your frontend.
Screen sharing
Publish screen sharing tracks from your frontend.
On this page
Overview
Images
Load into initial context
Upload from frontend
Sample video frames
Inference detail
Live video
Additional resources


Content from https://docs.livekit.io/agents/build/tools:

On this page
Overview
Tool definition
Name and description
Arguments
Return value
RunContext
Adding tools dynamically
Creating tools programmatically
Creating tools from raw schema
Error handling
Model Context Protocol (MCP)
Forwarding to the frontend
Agent implementation
Frontend implementation
Slow tool calls
External tools and MCP
Examples
Further reading
Copy page
See more page options
Overview
LiveKit Agents has full support for LLM tool use. This feature allows you to create a custom library of tools to extend your agent's context, create interactive experiences, and overcome LLM limitations. Within a tool, you can:
Generate
agent speech
with
session.say()
or
session.generate_reply()
.
Call methods on the frontend using
RPC
.
Handoff control to another agent as part of a
workflow
.
Store and retrieve session data from the
context
.
Anything else that a Python function can do.
Call external APIs or lookup data for RAG
.
Tool definition
Add tools to your agent class with the
@function_tool
decorator. The LLM has access to them automatically.
from
livekit
.
agents
import
function_tool
,
Agent
,
RunContext
class
MyAgent
(
Agent
)
:
@function_tool
(
)
async
def
lookup_weather
(
self
,
context
:
RunContext
,
location
:
str
,
)
-
>
dict
[
str
,
Any
]
:
"""Look up weather information for a given location.
Args:
location: The location to look up weather information for.
"""
return
{
"weather"
:
"sunny"
,
"temperature_f"
:
70
}
Best practices
A good tool definition is key to reliable tool use from your LLM. Be specific about what the tool does, when it should or should not be used, what the arguments are for, and what type of return value to expect.
Name and description
By default, the tool name is the name of the function, and the description is its docstring. Override this behavior with the
name
and
description
arguments to the
@function_tool
decorator.
Arguments
The tool arguments are copied automatically by name from the function arguments. Type hints for arguments are included, if present.
Place additional information about the tool arguments, if needed, in the tool description.
Return value
The tool return value is automatically converted to a string before being sent to the LLM. The LLM generates a new reply or additional tool calls based on the return value. Return
None
or nothing at all to complete the tool silently without requiring a reply from the LLM.
You can return an
Agent
instance to initiate a
handoff within a workflow
. You can additionally return the result of the original tool call to the LLM by returning a tuple that includes both the
Agent
instance and the result. The tool call and subsequent LLM reply are completed prior to the handoff.
When a handoff occurs, prompt the LLM to inform the user:
@function_tool
(
)
async
def
my_tool
(
context
:
RunContext
)
:
return
SomeAgent
(
)
,
"Transferring the user to SomeAgent"
RunContext
Tools include support for a special
context
argument. This contains access to the current
session
,
function_call
,
speech_handle
, and
userdata
. Consult the documentation on
speech
and
state within workflows
for more information about how to use these features.
Adding tools dynamically
You can exercise more control over the tools available by setting the
tools
argument directly.
To share a tool between multiple agents, define it outside of their class and then provide it to each. The
RunContext
is especially useful for this purpose to access the current session, agent, and state.
Tools set in the
tools
value are available alongside any registered within the class using the
@function_tool
decorator.
from
livekit
.
agents
import
function_tool
,
Agent
,
RunContext
@function_tool
(
)
async
def
lookup_user
(
context
:
RunContext
,
user_id
:
str
,
)
-
>
dict
:
"""Look up a user's information by ID."""
return
{
"name"
:
"John Doe"
,
"email"
:
"john.doe@example.com"
}
class
AgentA
(
Agent
)
:
def
__init__
(
self
)
:
super
(
)
.
__init__
(
tools
=
[
lookup_user
]
,
# ...
)
class
AgentB
(
Agent
)
:
def
__init__
(
self
)
:
super
(
)
.
__init__
(
tools
=
[
lookup_user
]
,
# ...
)
Use
agent.update_tools()
to update available tools after creating an agent. This replaces
all
tools, including those registered automatically within the agent class. To reference existing tools before replacement, access the
agent.tools
property:
# add a tool
await
agent
.
update_tools
(
agent
.
tools
+
[
tool_a
]
)
# remove a tool
await
agent
.
update_tools
(
agent
.
tools
-
[
tool_a
]
)
# replace all tools
await
agent
.
update_tools
(
[
tool_a
,
tool_b
]
)
Creating tools programmatically
To create a tool on the fly, use
function_tool
as a function rather than as a decorator. You must supply a name, description, and callable function. This is useful to compose specific tools based on the same underlying code or load them from external sources such as a database or Model Context Protocol (MCP) server.
In the following example, the app has a single function to set any user profile field but gives the agent one tool per field for improved reliability:
from
livekit
.
agents
import
function_tool
,
RunContext
class
Assistant
(
Agent
)
:
def
_set_profile_field_func_for
(
self
,
field
:
str
)
:
async
def
set_value
(
context
:
RunContext
,
value
:
str
)
:
# custom logic to set input
return
f"field
{
field
}
was set to
{
value
}
"
return
set_value
def
__init__
(
self
)
:
super
(
)
.
__init__
(
tools
=
[
function_tool
(
self
.
_set_profile_field_func_for
(
"phone"
)
,
name
=
"set_phone_number"
,
description
=
"Call this function when user has provided their phone number."
)
,
function_tool
(
self
.
_set_profile_field_func_for
(
"email"
)
,
name
=
"set_email"
,
description
=
"Call this function when user has provided their email."
)
,
# ... other tools ...
]
,
# instructions, etc ...
)
Creating tools from raw schema
For advanced use cases, you can create tools directly from a
raw function calling schema
. This is useful when integrating with existing function definitions, loading tools from external sources, or working with schemas that don't map cleanly to Python function signatures.
Use the
raw_schema
parameter in the
@function_tool
decorator to provide the full function schema:
from
livekit
.
agents
import
function_tool
,
RunContext
raw_schema
=
{
"type"
:
"function"
,
"name"
:
"get_weather"
,
"description"
:
"Get weather for a given location."
,
"parameters"
:
{
"type"
:
"object"
,
"properties"
:
{
"location"
:
{
"type"
:
"string"
,
"description"
:
"City and country e.g. New York"
}
}
,
"required"
:
[
"location"
]
,
"additionalProperties"
:
False
}
}
@function_tool
(
raw_schema
=
raw_schema
)
async
def
get_weather
(
raw_arguments
:
dict
[
str
,
object
]
,
context
:
RunContext
)
:
location
=
raw_arguments
[
"location"
]
# Your implementation here
return
f"The weather of
{
location
}
is ..."
When using raw schemas, function parameters are passed to your handler as a dictionary named
raw_arguments
. You can extract values from this dictionary using the parameter names defined in your schema.
You can also create tools programmatically using
function_tool
as a function with a raw schemas:
from
livekit
.
agents
import
function_tool
def
create_database_tool
(
table_name
:
str
,
operation
:
str
)
:
schema
=
{
"type"
:
"function"
,
"name"
:
f"
{
operation
}
_
{
table_name
}
"
,
"description"
:
f"Perform
{
operation
}
operation on
{
table_name
}
table"
,
"parameters"
:
{
"type"
:
"object"
,
"properties"
:
{
"record_id"
:
{
"type"
:
"string"
,
"description"
:
f"ID of the record to
{
operation
}
"
}
}
,
"required"
:
[
"record_id"
]
}
}
async
def
handler
(
raw_arguments
:
dict
[
str
,
object
]
,
context
:
RunContext
)
:
record_id
=
raw_arguments
[
"record_id"
]
# Perform database operation
return
f"Performed
{
operation
}
on
{
table_name
}
for record
{
record_id
}
"
return
function_tool
(
handler
,
raw_schema
=
schema
)
# Create tools dynamically
user_tools
=
[
create_database_tool
(
"users"
,
"read"
)
,
create_database_tool
(
"users"
,
"update"
)
,
create_database_tool
(
"users"
,
"delete"
)
]
class
DataAgent
(
Agent
)
:
def
__init__
(
self
)
:
super
(
)
.
__init__
(
instructions
=
"You are a database assistant."
,
tools
=
user_tools
,
)
Error handling
Raise the
ToolError
exception to return an error to the LLM in place of a response. You can include a custom message to describe the error and/or recovery options.
@function_tool
(
)
async
def
lookup_weather
(
self
,
context
:
RunContext
,
location
:
str
,
)
-
>
dict
[
str
,
Any
]
:
if
location
==
"mars"
:
raise
ToolError
(
"This location is coming soon. Please join our mailing list to stay updated."
)
else
:
return
{
"weather"
:
"sunny"
,
"temperature_f"
:
70
}
Model Context Protocol (MCP)
LiveKit Agents has full support for
MCP
servers to load tools from external sources.
To use it, first install the
mcp
optional dependencies:
pip
install
livekit-agents
[
mcp
]
~
=
1.0
Then pass the MCP server URL to the
AgentSession
or
Agent
constructor. The tools will be automatically loaded like any other tool.
from
livekit
.
agents
import
mcp
session
=
AgentSession
(
#... other arguments ...
mcp_servers
=
[
mcp
.
MCPServerHTTP
(
"https://your-mcp-server.com"
)
]
)
from
livekit
.
agents
import
mcp
agent
=
Agent
(
#... other arguments ...
mcp_servers
=
[
mcp
.
MCPServerHTTP
(
"https://your-mcp-server.com"
)
]
)
Forwarding to the frontend
Forward tool calls to a frontend app using
RPC
. This is useful when the data needed to fulfill the function call is only available at the frontend. You may also use RPC to trigger actions or UI updates in a structured way.
For instance, here's a function that accesses the user's live location from their web browser:
Agent implementation
from
livekit
.
agents
import
function_tool
,
get_job_context
,
RunContext
@function_tool
(
)
async
def
get_user_location
(
context
:
RunContext
,
high_accuracy
:
bool
)
:
"""Retrieve the user's current geolocation as lat/lng.
Args:
high_accuracy: Whether to use high accuracy mode, which is slower but more precise
Returns:
A dictionary containing latitude and longitude coordinates
"""
try
:
room
=
get_job_context
(
)
.
room
participant_identity
=
next
(
iter
(
room
.
remote_participants
)
)
response
=
await
room
.
local_participant
.
perform_rpc
(
destination_identity
=
participant_identity
,
method
=
"getUserLocation"
,
payload
=
json
.
dumps
(
{
"highAccuracy"
:
high_accuracy
}
)
,
response_timeout
=
10.0
if
high_accuracy
else
5.0
,
)
return
response
except
Exception
:
raise
ToolError
(
"Unable to retrieve user location"
)
Frontend implementation
The following example uses the JavaScript SDK. The same pattern works for other SDKs. For more examples, see the
RPC documentation
.
import
{
RpcError
,
RpcInvocationData
}
from
'livekit-client'
;
localParticipant
.
registerRpcMethod
(
'getUserLocation'
,
async
(
data
:
RpcInvocationData
)
=>
{
try
{
let
params
=
JSON
.
parse
(
data
.
payload
)
;
const
position
:
GeolocationPosition
=
await
new
Promise
(
(
resolve
,
reject
)
=>
{
navigator
.
geolocation
.
getCurrentPosition
(
resolve
,
reject
,
{
enableHighAccuracy
:
params
.
highAccuracy
??
false
,
timeout
:
data
.
responseTimeout
,
}
)
;
}
)
;
return
JSON
.
stringify
(
{
latitude
:
position
.
coords
.
latitude
,
longitude
:
position
.
coords
.
longitude
,
}
)
;
}
catch
(
error
)
{
throw
new
RpcError
(
1
,
"Could not retrieve user location"
)
;
}
}
)
;
Slow tool calls
For best practices on providing feedback to the user during long-running tool calls, see the section on
user feedback
in the External data and RAG guide.
External tools and MCP
To load tools from an external source as a Model Context Protocol (MCP) server, use the
function_tool
function and register the tools with the
tools
property or
update_tools()
method. See the following example for a complete MCP implementation:
ModelContextProtocol
MCP Agent
A voice AI agent with an integrated Model Context Protocol (MCP) server for the LiveKit API.
Examples
The following additional examples show how to use tools in different ways:
Use of enum
Example showing how to annotate arguments with enum.
Dynamic tool creation
Complete example with dynamic tool lists.
ModelContextProtocol
MCP Agent
A voice AI agent with an integrated Model Context Protocol (MCP) server for the LiveKit API.
LiveKit Docs RAG
An agent that can answer questions about LiveKit with lookups against the docs website.
Further reading
The following articles provide more information about the topics discussed in this guide:
RPC
Complete documentation on function calling between LiveKit participants.
Agent speech
More information about precise control over agent speech output.
Workflows
Read more about handing off control to other agents.
External data and RAG
Best practices for adding context and taking external actions.
On this page
Overview
Tool definition
Name and description
Arguments
Return value
RunContext
Adding tools dynamically
Creating tools programmatically
Creating tools from raw schema
Error handling
Model Context Protocol (MCP)
Forwarding to the frontend
Agent implementation
Frontend implementation
Slow tool calls
External tools and MCP
Examples
Further reading


Content from https://docs.livekit.io/agents/build/nodes:

On this page
Overview
How to implement
Lifecycle hooks
On enter
On exit
On user turn completed
STT-LLM-TTS pipeline nodes
STT node
LLM node
TTS node
Realtime model nodes
Realtime audio output node
Transcription node
Examples
Copy page
See more page options
Overview
You can fully customize your agent's behavior at multiple
nodes
in the processing path.
A node is a point in the path where one process transitions to another. Some example customizations include:
Use a custom STT, LLM, or TTS provider without a plugin.
Generate a custom greeting when an agent enters a session.
Modify STT output to remove filler words before sending it to the LLM.
Modify LLM output before sending it to TTS to customize pronunciation.
Update the user interface when an agent or user finishes speaking.
The
Agent
supports the following nodes and hooks. Some nodes are only available for STT-LLM-TTS pipeline models, and others are only available for realtime models.
Lifecycle hooks:
on_enter()
: Called after the agent becomes the active agent in a session.
on_exit()
: Called before the agent gives control to another agent in the same session.
on_user_turn_completed()
: Called when the user's
turn
has ended, before the agent's reply.
STT-LLM-TTS pipeline nodes:
stt_node()
: Transcribe input audio to text.
llm_node()
: Perform inference and generate a new conversation turn (or tool call).
tts_node()
: Synthesize speech from the LLM text output.
Realtime model nodes:
realtime_audio_output_node()
: Adjust output audio before publishing to the user.
Transcription node:
transcription_node()
: Access transcription timestamps, or adjust pipeline or realtime model transcription before sending to the user.
The following diagrams show the processing path for STT-LLM-TTS pipeline models and realtime models.
STT-LLM-TTS pipeline
Realtime model
How to implement
Override the method within a custom
Agent
subclass to customize the behavior of your agent at a specific node in the processing path. To use the default, call
Agent.default.<node-name>()
. For instance, this code overrides the STT node while maintaining the default behavior.
async
def
stt_node
(
self
,
audio
:
AsyncIterable
[
rtc
.
AudioFrame
]
,
model_settings
:
ModelSettings
)
-
>
Optional
[
AsyncIterable
[
stt
.
SpeechEvent
]
]
:
# insert custom before STT processing here
events
=
Agent
.
default
.
stt_node
(
self
,
audio
,
model_settings
)
# insert custom after STT processing here
return
events
Lifecycle hooks
The following lifecycle hooks are available for customization.
On enter
The
on_enter
node is called when the agent becomes the active agent in a session. Each session can have only one active agent at a time, which can be read from the
session.agent
property. Change the active agent using
Workflows
.
For example, to greet the user:
async
def
on_enter
(
self
)
:
await
self
.
session
.
generate_reply
(
instructions
=
"Greet the user with a warm welcome"
,
)
On exit
The
on_exit
node is called before the agent gives control to another agent in the same session as part of a
workflow
. Use it to save data, say goodbye, or perform other actions and cleanup.
For example, to say goodbye:
async
def
on_exit
(
self
)
:
await
self
.
session
.
generate_reply
(
instructions
=
"Tell the user a friendly goodbye before you exit."
,
)
On user turn completed
The
on_user_turn_completed
node is called when the user's
turn
has ended, before the agent's reply. Override this method to modify the content of the turn, cancel the agent's reply, or perform other actions.
Realtime model turn detection
To use the
on_user_turn_completed
node with a
realtime model
, you must configure
turn detection
to occur in your agent instead of within the realtime model.
The node receives the following parameters:
turn_ctx
: The full
ChatContext
, up to but not including the user's latest message.
new_message
: The user's latest message, representing their current turn.
After the node is complete, the
new_message
is added to the chat context.
One common use of this node is
retrieval-augmented generation (RAG)
. You can retrieve context relevant to the newest message and inject it into the chat context for the LLM.
from
livekit
.
agents
import
ChatContext
,
ChatMessage
async
def
on_user_turn_completed
(
self
,
turn_ctx
:
ChatContext
,
new_message
:
ChatMessage
,
)
-
>
None
:
rag_content
=
await
my_rag_lookup
(
new_message
.
text_content
(
)
)
turn_ctx
.
add_message
(
role
=
"assistant"
,
content
=
f"Additional information relevant to the user's next message:
{
rag_content
}
"
)
Additional messages added in this way are not persisted beyond the current turn. To permanently add messages to the chat history, use the
update_chat_ctx
method:
async
def
on_user_turn_completed
(
self
,
turn_ctx
:
ChatContext
,
new_message
:
ChatMessage
,
)
-
>
None
:
rag_content
=
await
my_rag_lookup
(
new_message
.
text_content
(
)
)
turn_ctx
.
add_message
(
role
=
"assistant"
,
content
=
rag_content
)
await
self
.
update_chat_ctx
(
turn_ctx
)
You can also edit the
new_message
object to modify the user's message before it's added to the chat context. For example, you can remove offensive content or add additional context. These changes are persisted to the chat history going forward.
async
def
on_user_turn_completed
(
self
,
turn_ctx
:
ChatContext
,
new_message
:
ChatMessage
,
)
-
>
None
:
new_message
.
content
=
[
"... modified message ..."
]
To abort generation entirely—for example, in a push-to-talk interface—you can do the following:
async
def
on_user_turn_completed
(
self
,
turn_ctx
:
ChatContext
,
new_message
:
ChatMessage
,
)
-
>
None
:
if
not
new_message
.
text_content
:
# for example, raise StopResponse to stop the agent from generating a reply
raise
StopResponse
(
)
For a complete example, see the
multi-user agent with push to talk example
.
STT-LLM-TTS pipeline nodes
The following nodes are available for STT-LLM-TTS pipeline models.
STT node
The
stt_node
transcribes audio frames into speech events, converting user audio input into text for the LLM. By default, this node uses the Speech-To-Text (STT) capability from the current agent. If the STT implementation doesn't support streaming natively, a Voice Activity Detection (VAD) mechanism wraps the STT.
You can override this node to implement:
Custom pre-processing of audio frames
Additional buffering mechanisms
Alternative STT strategies
Post-processing of the transcribed text
To use the default implementation, call
Agent.default.stt_node()
.
This example adds a noise filtering step:
from
livekit
import
rtc
from
livekit
.
agents
import
ModelSettings
,
stt
,
Agent
from
typing
import
AsyncIterable
,
Optional
async
def
stt_node
(
self
,
audio
:
AsyncIterable
[
rtc
.
AudioFrame
]
,
model_settings
:
ModelSettings
)
-
>
Optional
[
AsyncIterable
[
stt
.
SpeechEvent
]
]
:
async
def
filtered_audio
(
)
:
async
for
frame
in
audio
:
# insert custom audio preprocessing here
yield
frame
async
for
event
in
Agent
.
default
.
stt_node
(
self
,
filtered_audio
(
)
,
model_settings
)
:
# insert custom text postprocessing here
yield
event
LLM node
The
llm_node
is responsible for performing inference based on the current chat context and creating the
agent's response or tool calls. It may yield plain text (as
str
) for straightforward text generation, or
llm.ChatChunk
objects that can include text and optional tool calls.
ChatChunk
is helpful for capturing more complex outputs such as function calls, usage statistics, or other metadata.
You can override this node to:
Customize how the LLM is used
Modify the chat context prior to inference
Adjust how tool invocations and responses are handled
Implement a custom LLM provider without a plugin
To use the default implementation, call
Agent.default.llm_node()
.
from
livekit
.
agents
import
ModelSettings
,
llm
,
FunctionTool
,
Agent
from
typing
import
AsyncIterable
async
def
llm_node
(
self
,
chat_ctx
:
llm
.
ChatContext
,
tools
:
list
[
FunctionTool
]
,
model_settings
:
ModelSettings
)
-
>
AsyncIterable
[
llm
.
ChatChunk
]
:
# Insert custom preprocessing here
async
for
chunk
in
Agent
.
default
.
llm_node
(
self
,
chat_ctx
,
tools
,
model_settings
)
:
# Insert custom postprocessing here
yield
chunk
TTS node
The
tts_node
synthesizes audio from text segments, converting the LLM output into speech. By default, this node uses the Text-To-Speech capability from the agent. If the TTS implementation doesn't support streaming natively, it uses a sentence tokenizer to split text for incremental synthesis.
You can override this node to:
Provide different text chunking behavior
Implement a custom TTS engine
Add custom pronunciation rules
Adjust the volume of the audio output
Apply any other specialized audio processing
To use the default implementation, call
Agent.default.tts_node()
.
from
livekit
import
rtc
from
livekit
.
agents
import
ModelSettings
,
Agent
from
typing
import
AsyncIterable
async
def
tts_node
(
self
,
text
:
AsyncIterable
[
str
]
,
model_settings
:
ModelSettings
)
-
>
AsyncIterable
[
rtc
.
AudioFrame
]
:
# Insert custom text processing here
async
for
frame
in
Agent
.
default
.
tts_node
(
self
,
text
,
model_settings
)
:
# Insert custom audio processing here
yield
frame
Realtime model nodes
The following nodes are available for realtime models.
Realtime audio output node
The
realtime_audio_output_node
is called when a realtime model outputs speech. This allows you to modify the audio output before it's sent to the user. For example, you can
adjust the volume of the audio output
.
To use the default implementation, call
Agent.default.realtime_audio_output_node()
.
from
livekit
.
agents
import
ModelSettings
,
rtc
,
Agent
from
typing
import
AsyncIterable
async
def
realtime_audio_output_node
(
self
,
audio
:
AsyncIterable
[
rtc
.
AudioFrame
]
,
model_settings
:
ModelSettings
)
-
>
AsyncIterable
[
rtc
.
AudioFrame
]
:
# Insert custom audio preprocessing here
async
for
frame
in
Agent
.
default
.
realtime_audio_output_node
(
self
,
audio
,
model_settings
)
:
# Insert custom audio postprocessing here
yield
frame
Transcription node
The
transcription_node
is part of the forwarding path for
agent transcriptions
and can be used to adjust or post-process text coming from an LLM (or any other source) into a final transcribed form. It may also be used to access
transcription timestamps
for TTS-aligned transcriptions.
By default, the node simply passes the transcription to the task that forwards it to the designated output. You can override this node to:
Clean up formatting
Fix punctuation
Strip unwanted characters
Perform any other text transformations
Access
transcription timestamps
for TTS-aligned transcriptions
To use the default implementation, call
Agent.default.transcription_node()
.
from
livekit
.
agents
import
ModelSettings
from
typing
import
AsyncIterable
async
def
transcription_node
(
self
,
text
:
AsyncIterable
[
str
]
,
model_settings
:
ModelSettings
)
-
>
AsyncIterable
[
str
]
:
async
for
delta
in
text
:
yield
delta
.
replace
(
"😘"
,
""
)
Examples
The following examples demonstrate advanced usage of nodes and hooks:
Restaurant Agent
A restaurant front-of-house agent demonstrates the
on_enter
and
on_exit
lifecycle hooks.
Structured Output
Handle structured output from the LLM by overriding the
llm_node
and
tts_node
.
Chain-of-thought agent
Build an agent for chain-of-thought reasoning using the
llm_node
to clean the text before TTS.
Keyword Detection
Use the
stt_node
to detect keywords in the user's speech.
LLM Content Filter
Implement content filtering in the
llm_node
.
Speedup Output Audio
Speed up the output audio of an agent with the
tts_node
or
realtime_audio_output_node
.
On this page
Overview
How to implement
Lifecycle hooks
On enter
On exit
On user turn completed
STT-LLM-TTS pipeline nodes
STT node
LLM node
TTS node
Realtime model nodes
Realtime audio output node
Transcription node
Examples


Content from https://docs.livekit.io/agents/build/text:

On this page
Overview
Transcriptions
Synchronized transcription forwarding
Accessing from AgentSession
TTS-aligned transcriptions
Text input
Text-only session
Disable audio for the entire session
Toggle audio input/output
Frontend integration
Receiving text streams
Sending text input
Manual text input
Transcription events
Copy page
See more page options
Overview
LiveKit Agents supports text inputs and outputs in addition to audio, based on the
text streams
feature of the LiveKit SDKs. This guide explains what's possible and how to use it in your app.
Transcriptions
When an agent performs STT as part of its processing pipeline, the transcriptions are also published to the frontend in realtime. Additionally, a text representation of the agent speech is also published in sync with audio playback when the agent speaks. These features are both enabled by default when using
AgentSession
.
Transcriptions use the
lk.transcription
text stream topic. They include a
lk.transcribed_track_id
attribute and the sender identity is the transcribed participant.
To disable transcription output, set
transcription_enabled=False
in
RoomOutputOptions
.
Synchronized transcription forwarding
When both voice and transcription are enabled, the agent's speech is synchronized with its transcriptions, displaying text word by word as it speaks. If the agent is interrupted, the transcription stops and is truncated to match the spoken output.
Disabling synchronization
To send transcriptions to the client as soon as they become available, without synchronizing to the original speech, set
sync_transcription
to False in
RoomOutputOptions
.
await
session
.
start
(
agent
=
MyAgent
(
)
,
room
=
ctx
.
room
,
room_output_options
=
RoomOutputOptions
(
sync_transcription
=
False
)
,
)
Accessing from AgentSession
You can be notified within your agent whenever text input or output is committed to the chat history by listening to the
conversation_item_added
event.
TTS-aligned transcriptions
If your TTS provider supports it, you can enable TTS-aligned transcription forwarding to improve transcription synchronization to your frontend. This feature synchronizes the transcription output with the actual speech timing, enabling word-level synchronization. When using this feature, certain formatting may be lost from the original text (dependent on the TTS provider).
Currently, only
Cartesia
and
ElevenLabs
support word-level transcription timing. For other providers, the alignment is applied at the sentence level and still improves synchronization reliability for multi-sentence turns.
To enable this feature, set
use_tts_aligned_transcript=True
in your
AgentSession
configuration:
session
=
AgentSession
(
# ... stt, llm, tts, vad, etc...
use_tts_aligned_transcript
=
True
,
)
To access timing information in your code, implement a
transcription_node
method in your agent. The iterator yields a
TimedString
which includes
start_time
and
end_time
for each word, in seconds relative to the start of the agent's current
turn
.
Experimental feature
The
transcription_node
and
TimedString
implementations are experimental and may change in a future version of the SDK.
async
def
transcription_node
(
self
,
text
:
AsyncIterable
[
str
|
TimedString
]
,
model_settings
:
ModelSettings
)
-
>
AsyncGenerator
[
str
|
TimedString
,
None
]
:
async
for
chunk
in
text
:
if
isinstance
(
chunk
,
TimedString
)
:
logger
.
info
(
f"TimedString: '
{
chunk
}
' (
{
chunk
.
start_time
}
-
{
chunk
.
end_time
}
)"
)
yield
chunk
Text input
Your agent also monitors the
lk.chat
text stream topic for incoming text messages from its linked participant. The agent interrupts its current speech, if any, to process the message and generate a new response.
To disable text input, set
text_enabled=False
in
RoomInputOptions
.
Text-only session
Disable audio for the entire session
To disable audio input or output for the entire session, set
audio_enabled=False
in
RoomInputOptions
or
RoomOutputOptions
. When audio output is disabled, the agent will not publish audio tracks to the room. Text responses will be sent without the
lk.transcribed_track_id
attribute and without speech synchronization.
session
=
AgentSession
(
.
.
.
,
room_input_options
=
RoomInputOptions
(
audio_enabled
=
False
)
,
room_output_options
=
RoomOutputOptions
(
audio_enabled
=
False
)
,
)
Toggle audio input/output
For hybrid sessions where audio input and output may be used, such as when a user toggles an audio switch, the audio track should remain published to the room. The agent can toggle audio input and output dynamically using
session.input.set_audio_enabled()
and
session.output.set_audio_enabled()
.
session
=
AgentSession
(
.
.
.
)
# start with audio disabled
session
.
input
.
set_audio_enabled
(
False
)
session
.
output
.
set_audio_enabled
(
False
)
await
session
.
start
(
.
.
.
)
# user toggles audio switch
@room
.
local_participant
.
register_rpc_method
(
"toggle_audio"
)
async
def
on_toggle_audio
(
data
:
rtc
.
RpcInvocationData
)
-
>
None
:
session
.
input
.
set_audio_enabled
(
not
session
.
input
.
audio_enabled
)
session
.
output
.
set_audio_enabled
(
not
session
.
output
.
audio_enabled
)
Frontend integration
LiveKit client SDKs have native support for text streams. For more information, see the
text streams
documentation.
Receiving text streams
Use the
registerTextStreamHandler
method to receive incoming transcriptions or text:
JavaScript
Swift
room
.
registerTextStreamHandler
(
'lk.transcription'
,
async
(
reader
,
participantInfo
)
=>
{
const
message
=
await
reader
.
readAll
(
)
;
if
(
reader
.
info
.
attributes
[
'lk.transcribed_track_id'
]
)
{
console
.
log
(
`
New transcription from
${
participantInfo
.
identity
}
:
${
message
}
`
)
;
}
else
{
console
.
log
(
`
New message from
${
participantInfo
.
identity
}
:
${
message
}
`
)
;
}
}
)
;
Sending text input
Use the
sendText
method to send text messages:
JavaScript
Swift
const
text
=
'Hello how are you today?'
;
const
info
=
await
room
.
localParticipant
.
sendText
(
text
,
{
topic
:
'lk.chat'
,
}
)
;
Manual text input
To insert text input and generate a response, use the
generate_reply
method of AgentSession:
session.generate_reply(user_input="...")
.
Transcription events
Frontend SDKs can also receive transcription events via
RoomEvent.TranscriptionReceived
.
Deprecated feature
Transcription events will be removed in a future version. Use
text streams
on the
lk.chat
topic instead.
Android
Flutter
JavaScript
Swift
room
.
events
.
collect
{
event
->
if
(
event
is
RoomEvent
.
TranscriptionReceived
)
{
event
.
transcriptionSegments
.
forEach
{
segment
->
println
(
"New transcription from
${
segment
.
senderIdentity
}
:
${
segment
.
text
}
"
)
}
}
}
On this page
Overview
Transcriptions
Synchronized transcription forwarding
Accessing from AgentSession
TTS-aligned transcriptions
Text input
Text-only session
Disable audio for the entire session
Toggle audio input/output
Frontend integration
Receiving text streams
Sending text input
Manual text input
Transcription events


Content from https://docs.livekit.io/agents/build/turns/:

On this page
Overview
Turn detection
Turn detector model
Realtime models
VAD only
STT endpointing
Manual turn control
Reducing background noise
Interruptions
Session configuration
Turn-taking events
Further reading
Copy page
See more page options
Overview
Turn detection is the process of determining when a user begins or ends their "turn" in a conversation. This lets the agent know when to start listening and when to respond.
Most turn detection techniques rely on voice activity detection (VAD) to detect periods of silence in user input. The agent applies heuristics to the VAD data to perform phrase endpointing, which determines the end of a sentence or thought. The agent can use endpoints alone or apply more contextual analysis to determine when a turn is complete.
Effective turn detection and interruption management is essential to great voice AI experiences.
Turn detection
The
AgentSession
supports the following turn detection modes, in addition to manual turn control that's always available.
Turn detector model
: A custom, open-weights model for context-aware turn detection on top of VAD or STT endpoint data.
Realtime models
: Support for the built-in turn detection or VAD in realtime models like the OpenAI Realtime API.
VAD only
: Detect end of turn from speech and silence data alone.
STT endpointing
: Use phrase endpoints returned in realtime STT data from your chosen provider.
Manual turn control
: Disable automatic turn detection entirely.
Turn detector model
To achieve the recommended behavior of an agent that listens while the user speaks and replies after they finish their thought, use the following plugins in an STT-LLM-TTS pipeline:
Turn detection model
Open-weights model for contextually-aware turn detection.
Silero VAD
Silero VAD model for voice activity detection.
from
livekit
.
plugins
.
turn_detector
.
multilingual
import
MultilingualModel
from
livekit
.
plugins
import
silero
session
=
AgentSession
(
turn_detection
=
MultilingualModel
(
)
,
# or EnglishModel()
vad
=
silero
.
VAD
.
load
(
)
,
# ... stt, tts, llm, etc.
)
See the
Voice AI quickstart
for a complete example.
Realtime model turn detection
For a realtime model, LiveKit recommends using the built-in turn detection capabilities of the
chosen model provider
. This is the most cost-effective option, since the custom turn detection model requires realtime speech-to-text (STT) that would need to run separately.
Realtime models
Realtime models include built-in turn detection options based on VAD and other techniques. Leave the
turn_detection
parameter unset and configure the realtime model's turn detection options directly.
To use the LiveKit turn detector model with a realtime model, you must also provide an STT plugin. The turn detector model operates on STT output.
OpenAI Realtime API turn detection
Turn detection options for the OpenAI Realtime API.
Gemini Live API turn detection
Turn detection options for the Gemini Live API.
VAD only
In some cases, VAD is the best option for turn detection. For example, VAD works with any spoken language. To use VAD alone, use the Silero VAD plugin and set
turn_detection="vad"
.
session
=
AgentSession
(
turn_detection
=
"vad"
,
vad
=
silero
.
VAD
.
load
(
)
,
# ... stt, tts, llm, etc.
)
STT endpointing
You can also use your STT model's built-in phrase endpointing features for turn detection. Some providers, including
AssemblyAI
, include sophisticated semantic turn detection models.
You should still provide a VAD plugin for responsive interruption handling. When you use STT endpointing only, your agent is less responsive to user interruptions.
To use STT endpointing, set
turn_detection="stt"
and provide an STT plugin.
session
=
AgentSession
(
turn_detection
=
"stt"
,
stt
=
assemblyai
.
STT
(
)
,
# AssemblyAI is the recommended STT plugin for STT-based endpointing
vad
=
silero
.
VAD
.
load
(
)
,
# Recommended for responsive interruption handling
# ... tts, llm, etc.
)
Manual turn control
Disable automatic turn detection entirely by setting
turn_detection="manual"
in the
AgentSession
constructor.
You can now control the user's turn with
session.interrupt()
,
session.clear_user_turn()
, and
session.commit_user_turn()
methods.
For instance, you can use this to implement a push-to-talk interface. Here is a simple example using
RPC
methods that the frontend can call:
session
=
AgentSession
(
turn_detection
=
"manual"
,
# ... stt, tts, llm, etc.
)
# Disable audio input at the start
session
.
input
.
set_audio_enabled
(
False
)
# When user starts speaking
@ctx
.
room
.
local_participant
.
register_rpc_method
(
"start_turn"
)
async
def
start_turn
(
data
:
rtc
.
RpcInvocationData
)
:
session
.
interrupt
(
)
# Stop any current agent speech
session
.
clear_user_turn
(
)
# Clear any previous input
session
.
input
.
set_audio_enabled
(
True
)
# Start listening
# When user finishes speaking
@ctx
.
room
.
local_participant
.
register_rpc_method
(
"end_turn"
)
async
def
end_turn
(
data
:
rtc
.
RpcInvocationData
)
:
session
.
input
.
set_audio_enabled
(
False
)
# Stop listening
session
.
commit_user_turn
(
)
# Process the input and generate response
# When user cancels their turn
@ctx
.
room
.
local_participant
.
register_rpc_method
(
"cancel_turn"
)
async
def
cancel_turn
(
data
:
rtc
.
RpcInvocationData
)
:
session
.
input
.
set_audio_enabled
(
False
)
# Stop listening
session
.
clear_user_turn
(
)
# Discard the input
A more complete example is available here:
Push-to-Talk Agent
A voice AI agent that uses push-to-talk for controlled multi-participant conversations, only enabling audio input when explicitly triggered.
Reducing background noise
Enhanced noise cancellation
is available in LiveKit Cloud and improves the quality of turn detection and speech-to-text (STT) for voice AI apps. You can add background noise and voice cancellation to your agent by adding it to the
room_input_options
when you start your agent session. To learn how to enable it, see the
Voice AI quickstart
.
Interruptions
The user can interrupt the agent at any time, either by speaking with automatic turn detection or via the
session.interrupt()
method. When an interruption occurs, the agent stops speaking and automatically truncates its conversation history to reflect only the speech that the user actually heard before interruption.
Session configuration
The following parameters related to turn detection and interruptions are available on the
AgentSession
constructor:
allow_interruptions
bool
Optional
Default:
True
#
Whether to allow the user to interrupt the agent mid-turn. Ignored when using a realtime model with built-in turn detection.
min_interruption_duration
float
Optional
Default:
0.5
#
Minimum detected speech duration before triggering an interruption.
min_endpointing_delay
float
Optional
Default:
0.5
#
The number of seconds to wait before considering the turn complete. The session uses this delay when no turn detector model is present, or when the model indicates a likely turn boundary.
max_endpointing_delay
float
Optional
Default:
6.0
#
The maximum time to wait for the user to speak after the turn detector model indicates the user is likely to continue speaking. This parameter has no effect without the turn detector model.
Turn-taking events
The
AgentSession
exposes user and agent state events to monitor the flow of a conversation:
from
livekit
.
agents
import
UserStateChangedEvent
,
AgentStateChangedEvent
@session
.
on
(
"user_state_changed"
)
def
on_user_state_changed
(
ev
:
UserStateChangedEvent
)
:
if
ev
.
new_state
==
"speaking"
:
print
(
"User started speaking"
)
elif
ev
.
new_state
==
"listening"
:
print
(
"User stopped speaking"
)
elif
ev
.
new_state
==
"away"
:
print
(
"User is not present (e.g. disconnected)"
)
@session
.
on
(
"agent_state_changed"
)
def
on_agent_state_changed
(
ev
:
AgentStateChangedEvent
)
:
if
ev
.
new_state
==
"initializing"
:
print
(
"Agent is starting up"
)
elif
ev
.
new_state
==
"idle"
:
print
(
"Agent is ready but not processing"
)
elif
ev
.
new_state
==
"listening"
:
print
(
"Agent is listening for user input"
)
elif
ev
.
new_state
==
"thinking"
:
print
(
"Agent is processing user input and generating a response"
)
elif
ev
.
new_state
==
"speaking"
:
print
(
"Agent started speaking"
)
Further reading
Agent speech
Guide to agent speech and related methods.
Pipeline nodes
Monitor input and output as it flows through the voice pipeline.
On this page
Overview
Turn detection
Turn detector model
Realtime models
VAD only
STT endpointing
Manual turn control
Reducing background noise
Interruptions
Session configuration
Turn-taking events
Further reading


Content from https://docs.livekit.io/agents/build/external-data:

On this page
Overview
Initial context
Tool calls
Add context during conversation
User feedback
Verbal status updates
"Thinking" sounds
Frontend UI
Fine-tuned models
RAG providers and services
Additional examples
Copy page
See more page options
Overview
Your agent can connect to external data sources to retrieve information, store data, or take other actions. In general, you can install any Python package or add custom code to the agent to use any database or API that you need.
For instance, your agent might need to:
Load a user's profile information from a database before starting a conversation.
Search a private knowledge base for information to accurately answer user queries.
Perform read/write/update operations on a database or service such as a calendar.
Store conversation history or other data to a remote server.
This guide covers best practices and techniques for job initialization, retrieval-augmented generation (RAG), tool calls, and other techniques to connect your agent to external data sources and other systems.
Initial context
By default, each
AgentSession
begins with an empty chat context. You can load user or task-specific data into the agent's context before connecting to the room and starting the session. For instance, this agent greets the user by name based on the
job metadata
.
from
livekit
import
agents
from
livekit
.
agents
import
Agent
,
ChatContext
,
AgentSession
class
Assistant
(
Agent
)
:
def
__init__
(
self
,
chat_ctx
:
ChatContext
)
-
>
None
:
super
(
)
.
__init__
(
chat_ctx
=
chat_ctx
,
instructions
=
"You are a helpful voice AI assistant."
)
async
def
entrypoint
(
ctx
:
agents
.
JobContext
)
:
# Simple lookup, but you could use a database or API here if needed
metadata
=
json
.
loads
(
ctx
.
job
.
metadata
)
user_name
=
metadata
[
"user_name"
]
session
=
AgentSession
(
# ... stt, llm, tts, vad, turn_detection, etc.
)
initial_ctx
=
ChatContext
(
)
initial_ctx
.
add_message
(
role
=
"assistant"
,
content
=
f"The user's name is
{
user_name
}
."
)
await
session
.
start
(
room
=
ctx
.
room
,
agent
=
Assistant
(
chat_ctx
=
initial_ctx
)
,
# ... room_input_options, etc.
)
await
session
.
generate_reply
(
instructions
=
"Greet the user by name and offer your assistance."
)
Load time optimizations
If your agent requires external data in order to start, the following tips can help minimize the impact to the user experience:
For static data (not user-specific) load it in the
prewarm function
Send user specific data in the
job metadata
,
room metadata
, or
participant attributes
rather than loading it in the entrypoint.
If you must make a network call in the entrypoint, do so before
ctx.connect()
. This ensures your frontend doesn't show the agent participant before it is listening to incoming audio.
Tool calls
To achieve the highest degree of precision or take external actions, you can offer the LLM a choice of
tools
to use in its response. These tools can be as generic or as specific as needed for your use case.
For instance, define tools for
search_calendar
,
create_event
,
update_event
, and
delete_event
to give the LLM complete access to the user's calendar. Use
participant attributes
or
job metadata
to pass the user's calendar ID and access tokens to the agent.
Tool definition and use
Guide to defining and using custom tools in LiveKit Agents.
Add context during conversation
You can use the
on_user_turn_completed node
to perform a RAG lookup based on the user's most recent turn, prior to the LLM generating a response. This method can be highly performant as it avoids the extra round-trips involved in tool calls, but it's only available for STT-LLM-TTS pipelines that have access to the user's turn in text form. Additionally, the results are only as good as the accuracy of the search function you implement.
For instance, you can use vector search to retrieve additional context relevant to the user's query and inject it into the chat context for the next LLM generation. Here is a simple example:
from
livekit
.
agents
import
ChatContext
,
ChatMessage
async
def
on_user_turn_completed
(
self
,
turn_ctx
:
ChatContext
,
new_message
:
ChatMessage
,
)
-
>
None
:
# RAG function definition omitted for brevity
rag_content
=
await
my_rag_lookup
(
new_message
.
text_content
(
)
)
turn_ctx
.
add_message
(
role
=
"assistant"
,
content
=
f"Additional information relevant to the user's next message:
{
rag_content
}
"
)
User feedback
It’s important to provide users with direct feedback about status updates—for example, to explain a delay or failure. Here are a few example use cases:
When an operation takes more than a few hundred milliseconds.
When performing write operations such as sending an email or scheduling a meeting.
When the agent is unable to perform an operation.
The following section describes various techniques to provide this feedback to the user.
Verbal status updates
Use
Agent speech
to provide verbal feedback to the user during a long-running tool call or other operation.
In the following example, the agent speaks a status update only if the call takes longer than a specified timeout. The update is dynamically generated based on the query, and could be extended to include an estimate of the remaining time or other information.
import
asyncio
from
livekit
.
agents
import
function_tool
,
RunContext
@function_tool
(
)
async
def
search_knowledge_base
(
self
,
context
:
RunContext
,
query
:
str
,
)
-
>
str
:
# Send a verbal status update to the user after a short delay
async
def
_speak_status_update
(
delay
:
float
=
0.5
)
:
await
asyncio
.
sleep
(
delay
)
await
context
.
session
.
generate_reply
(
instructions
=
f"""
You are searching the knowledge base for \"
{
query
}
\" but it is taking a little while.
Update the user on your progress, but be very brief.
"""
)
status_update_task
=
asyncio
.
create_task
(
_speak_status_update
(
0.5
)
)
# Perform search (function definition omitted for brevity)
result
=
await
_perform_search
(
query
)
# Cancel status update if search completed before timeout
status_update_task
.
cancel
(
)
return
result
For more information, see the following article:
Agent speech
Explore the speech capabilities and features of LiveKit Agents.
"Thinking" sounds
Add
background audio
to play a "thinking" sound automatically when tool calls are ongoing. This can be useful to provide a more natural feel to the agent's responses.
from
livekit
.
agents
import
BackgroundAudioPlayer
,
AudioConfig
,
BuiltinAudioClip
async
def
entrypoint
(
ctx
:
agents
.
JobContext
)
:
session
=
AgentSession
(
# ... stt, llm, tts, vad, turn_detection, etc.
)
await
session
.
start
(
room
=
ctx
.
room
,
# ... agent, etc.
)
background_audio
=
BackgroundAudioPlayer
(
thinking_sound
=
[
AudioConfig
(
BuiltinAudioClip
.
KEYBOARD_TYPING
,
volume
=
0.8
)
,
AudioConfig
(
BuiltinAudioClip
.
KEYBOARD_TYPING2
,
volume
=
0.7
)
,
]
,
)
await
background_audio
.
start
(
room
=
ctx
.
room
,
agent_session
=
session
)
Frontend UI
If your app includes a frontend, you can add custom UI to represent the status of the agent's operations. For instance, present a popup for a long-running operation that the user can optionally cancel:
from
livekit
.
agents
import
get_job_context
import
json
import
asyncio
@function_tool
(
)
async
def
perform_deep_search
(
self
,
context
:
RunContext
,
summary
:
str
,
query
:
str
,
)
-
>
str
:
"""
Initiate a deep internet search that will reference many external sources to answer the given query. This may take 1-5 minutes to complete.
Summary: A user-friendly summary of the query
Query: the full query to be answered
"""
async
def
_notify_frontend
(
query
:
str
)
:
room
=
get_job_context
(
)
.
room
response
=
await
room
.
local_participant
.
perform_rpc
(
destination_identity
=
next
(
iter
(
room
.
remote_participants
)
)
,
# frontend method that shows a cancellable popup
# (method definition omitted for brevity, see RPC docs)
method
=
'start_deep_search'
,
payload
=
json
.
dumps
(
{
"summary"
:
summary
,
"estimated_completion_time"
:
300
,
}
)
,
# Allow the frontend a long time to return a response
response_timeout
=
500
,
)
# In this example the frontend has a Cancel button that returns "cancelled"
# to stop the task
if
response
==
"cancelled"
:
deep_search_task
.
cancel
(
)
notify_frontend_task
=
asyncio
.
create_task
(
_notify_frontend
(
query
)
)
# Perform deep search (function definition omitted for brevity)
deep_search_task
=
asyncio
.
create_task
(
_perform_deep_search
(
query
)
)
try
:
result
=
await
deep_search_task
except
asyncio
.
CancelledError
:
result
=
"Search cancelled by user"
finally
:
notify_frontend_task
.
cancel
(
)
return
result
For more information and examples, see the following articles:
Web and mobile frontends
Guide to building a custom web or mobile frontend for your agent.
RPC
Learn how to use RPC to communicate with your agent from the frontend.
Fine-tuned models
Sometimes the best way to get the most relevant results is to fine-tune a model for your specific use case. You can explore the available
LLM integrations
to find a provider that supports fine-tuning, or use
Ollama
to integrate a custom model.
RAG providers and services
You can integrate with any RAG provider or tool of your choice to enhance your agent with additional context. Suggested providers and tools include:
LlamaIndex
- Framework for connecting custom data to LLMs.
Mem0
- Memory layer for AI assistants.
TurboPuffer
- Fast serverless vector search built on object storage.
Pinecone
- Managed vector database for AI applications.
Annoy
- Open source Python library from Spotify for nearest neighbor search.
Additional examples
The following examples show how to implement RAG and other techniques:
LlamaIndex RAG
A voice AI agent that uses LlamaIndex for RAG to answer questions from a knowledge base.
LiveKit Docs RAG
An agent that can answer questions about LiveKit with lookups against the docs website.
On this page
Overview
Initial context
Tool calls
Add context during conversation
User feedback
Verbal status updates
"Thinking" sounds
Frontend UI
Fine-tuned models
RAG providers and services
Additional examples


Content from https://docs.livekit.io/agents/build/metrics:

On this page
Overview
Logging events
Aggregating metrics
Metrics reference
Speech-to-text (STT)
LLM
Text-to-speech (TTS)
End-of-utterance (EOU)
Measuring conversation latency
Telemetry
Collected data
Enabling telemetry
Trace example
Example
Copy page
See more page options
Overview
LiveKit Agents provides built-in support for logging, collecting, and analyzing metrics to help you monitor and optimize your agent's performance. Agent sessions emit structured metrics events that can be logged in real time or aggregated to analyze latency and usage patterns.
In addition to per-event metrics, LiveKit’s OpenTelemetry integration provides trace-based observability. This enables you to capture the execution flow of an agents's lifecycle—from session start to individual
node
operations. You can use any OpenTelemetry-compatible provider to collect and analyze telemetry data, giving you  insight into conversation latency, tool usage, and performance bottlenecks.
Logging events
Agent metrics events are fired by the
AgentSession
whenever there is a new metrics object available during an active session.
A
log_metrics
helper function is also provided to format logging output for each metric type.
from
livekit
.
agents
import
metrics
,
MetricsCollectedEvent
.
.
.
@session
.
on
(
"metrics_collected"
)
def
_on_metrics_collected
(
ev
:
MetricsCollectedEvent
)
:
metrics
.
log_metrics
(
ev
.
metrics
)
Aggregating metrics
The
metrics
module also includes a
UsageCollector
helper class for aggregating usage metrics across a session. It tracks metrics such as LLM, TTS, and STT API usage, which can help estimate session cost.
from
livekit
.
agents
import
metrics
,
MetricsCollectedEvent
.
.
.
usage_collector
=
metrics
.
UsageCollector
(
)
@session
.
on
(
"metrics_collected"
)
def
_on_metrics_collected
(
ev
:
MetricsCollectedEvent
)
:
usage_collector
.
collect
(
ev
.
metrics
)
async
def
log_usage
(
)
:
summary
=
usage_collector
.
get_summary
(
)
logger
.
info
(
f"Usage:
{
summary
}
"
)
# At shutdown, generate and log the summary from the usage collector
ctx
.
add_shutdown_callback
(
log_usage
)
Metrics reference
Speech-to-text (STT)
STTMetrics
is emitted after the STT model has processed the audio input. This metrics is only available when an STT component is used, which does not apply to Realtime APIs.
Metric
Description
audio_duration
The duration (seconds) of the audio input received by the STT model.
duration
For non-streaming STT, the amount of time (seconds) it took to create the transcript. Always
0
for streaming STT.
streamed
True
if the STT is in streaming mode.
LLM
LLMMetrics
is emitted after each LLM inference completes. If the response includes tool calls, the event does not include the time taken to execute those calls. Each tool call response triggers a separate
LLMMetrics
event.
Metric
Description
duration
The amount of time (seconds) it took for the LLM to generate the entire completion.
completion_tokens
The number of tokens generated by the LLM in the completion.
prompt_tokens
The number of tokens provided in the prompt sent to the LLM.
prompt_cached_tokens
The number of cached tokens in the input prompt.
speech_id
An unique identifier representing a turn in the user input.
total_tokens
Total token usage for the completion.
tokens_per_second
The rate of token generation (tokens/second) by the LLM to generate the completion.
ttft
The amount of time (seconds) that it took for the LLM to generate the first token of the completion.
Text-to-speech (TTS)
TTSMetrics
is emitted after a TTS has generated speech from text input.
Metric
Description
audio_duration
The duration (seconds) of the audio output generated by the TTS model.
characters_count
The number of characters in the text input to the TTS model.
duration
The amount of time (seconds) it took for the TTS model to generate the entire audio output.
ttfb
The amount of time (seconds) that it took for the TTS model to generate the first byte of its audio output.
speech_id
An identifier linking to a user's turn.
streamed
True
if the TTS is in streaming mode.
End-of-utterance (EOU)
EOUMetrics
is emitted when the user is determined to have finished speaking. It includes metrics related to end-of-turn detection and transcription latency.
This event is only available in Realtime APIs when
turn_detection
is set to either VAD or LiveKit's turn detector plugin. When using server-side turn detection, EOUMetrics is not emitted, as this information is not available.
Metric
Description
end_of_utterance_delay
Time (in seconds) from the end of speech (as detected by VAD) to the point when the user's turn is considered complete. This includes any
transcription_delay
.
transcription_delay
Time (seconds) between the end of speech and when final transcript is available
on_user_turn_completed_delay
Time (in seconds) taken to execute the
on_user_turn_completed
callback.
speech_id
A unique identifier indicating the user's turn.
Measuring conversation latency
Total conversation latency is defined as the time it takes for the agent to respond to a user's utterance. Given the metrics above, it can be computed as follows:
total_latency
=
eou
.
end_of_utterance_delay
+
llm
.
ttft
+
tts
.
ttfb
Telemetry
LiveKit's
OpenTelemetry
integration automatically collects telemetry data from your agents and publishes it to any OpenTelemetry-compatible provider you choose. This enables monitoring and analysis of your agent's behavior and performance.
Collected data
A
trace
represents the execution flow of a single request within an LLM application. It captures all relevant steps, including duration and metadata.
Agent telemetry records traces for the following activities:
Session start
Agent turn
LLM node
Function tool
TTS node
End-of-turn detection
LLM and TTS metrics
Enabling telemetry
To enable telemetry, configure a tracer provider using
set_tracer_provider
in your entrypoint function. You can use any
OpenTelemetry-compatible provider
.
The following example uses
LangFuse
. Set the required public key, secret key, and host as environment variables:
import
base64
import
os
from
livekit
.
agents
.
telemetry
import
set_tracer_provider
def
setup_langfuse
(
host
:
str
|
None
=
None
,
public_key
:
str
|
None
=
None
,
secret_key
:
str
|
None
=
None
)
:
from
opentelemetry
.
exporter
.
otlp
.
proto
.
http
.
trace_exporter
import
OTLPSpanExporter
from
opentelemetry
.
sdk
.
trace
import
TracerProvider
from
opentelemetry
.
sdk
.
trace
.
export
import
BatchSpanProcessor
public_key
=
public_key
or
os
.
getenv
(
"LANGFUSE_PUBLIC_KEY"
)
secret_key
=
secret_key
or
os
.
getenv
(
"LANGFUSE_SECRET_KEY"
)
host
=
host
or
os
.
getenv
(
"LANGFUSE_HOST"
)
if
not
public_key
or
not
secret_key
or
not
host
:
raise
ValueError
(
"LANGFUSE_PUBLIC_KEY, LANGFUSE_SECRET_KEY, and LANGFUSE_HOST must be set"
)
langfuse_auth
=
base64
.
b64encode
(
f"
{
public_key
}
:
{
secret_key
}
"
.
encode
(
)
)
.
decode
(
)
os
.
environ
[
"OTEL_EXPORTER_OTLP_ENDPOINT"
]
=
f"
{
host
.
rstrip
(
'/'
)
}
/api/public/otel"
os
.
environ
[
"OTEL_EXPORTER_OTLP_HEADERS"
]
=
f"Authorization=Basic
{
langfuse_auth
}
"
trace_provider
=
TracerProvider
(
)
trace_provider
.
add_span_processor
(
BatchSpanProcessor
(
OTLPSpanExporter
(
)
)
)
set_tracer_provider
(
trace_provider
)
async
def
entrypoint
(
ctx
:
JobContext
)
:
setup_langfuse
(
)
# set up the langfuse tracer provider
# ...
Trace example
The following diagram shows a trace of an agent session with user turns.
Example
For a full example, see the following in the LiveKit Agents GitHub repository.
LangFuse trace example
An example of an agent using LangFuse as the tracer provider.
On this page
Overview
Logging events
Aggregating metrics
Metrics reference
Speech-to-text (STT)
LLM
Text-to-speech (TTS)
End-of-utterance (EOU)
Measuring conversation latency
Telemetry
Collected data
Enabling telemetry
Trace example
Example


Content from https://docs.livekit.io/agents/build/events:

On this page
Events
user_input_transcribed
conversation_item_added
function_tools_executed
metrics_collected
speech_created
agent_state_changed
user_state_changed
close
Handling errors
FallbackAdapter
Error event
Example
Copy page
See more page options
Events
AgentSession
emits events to notify you of state changes. Each event is emitted with an event object as its sole argument.
user_input_transcribed
A
UserInputTranscribedEvent
is emitted when user transcription is available.
Properties
transcript
: str
is_final
: bool
speaker_id
: str | None - Only available if speaker diarization is supported in your STT plugin.
Example
from
livekit
.
agents
import
UserInputTranscribedEvent
@session
.
on
(
"user_input_transcribed"
)
def
on_user_input_transcribed
(
event
:
UserInputTranscribedEvent
)
:
print
(
f"User input transcribed:
{
event
.
transcript
}
, final:
{
event
.
is_final
}
, speaker id:
{
event
.
speaker_id
}
"
)
conversation_item_added
A
ConversationItemAddedEvent
is emitted when a item is committed to the chat history. This event is emitted for both user and agent items.
Properties
item
:
ChatMessage
Example
from
livekit
.
agents
import
ConversationItemAddedEvent
from
livekit
.
agents
.
llm
import
ImageContent
,
AudioContent
.
.
.
@session
.
on
(
"conversation_item_added"
)
def
on_conversation_item_added
(
event
:
ConversationItemAddedEvent
)
:
print
(
f"Conversation item added from
{
event
.
item
.
role
}
:
{
event
.
item
.
text_content
}
. interrupted:
{
event
.
item
.
interrupted
}
"
)
# to iterate over all types of content:
for
content
in
event
.
item
.
content
:
if
isinstance
(
content
,
str
)
:
print
(
f" - text:
{
content
}
"
)
elif
isinstance
(
content
,
ImageContent
)
:
# image is either a rtc.VideoFrame or URL to the image
print
(
f" - image:
{
content
.
image
}
"
)
elif
isinstance
(
content
,
AudioContent
)
:
# frame is a list[rtc.AudioFrame]
print
(
f" - audio:
{
content
.
frame
}
, transcript:
{
content
.
transcript
}
"
)
function_tools_executed
FunctionToolsExecutedEvent
is emitted after all function tools have been executed for a given user input.
Methods
zipped()
returns a list of tuples of function calls and their outputs.
Properties
function_calls
: list[
FunctionCall
]
function_call_outputs
: list[
FunctionCallOutput
]
metrics_collected
MetricsCollectedEvent
is emitted when new metrics are available to be reported. For more information on metrics, see
Capturing metrics
.
Properties
metrics
: Union[STTMetrics, LLMMetrics, TTSMetrics, VADMetrics, EOUMetrics]
speech_created
SpeechCreatedEvent
is emitted when new agent speech is created. Speech could be created for any of the following reasons:
the user has provided input
session.say
is used to create agent speech
session.generate_reply
is called to create a reply
Properties
user_initiated
: str - True if speech was created using public methods like
say
or
generate_reply
source
: str - "say", "generate_reply", or "tool_response"
speech_handle
:
SpeechHandle
- handle to track speech playout.
agent_state_changed
AgentStateChangedEvent
is emitted when the agent's state changes. The
lk.agent.state
attribute on the agent participant is updated to reflect the new state, allowing frontend code to easily respond to changes.
Properties
old_state
: AgentState
new_state
: AgentState
AgentState
The agent could be in one of the following states:
initializing
- agent is starting up. this should be brief.
listening
- agent is waiting for user input
thinking
- agent is processing user input
speaking
- agent is speaking
user_state_changed
UserStateChangedEvent
is emitted when the user's state changes. This change is driven by the VAD module running on the user's audio input.
Properties
old_state
: UserState
new_state
: UserState
UserState
The user's state can be one of the following:
speaking
- VAD detected user has started speaking
listening
- VAD detected the user has stopped speaking
away
- The user hasn't responded for a while (default: 15s).
Specify a custom timeout with
AgentSession(user_away_timeout=...)
.
Example
Handling idle user
Check in with the user after they go idle.
close
The
CloseEvent
is emitted when the AgentSession has closed and the agent is no longer running. This can occur for several reasons:
The user ended the conversation
session.aclose()
was called
The room was deleted, disconnecting the agent
An unrecoverable error occurred during the session
Properties
error
: LLMError | STTError | TTSError | RealtimeModelError | None - The error that caused the session to close, if applicable
Handling errors
In addition to state changes, it's important to handle errors that may occur during a session. In real-time conversations, inference API failures can disrupt the flow, potentially leaving the agent unable to continue.
FallbackAdapter
For STT, LLM, and TTS, the Agents framework includes a
FallbackAdapter
that can fall back to secondary providers if the primary one fails.
When in use,
FallbackAdapter
would:
automatically resubmit the failed request to backup providers when the primary provider fails
mark the failed provider as unhealthy and stop sending requests to it
continue to use the backup providers until the primary provider recovers
periodically checks the primary provider’s status in the background
from
livekit
.
agents
import
llm
,
stt
,
tts
from
livekit
.
plugins
import
assemblyai
,
deepgram
,
elevenlabs
,
openai
,
groq
session
=
AgentSession
(
stt
=
stt
.
FallbackAdapter
(
[
assemblyai
.
STT
(
)
,
deepgram
.
STT
(
)
,
]
)
,
llm
=
llm
.
FallbackAdapter
(
[
openai
.
LLM
(
model
=
"gpt-4o"
)
,
openai
.
LLM
.
with_azure
(
model
=
"gpt-4o"
,
.
.
.
)
,
]
)
,
tts
=
tts
.
FallbackAdapter
(
[
elevenlabs
.
TTS
(
.
.
.
)
,
groq
.
TTS
(
.
.
.
)
,
]
)
,
)
Error event
AgentSession
emits
ErrorEvent
when errors occur during the session. It includes an
error
object with a
recoverable
field indicating whether the session will retry the failed operation.
If
recoverable
is
True
, the event is informational, and the session will continue as expected.
If
recoverable
is
False
(e.g., after exhausting retries), the session requires intervention. You can handle the error—for instance, by using
.say()
to inform the user of an issue.
Properties
model_config
: dict - a dictionary representing the current model's configuration
error
:
LLMError | STTError | TTSError | RealtimeModelError
- the error that occurred.
recoverable
is a field within
error
.
source
: LLM | STT | TTS | RealtimeModel - the source object responsible for the error
Example
Error handling
Handling unrecoverable errors with a presynthesized message.
On this page
Events
user_input_transcribed
conversation_item_added
function_tools_executed
metrics_collected
speech_created
agent_state_changed
user_state_changed
close
Handling errors
FallbackAdapter
Error event
Example


Content from https://docs.livekit.io/agents/build/latency:

Copy page
See more page options
Article coming soon
While docs are still under development, the best place to get the most current reference is on
GitHub
.


Content from https://docs.livekit.io/agents/build/testing:

On this page
Overview
What to test
Example test
Writing tests
Test setup
Result structure
Assertions
Mocking tools
Testing multiple turns
Loading conversation history
Verbose output
Integrating with CI
Third-party testing tools
Additional resources
Copy page
See more page options
Overview
Writing effective tests and evaluations are a key part of developing a reliable and production-ready AI agent. LiveKit Agents includes helpers that work with any Python testing framework, such as
pytest
, to write behavioral tests and evaluations alongside your existing unit and integration tests.
Use these tools to fine-tune your agent's behavior, work around tricky edge cases, and iterate on your agent's capabilities without breaking previously existing functionality.
What to test
You should plan to test your agent's behavior in the following areas:
Expected behavior
: Does your agent respond with the right intent and tone for typical use cases?
Tool usage
: Are functions called with correct arguments and proper context?
Error handling
: How does your agent respond to invalid inputs or tool failures?
Grounding
: Does your agent stay factual and avoid hallucinating information?
Misuse resistance
: How does your agent handle intentional attempts to misuse or manipulate it?
Text-only testing
The built-in testing helpers are designed to work with text input and output, using an LLM plugin or realtime model in text-only mode. This is the most cost-effective and intuitive way to write comprehensive tests of your agent's behavior.
For testing options that exercise the entire audio pipeline, see the
external tools
section at the end of this guide.
Example test
Here is a simple behavioral test for the agent created in the
voice AI quickstart
, using
pytest
. It ensures that the agent responds with a friendly greeting and offers assistance.
from
livekit
.
agents
import
AgentSession
from
livekit
.
plugins
import
openai
from
my_agent
import
Assistant
@pytest
.
mark
.
asyncio
async
def
test_assistant_greeting
(
)
-
>
None
:
async
with
(
openai
.
LLM
(
model
=
"gpt-4o-mini"
)
as
llm
,
AgentSession
(
llm
=
llm
)
as
session
,
)
:
await
session
.
start
(
Assistant
(
)
)
result
=
await
session
.
run
(
user_input
=
"Hello"
)
await
result
.
expect
.
next_event
(
)
.
is_message
(
role
=
"assistant"
)
.
judge
(
llm
,
intent
=
"Makes a friendly introduction and offers assistance."
)
result
.
expect
.
no_more_events
(
)
Writing tests
Testing frameworks
This guide assumes the use of
pytest
, but is adaptable to other testing frameworks.
You must install both the
pytest
and
pytest-asyncio
packages to write tests for your agent.
pip
install
pytest pytest-asyncio
Test setup
Each test typically follows the same pattern:
@pytest
.
mark
.
asyncio
# Or your async testing framework of choice
async
def
test_your_agent
(
)
-
>
None
:
async
with
(
# You must create an LLM instance for the `judge` method
openai
.
LLM
(
model
=
"gpt-4o-mini"
)
as
llm
,
# Create a session for the life of this test.
# LLM is not required - it will use the agent's LLM if you don't provide one here
AgentSession
(
llm
=
llm
)
as
session
,
)
:
# Start the agent in the session
await
session
.
start
(
Assistant
(
)
)
# Run a single conversation turn based on the given user input
result
=
await
session
.
run
(
user_input
=
"Hello"
)
# ...your assertions go here...
Result structure
The
run
method executes a single conversation turn and returns a
RunResult
, which contains each of the events that occurred during the turn, in order, and offers a fluent assertion API.
Simple turns where the agent responds with a single message and no tool calls can be straightforward, with only a single entry:
Loading diagram…
However, a more complex turn may contain tool calls, tool outputs, handoffs, and one or more messages.
Loading diagram…
To validate these multi-part turns, you can use either of the following approaches.
Sequential navigation
Cursor through the events with
next_event()
.
Validate individual events with
is_*
assertions such as
is_message()
.
Use
no_more_events()
to assert that you have reached the end of the list and no more events remain.
For example, to validate that the agent responds with a friendly greeting, you can use the following code:
result
.
expect
.
next_event
(
)
.
is_message
(
role
=
"assistant"
)
Skipping events
You can also skip events without validation:
Use
skip_next()
to skip one event, or pass a number to skip multiple events.
Use
skip_next_event_if()
to skip events conditionally if it matches the given type (
"message"
,
"function_call"
,
"function_call_output"
, or
"agent_handoff"
), plus optional other arguments of the same format as the
is_*
assertions.
Use
next_event()
with a type and other arguments in the same format as the
is_*
assertions to skip non-matching events implicitly.
Example:
result
.
expect
.
skip_next
(
)
# skips one event
result
.
expect
.
skip_next
(
2
)
# skips two events
result
.
expect
.
skip_next_event_if
(
type
=
"message"
,
role
=
"assistant"
)
# Skips the next assistant message
result
.
expect
.
next_event
(
type
=
"message"
,
role
=
"assistant"
)
# Advances to the next assistant message, skipping anything else. If no matching event is found, an assertion error is raised.
Indexed access
Access single events by index, without advancing the cursor, using the
[]
operator.
result
.
expect
[
0
]
.
is_message
(
role
=
"assistant"
)
Search
Look for the presence of individual events in an order-agnostic way with the
contains_*
methods such as
contains_message()
. This can be combined with slices using the
[:]
operator to search within a range.
result
.
expect
.
contains_message
(
role
=
"assistant"
)
result
.
expect
[
0
:
2
]
.
contains_message
(
role
=
"assistant"
)
Assertions
The framework includes a number of assertion helpers to validate the content and types of events within each result.
Message assertions
Use
is_message()
and
contains_message()
to test individual messages. These methods accept an optional
role
argument to match the message role.
result
.
expect
.
next_event
(
)
.
is_message
(
role
=
"assistant"
)
result
.
expect
[
0
:
2
]
.
contains_message
(
role
=
"assistant"
)
Access additional properties with the
event()
method:
event().item.content
- Message content
event().item.role
- Message role
LLM-based judgment
Use
judge()
to perform a qualitative evaluation of the message content using your LLM of choice. Specify the intended content, structure, or style of the message as a string, and include an
LLM
instance to evaluate it. The LLM receives the message string and the intent string, without surrounding context.
Here's an example:
result
=
await
session
.
run
(
user_input
=
"Hello"
)
await
(
result
.
expect
.
next_event
(
)
.
is_message
(
role
=
"assistant"
)
.
judge
(
llm
,
intent
=
"Offers a friendly introduction and offer of assistance."
)
)
The
llm
argument can be any LLM instance and does not need to be the same one used in the agent itself. Ensure you have setup the plugin correctly with the appropriate API keys and any other needed setup.
Tool call assertions
You can test three aspects of your agent's use of tools in these ways:
Function calls
: Verify that the agent calls the correct tool with the correct arguments.
Function call outputs
: Verify that the tool returns the expected output.
Agent response
: Verify that the agent performs the appropriate next step based on the tool output.
This example tests all three aspects in order:
result
=
await
session
.
run
(
user_input
=
"What's the weather in Tokyo?"
)
# Test that the agent's first conversation item is a function call
fnc_call
=
result
.
expect
.
next_event
(
)
.
is_function_call
(
name
=
"lookup_weather"
,
arguments
=
{
"location"
:
"Tokyo"
}
)
# Test that the tool returned the expected output to the agent
result
.
expect
.
next_event
(
)
.
is_function_call_output
(
output
=
"sunny with a temperature of 70 degrees."
)
# Test that the agent's response is appropriate based on the tool output
await
(
result
.
expect
.
next_event
(
)
.
is_message
(
role
=
"assistant"
)
.
judge
(
llm
,
intent
=
"Informs the user that the weather in Tokyo is sunny with a temperature of 70 degrees."
,
)
)
# Verify the agent's turn is complete, with no additional messages or function calls
result
.
expect
.
no_more_events
(
)
Access individual properties with the
event()
method:
is_function_call().event().item.name
- Function name
is_function_call().event().item.arguments
- Function arguments
is_function_call_output().event().item.output
- Raw function output
is_function_call_output().event().item.is_error
- Whether the output is an error
is_function_call_output().event().item.call_id
- The function call ID
Agent handoff assertions
Use
is_agent_handoff()
and
contains_agent_handoff()
to test that the agent performs a
handoff
to a new agent.
# The next event must be an agent handoff to the specified agent
result
.
expect
.
next_event
(
)
.
is_agent_handoff
(
new_agent_type
=
MyAgent
)
# A handoff must occur somewhere in the turn
result
.
expect
.
contains_agent_handoff
(
new_agent_type
=
MyAgent
)
Mocking tools
In many cases, you should mock your tools for testing. This is useful to easily test edge cases, such as errors or other unexpected behavior, or when the tool has a dependency on an external service that you don't need to test against.
Use the
mock_tools
helper in a
with
block to mock one or more tools for a specific Agent. For instance, to mock a tool to raise an error, use the following code:
from
livekit
.
agents
.
testing
import
mock_tools
# Mock a tool error
with
mock_tools
(
Assistant
,
{
"lookup_weather"
:
lambda
:
RuntimeError
(
"Weather service is unavailable"
)
}
,
)
:
result
=
await
session
.
run
(
user_input
=
"What's the weather in Tokyo?"
)
await
result
.
expect
.
next_event
(
type
=
"message"
)
.
judge
(
llm
,
intent
=
"Should inform the user that an error occurred while looking up the weather."
)
If you need a more complex mock, pass a function instead of a lambda:
def
_mock_weather_tool
(
location
:
str
)
-
>
str
:
if
location
==
"Tokyo"
:
return
"sunny with a temperature of 70 degrees."
else
:
return
"UNSUPPORTED_LOCATION"
# Mock a specific tool response
with
mock_tools
(
Assistant
,
{
"lookup_weather"
:
_mock_weather_tool
}
)
:
result
=
await
session
.
run
(
user_input
=
"What's the weather in Tokyo?"
)
await
result
.
expect
.
next_event
(
type
=
"message"
)
.
judge
(
llm
,
intent
=
"Should indicate the weather in Tokyo is sunny with a temperature of 70 degrees."
,
)
result
=
await
session
.
run
(
user_input
=
"What's the weather in Paris?"
)
await
result
.
expect
.
next_event
(
type
=
"message"
)
.
judge
(
llm
,
intent
=
"Should indicate that weather lookups in Paris are not supported."
,
)
Testing multiple turns
You can test multiple turns of a conversation by executing the
run
method multiple times. The conversation history builds automatically across turns.
# First turn
result1
=
await
session
.
run
(
user_input
=
"Hello"
)
await
result1
.
expect
.
next_event
(
)
.
is_message
(
role
=
"assistant"
)
.
judge
(
llm
,
intent
=
"Friendly greeting"
)
# Second turn builds on conversation history
result2
=
await
session
.
run
(
user_input
=
"What's the weather like?"
)
result2
.
expect
.
next_event
(
)
.
is_function_call
(
name
=
"lookup_weather"
)
result2
.
expect
.
next_event
(
)
.
is_function_call_output
(
)
await
result2
.
expect
.
next_event
(
)
.
is_message
(
role
=
"assistant"
)
.
judge
(
llm
,
intent
=
"Provides weather information"
)
Loading conversation history
To load conversation history manually, use the
ChatContext
class just as in your agent code:
from
livekit
.
agents
import
ChatContext
agent
=
Assistant
(
)
await
session
.
start
(
agent
)
chat_ctx
=
ChatContext
(
)
chat_ctx
.
add_message
(
role
=
"user"
,
content
=
"My name is Alice"
)
chat_ctx
.
add_message
(
role
=
"assistant"
,
content
=
"Nice to meet you, Alice!"
)
await
agent
.
update_chat_ctx
(
chat_ctx
)
# Test that the agent remembers the context
result
=
await
session
.
run
(
user_input
=
"What's my name?"
)
await
result
.
expect
.
next_event
(
)
.
is_message
(
role
=
"assistant"
)
.
judge
(
llm
,
intent
=
"Should remember and mention the user's name is Alice"
)
Verbose output
The
LIVEKIT_EVALS_VERBOSE
environment variable turns on detailed output for each agent execution. To use it with pytest, you must also set the
-s
flag to disable pytest's automatic capture of stdout:
LIVEKIT_EVALS_VERBOSE
=
1
pytest
-s
-o
log_cli
=
true
<
your-test-file
>
Sample verbose output:
evals
/
test_agent
.
py
:
:
test_offers_assistance
+
RunResult
(
user_input
=
`
Hello
`
events
:
[
0
]
ChatMessageEvent
(
item
=
{
'role'
:
'assistant'
,
'content'
:
[
'Hi there! How can I assist you today?'
]
}
)
)
-
Judgment
succeeded
for
`
Hi there! How can I assist...
`
:
`
The message provides a friendly greeting and explicitly offers assistance, fulfilling the intent.
`
PASSED
Integrating with CI
As the testing helpers work live against your LLM provider to test real agent behavior, you need to set up your CI system to include any necessary LLM API keys in order to work. Testing does not require LiveKit API keys as it does not make a LiveKit connection.
For GitHub Actions, see the guide on
using secrets in GitHub Actions
.
Warning
Never commit API keys to your repository. Use environment variables and CI secrets instead.
Third-party testing tools
To perform end-to-end testing of deployed agents, including the audio pipeline, consider these third-party services:
Bluejay
End-to-end testing for voice agents powered by real-world simulations.
Cekura
Testing and monitoring for voice AI agents.
Coval
Manage your AI conversational agents. Simulation & evaluations for voice and chat agents.
Hamming
At-scale testing & production monitoring for AI voice agents.
Additional resources
These examples and resources provide more help with testing and evaluation.
Drive-thru agent evals
Complete evaluation suite for a complex food ordering agent.
Front-desk agent evals
Complete evaluation suite for a calendar booking agent.
Agent starter project
Starter project with a complete testing integration.
RunResult API reference
API reference for the
RunResult
class.
On this page
Overview
What to test
Example test
Writing tests
Test setup
Result structure
Assertions
Mocking tools
Testing multiple turns
Loading conversation history
Verbose output
Integrating with CI
Third-party testing tools
Additional resources


Content from https://docs.livekit.io/agents/worker/agent-dispatch/:

On this page
Dispatching agents
Automatic agent dispatch
Explicit agent dispatch
Dispatch via API
Dispatch from inbound SIP calls
Dispatch on participant connection
Copy page
See more page options
Dispatching agents
Dispatch is the process of assigning an agent to a room. LiveKit server manages this process as part of the
worker lifecycle
. LiveKit optimizes dispatch for high concurrency and low latency, typically supporting hundred of thousands of new connections per second with a max dispatch time under 150 ms.
Automatic agent dispatch
By default, an agent is automatically dispatched to each new room. Automatic dispatch is the best option if you want to assign the same agent to all new participants.
Explicit agent dispatch
Explicit dispatch is available for greater control over when and how agents join rooms. This approach leverages the same worker systems, allowing you to run agent workers in the same way.
To use explicit dispatch, set the
agent_name
field in the
WorkerOptions
:
Python
Node.js
opts
=
WorkerOptions
(
.
.
.
agent_name
=
"test-agent"
,
)
Important
Automatic dispatch is disabled if the
agent_name
property is set.
Dispatch via API
Agent workers with the
agent_name
set can be explicitly dispatched to a room via
AgentDispatchService
.
Python
Node.js
LiveKit CLI
Go
import
asyncio
from
livekit
import
api
room_name
=
"my-room"
agent_name
=
"test-agent"
async
def
create_explicit_dispatch
(
)
:
lkapi
=
api
.
LiveKitAPI
(
)
dispatch
=
await
lkapi
.
agent_dispatch
.
create_dispatch
(
api
.
CreateAgentDispatchRequest
(
agent_name
=
agent_name
,
room
=
room_name
,
metadata
=
'{"user_id": "12345"}'
)
)
print
(
"created dispatch"
,
dispatch
)
dispatches
=
await
lkapi
.
agent_dispatch
.
list_dispatch
(
room_name
=
room_name
)
print
(
f"there are
{
len
(
dispatches
)
}
dispatches in
{
room_name
}
"
)
await
lkapi
.
aclose
(
)
asyncio
.
run
(
create_explicit_dispatch
(
)
)
The room,
my-room
, is automatically created during dispatch if it doesn't already exist, and the worker assigns
test-agent
to it.
Job metadata
Explicit dispatch allows you to pass metadata to the agent, available in the
JobContext
. This is useful for including details the like the user's ID, name, or phone number.
The metadata field is a string. LiveKit recommends using JSON to pass structured data.
The
examples
in the previous section demonstrate how to pass job metadata during dispatch.
For information on consuming job metadata in an agent, see the following guide:
Job metadata
Learn how to consume job metadata in an agent.
Dispatch from inbound SIP calls
Agents can be explicitly dispatched for inbound SIP calls.
SIP dispatch rules
can define one or more agents using the
room_config.agents
field.
LiveKit recommends explicit agent dispatch for SIP inbound calls rather than automatic agent dispatch as it allows multiple agents within a single project.
Dispatch on participant connection
You can configure a participant's token to dispatch one or more agents immediately upon connection.
To dispatch multiple agents, include multiple
RoomAgentDispatch
entries in
RoomConfiguration
.
The following example creates a token that dispatches the
test-agent
agent to the
my-room
room when the participant connects:
Python
Node.js
Go
from
livekit
.
api
import
(
AccessToken
,
RoomAgentDispatch
,
RoomConfiguration
,
VideoGrants
,
)
room_name
=
"my-room"
agent_name
=
"test-agent"
def
create_token_with_agent_dispatch
(
)
-
>
str
:
token
=
(
AccessToken
(
)
.
with_identity
(
"my_participant"
)
.
with_grants
(
VideoGrants
(
room_join
=
True
,
room
=
room_name
)
)
.
with_room_config
(
RoomConfiguration
(
agents
=
[
RoomAgentDispatch
(
agent_name
=
"test-agent"
,
metadata
=
'{"user_id": "12345"}'
)
]
,
)
,
)
.
to_jwt
(
)
)
return
token
On this page
Dispatching agents
Automatic agent dispatch
Explicit agent dispatch
Dispatch via API
Dispatch from inbound SIP calls
Dispatch on participant connection


Content from https://docs.livekit.io/agents/worker/job/:

On this page
Lifecycle
Entrypoint
Adding custom fields to agent logs
Passing data to a job
Job metadata
Room metadata and participant attributes
Ending the session
Disconnecting the agent
Disconnecting everyone
Post-processing and cleanup
Copy page
See more page options
Lifecycle
When a
worker
accepts a job request from LiveKit server, it starts a new process and runs your agent code inside. Each job runs in a separate process to isolate agents from each other. If a session instance crashes, it doesn't affect other agents running on the same worker. The job runs until all standard and SIP participants leave the room, or you explicitly shut it down.
Entrypoint
The
entrypoint
is executed as the main function of the process for each new job run by the worker, effectively handing control over to your code. You should load any necessary app-specific data and then execute your agent's logic.
Note
If you use
AgentSession
, it connects to LiveKit automatically when started. If not using
AgentSession
, or If you need to control the precise timing or method of connection, for instance to enable
end-to-end encryption
, use the
JobContext
's
connect method
.
This example shows a simple entrypoint that processes incoming audio tracks and publishes a text message to the room.
Python
async
def
do_something
(
track
:
rtc
.
RemoteAudioTrack
)
:
audio_stream
=
rtc
.
AudioStream
(
track
)
async
for
event
in
audio_stream
:
# Do something here to process event.frame
pass
await
audio_stream
.
aclose
(
)
async
def
entrypoint
(
ctx
:
JobContext
)
:
# an rtc.Room instance from the LiveKit Python SDK
room
=
ctx
.
room
# set up listeners on the room before connecting
@room
.
on
(
"track_subscribed"
)
def
on_track_subscribed
(
track
:
rtc
.
Track
,
*
_
)
:
if
track
.
kind
==
rtc
.
TrackKind
.
KIND_AUDIO
:
asyncio
.
create_task
(
do_something
(
track
)
)
# connect to room
await
ctx
.
connect
(
auto_subscribe
=
AutoSubscribe
.
AUDIO_ONLY
)
# when connected, room.local_participant represents the agent
await
room
.
local_participant
.
send_text
(
'hello world'
,
topic
=
'hello-world'
)
)
# iterate through currently connected remote participants
for
rp
in
room
.
remote_participants
.
values
(
)
:
print
(
rp
.
identity
)
For more LiveKit Agents examples, see the
GitHub repository
. To learn more about publishing and receiving tracks, see the following topics:
Media tracks
Use the microphone, speaker, cameras, and screenshare with your agent.
Realtime text and data
Use text and data channels to communicate with your agent.
Adding custom fields to agent logs
Each job outputs JSON-formatted logs that include the user transcript, turn detection data, job ID, process ID, and more. You can include custom fields in the logs using
ctx.log_fields_context
for additional diagnostic context.
The following example adds worker ID and room name to the logs:
async
def
entrypoint
(
ctx
:
JobContext
)
:
ctx
.
log_context_fields
=
{
"worker_id"
:
ctx
.
worker_id
,
"room_name"
:
ctx
.
room
.
name
,
}
To learn more, see the reference documentation for
JobContext.log_context_fields
.
Passing data to a job
You can customize a job with user or job-specific data using either job metadata, room metadata, or participant attributes.
Job metadata
Job metadata is a freeform string field defined in the
dispatch request
and consumed in the
entrypoint
. Use JSON or similar structured data to pass complex information.
For instance, you can pass the user's ID, name, and phone number:
import
json
async
def
entrypoint
(
ctx
:
JobContext
)
:
metadata
=
json
.
loads
(
ctx
.
job
.
metadata
)
user_id
=
metadata
[
"user_id"
]
user_name
=
metadata
[
"user_name"
]
user_phone
=
metadata
[
"user_phone"
]
# ...
For more information on dispatch, see the following article:
Agent dispatch
Learn how to dispatch an agent with custom metadata.
Room metadata and participant attributes
You can also use properties such as the room's name, metadata, and participant attributes to customize agent behavior.
Here's an example showing how to access various properties:
async
def
entrypoint
(
ctx
:
JobContext
)
:
# connect to the room
await
ctx
.
connect
(
auto_subscribe
=
AutoSubscribe
.
AUDIO_ONLY
)
# wait for the first participant to arrive
participant
=
await
ctx
.
wait_for_participant
(
)
# customize behavior based on the participant
print
(
f"connected to room
{
ctx
.
room
.
name
}
with participant
{
participant
.
identity
}
"
)
# inspect the current value of the attribute
language
=
participant
.
attributes
.
get
(
"user.language"
)
# listen to when the attribute is changed
@ctx
.
room
.
on
(
"participant_attributes_changed"
)
def
on_participant_attributes_changed
(
changed_attrs
:
dict
[
str
,
str
]
,
p
:
rtc
.
Participant
)
:
if
p
==
participant
:
language
=
p
.
attributes
.
get
(
"user.language"
)
print
(
f"participant
{
p
.
identity
}
changed language to
{
language
}
"
)
For more information, see the following articles:
Room metadata
Learn how to set and use room metadata.
Participant attributes & metadata
Learn how to set and use participant attributes and metadata.
Ending the session
Disconnecting the agent
You can disconnect an agent after it completes its task and is no longer needed in the room. This allows the other participants in the LiveKit session to continue. Your
shutdown hooks
run after the
shutdown
function.
Python
async
def
entrypoint
(
ctx
:
JobContext
)
:
# do some work
.
.
.
# disconnect from the room
ctx
.
shutdown
(
reason
=
"Session ended"
)
Disconnecting everyone
If the session should end for everyone, use the server API
deleteRoom
to end the session.
The
Disconnected
room event
will be sent, and the room will be removed from the server.
Python
from
livekit
import
api
async
def
entrypoint
(
ctx
:
JobContext
)
:
# do some work
.
.
.
api_client
=
api
.
LiveKitAPI
(
os
.
getenv
(
"LIVEKIT_URL"
)
,
os
.
getenv
(
"LIVEKIT_API_KEY"
)
,
os
.
getenv
(
"LIVEKIT_API_SECRET"
)
,
)
await
api_client
.
room
.
delete_room
(
api
.
DeleteRoomRequest
(
room
=
ctx
.
job
.
room
.
name
,
)
)
Post-processing and cleanup
After a session ends, you can perform post-processing or cleanup tasks using shutdown hooks. For example, you might want to save user state in a database.
Python
async
def
entrypoint
(
ctx
:
JobContext
)
:
async
def
my_shutdown_hook
(
)
:
# save user state
.
.
.
ctx
.
add_shutdown_callback
(
my_shutdown_hook
)
Note
Shutdown hooks should complete within a short amount of time. By default, the framework waits 60 seconds before forcefully terminating the process. You can adjust this timeout using the
shutdown_process_timeout
parameter in
WorkerOptions
.
On this page
Lifecycle
Entrypoint
Adding custom fields to agent logs
Passing data to a job
Job metadata
Room metadata and participant attributes
Ending the session
Disconnecting the agent
Disconnecting everyone
Post-processing and cleanup


Content from https://docs.livekit.io/agents/worker/options/:

On this page
WorkerOptions parameters
Entrypoint
Request handler
Prewarm function
Load function
Drain timeout
Permissions
Worker type
Starting the worker
Copy page
See more page options
WorkerOptions parameters
The interface for creating a worker is through the
WorkerOptions
class. The following only
includes some of the available parameters. For the complete list, see the
WorkerOptions reference
.
Python
opts
=
WorkerOptions
(
# entrypoint function is called when a job is assigned to this worker
# this is the only required parameter to WorkerOptions
# https://docs.livekit.io/agents/worker/job/#entrypoint
entrypoint_fnc
,
# inspect the request and decide if the current worker should handle it.
request_fnc
,
# a function to perform any necessary initialization in a new process.
prewarm_fnc
,
# a function that reports the current system load, whether CPU or RAM, etc.
load_fnc
,
# the maximum value of load_fnc, above which new processes will not spawn
load_threshold
,
# whether the agent can subscribe to tracks, publish data, update metadata, etc.
permissions
,
# amount of time to wait for existing jobs to finish when SIGTERM or SIGINT is received
drain_timeout
,
# the type of worker to create, either JT_ROOM or JT_PUBLISHER
worker_type
=
WorkerType
.
ROOM
,
)
# start the worker
cli
.
run_app
(
opts
)
Caution
For security purposes, set the LiveKit API key and secret as environment variables rather than as
WorkerOptions
parameters.
Entrypoint
The
entrypoint_fnc
is the main function called for each new job, and is the heart of your agent app. To learn more, see the
entrypoint documentation
in the job lifecycle article.
Python
async
def
entrypoint
(
ctx
:
JobContext
)
:
# connect to the room
# handle the session
.
.
.
Request handler
The
request_fnc
function is executed each time that the server has a job for the agent. The framework expects workers to explicitly accept or reject each job request. If you accept the request, your entrypoint function is called. If the request is rejected, it's sent to the next available worker.
By default, if left blank, the behavior is to auto-accept all requests dispatched to the worker.
Python
async
def
request_fnc
(
req
:
JobRequest
)
:
# accept the job request
await
req
.
accept
(
# the agent's name (Participant.name), defaults to ""
name
=
"agent"
,
# the agent's identity (Participant.identity), defaults to "agent-<jobid>"
identity
=
"identity"
,
# attributes to set on the agent participant upon join
attributes
=
{
"myagent"
:
"rocks"
}
,
)
# or reject it
# await req.reject()
opts
=
WorkerOptions
(
entrypoint_fnc
=
entrypoint
,
request_fnc
=
request_fnc
)
Prewarm function
For isolation and performance reasons, the framework runs each agent job in its own process. Agents often need access to model files that take time to load. To address this, you can use a
prewarm
function to warm up the process before assigning any jobs to it. You can control the number of processes to keep warm using the
num_idle_processes
parameter.
Python
def
prewarm_fnc
(
proc
:
JobProcess
)
:
# load silero weights and store to process userdata
proc
.
userdata
[
"vad"
]
=
silero
.
VAD
.
load
(
)
async
def
entrypoint
(
ctx
:
JobContext
)
:
# access the loaded silero instance
vad
:
silero
.
VAD
=
ctx
.
proc
.
userdata
[
"vad"
]
opts
=
WorkerOptions
(
entrypoint_fnc
=
entrypoint
,
prewarm_fnc
=
prewarm_fnc
)
Load function
load_fnc
and
load_threshold
control when a worker should stop accepting new jobs.
load_fnc
: A function that returns the current load of the worker as a float between 0 and 1.0.
load_threshold
: The maximum load value at which the worker will still accept new jobs.
If load_fnc is not provided, it defaults to using the worker's average CPU utilization over a 5-second window.
The following example shows how to define a custom load function that limits the worker to 9 concurrent jobs, independent of CPU usage:
from
livekit
.
agents
import
Worker
,
WorkerOptions
def
compute_load
(
worker
:
Worker
)
-
>
float
:
return
min
(
len
(
worker
.
active_jobs
)
/
10
,
1.0
)
opts
=
WorkerOptions
(
load_fnc
=
compute_load
,
load_threshold
=
0.9
,
)
Drain timeout
Since agent sessions are stateful, they should not be terminated abruptly when the process is shutting down. The Agents framework supports graceful termination: when a
SIGTERM
or
SIGINT
is received, the worker enters a
draining
state. In this state, it stops accepting new jobs but allows existing ones to complete, up to a configured timeout.
The
drain_timeout
parameter sets the maximum time to wait for active jobs to finish. It defaults to 30 minutes.
Permissions
By default, agents can both publish to and subscribe from the other participants in the same room. However, you can customize these permissions by setting the
permissions
parameter in
WorkerOptions
. To see the full list of parameters, see the
WorkerPermissions reference
.
Python
opts
=
WorkerOptions
(
.
.
.
permissions
=
WorkerPermissions
(
can_publish
=
True
,
can_subscribe
=
True
,
can_publish_data
=
True
,
# when set to true, the agent won't be visible to others in the room.
# when hidden, it will also not be able to publish tracks to the room as it won't be visible.
hidden
=
False
,
)
,
)
Worker type
You can choose to start a new instance of the agent for each room or for each publisher in the room. This can be set when you register your worker:
Python
Node.js
opts
=
WorkerOptions
(
.
.
.
# when omitted, the default is WorkerType.ROOM
worker_type
=
WorkerType
.
ROOM
,
)
The
WorkerType
enum has two options:
ROOM
: Create a new instance of the agent for each room.
PUBLISHER
: Create a new instance of the agent for each publisher in the room.
If the agent is performing resource-intensive operations in a room that could potentially include multiple publishers (for example, processing incoming video from a set of security cameras), you can set
worker_type
to
JT_PUBLISHER
to ensure that each publisher has its own instance of the agent.
For
PUBLISHER
jobs, call the
entrypoint
function once for each publisher in the room. The
JobContext.publisher
object contains a
RemoteParticipant
representing that publisher.
Starting the worker
To spin up a worker with the configuration defined using
WorkerOptions
, call the CLI:
Python
Node.js
if
__name__
==
"__main__"
:
cli
.
run_app
(
opts
)
The Agents worker CLI provides two subcommands:
start
and
dev
. The former outputs raw JSON data to stdout, and is recommended for production.
dev
is recommended to use for development, as it outputs human-friendly colored logs, and supports hot reloading on Python.
On this page
WorkerOptions parameters
Entrypoint
Request handler
Prewarm function
Load function
Drain timeout
Permissions
Worker type
Starting the worker


Content from https://docs.livekit.io/agents/ops/deployment:

On this page
Overview
Where to deploy
Networking
Environment variables
Storage
Memory and CPU
Rollout
Load balancing
Worker availability
Autoscaling
Copy page
See more page options
Overview
LiveKit Agents use a worker pool model suited to a container orchestration system like Kubernetes. Each worker — an instance of
python agent.py start
— registers with LiveKit server. LiveKit server balances job dispatch across available workers. The workers themselves spawn a new sub-process for each job, and that job is where your code and agent participant run.
Deploying to production generally requires a simple
Dockerfile
that ends in
CMD ["python", "agent.py", "start"]
and a deployment platform that scales your worker pool based on load.
Python Voice Agent
A voice AI starter project which includes a working Dockerfile and CI configuration.
GitHub
livekit-examples/agent-starter-python
Where to deploy
LiveKit Agents can be deployed anywhere. The recommended approach is to use
Docker
and deploy to an orchestration service. The LiveKit team and community have found the following deployment platforms to be the easiest to deploy and autoscale workers.
New
LiveKit Cloud Agents Beta
Run your agent on the same network and infrastructure that serves LiveKit Cloud, with builds, deployment, and scaling handled for you. Sign up for the public beta to get started.
Kubernetes
Sample configuration for deploying and autoscaling LiveKit Agents on Kubernetes.
Render.com
Sample configuration for deploying and autoscaling LiveKit Agents on Render.com.
More deployment examples
Example
Dockerfile
and configuration files for a variety of deployment platforms.
Networking
Workers use a WebSocket connection to register with LiveKit server and accept incoming jobs. This means that workers do not need to expose any inbound hosts or ports to the public internet.
You may optionally expose a private health check endpoint for monitoring, but this is not required for normal operation.
The default health check server listens on
http://0.0.0.0:8081/
.
Environment variables
It is best to configure your worker with environment variables for secrets like API keys. In addition to the LiveKit variables, you are likely to need additional keys for external services your agent depends on.
For instance, an agent built with the
Voice AI quickstart
needs the following keys at a minimum:
.env
DEEPGRAM_API_KEY
=
<
Your Deepgram API Key
>
OPENAI_API_KEY
=
<
Your OpenAI API Key
>
CARTESIA_API_KEY
=
<
Your Cartesia API Key
>
LIVEKIT_API_KEY
=
<
your API Key
>
LIVEKIT_API_SECRET
=
<
your API Secret
>
LIVEKIT_URL
=
<
your LiveKit server URL
>
Reveal API Key and Secret
Project environments
It's recommended to use a separate LiveKit instance for staging, production, and development environments. This ensures you can continue working on your agent locally without accidentally processing real user traffic.
In LiveKit Cloud, make a separate project for each environment. Each has a unique URL, API key, and secret.
For self-hosted LiveKit server, use a separate deployment for staging and production and a local server for development.
Storage
Worker and job processes have no particular storage requirements beyond the size of the Docker image itself (typically <1GB). 10GB of ephemeral storage should be more than enough to account for this and any temporary storage needs your app has.
Memory and CPU
Memory and CPU requirements vary significantly based on the specific details of your app. For instance, agents that apply
enhanced noise cancellation
require more CPU and memory than those that don't.
LiveKit recommends 4 cores and 8GB of memory for every 25 concurrent sessions as a starting rule for most voice-to-voice apps.
Real world load test results
LiveKit ran a load test to evaluate the memory and CPU requirements of a typical voice-to-voice app.
30 agents each placed in their own LiveKit Cloud room.
30 simulated user participants, one in each room.
Each simulated participant published looping speech audio to the agents.
Each agent subscribed to the incoming audio of the user and ran the Silero VAD plugin.
Each agent published their own audio (simple looping sine wave).
One additional user participant with a corresponding voice AI agent to ensure subjective quality of service.
This test ran all agents on a single 4-Core, 8GB machine. This machine reached peak usage of:
CPU: ~3.8 cores utilized
Memory: ~2.8GB used
Rollout
Workers stop accepting jobs upon
SIGINT
or
SIGTERM
. Any job still running on the worker continues to run to completion. It's important that you configure a large enough grace period such that your jobs can finish without interrupting the user experience.
Voice AI apps might require a 10+ minute grace period to allow for conversations to finish.
Different deployment platforms have different ways of setting this grace period.
In Kubernetes, it's the
terminationGracePeriodSeconds
field in the pod spec.
Consult your deployment platform's documentation for more information.
Load balancing
LiveKit server includes a built-in balanced job distribution system. This system peforms round-robin distribution with a single-assignment principle that ensures each job is assigned to only one worker. If a worker fails to accept the job within a predetermined timeout period, the job is sent to another available worker instead.
LiveKit Cloud additionally exercises geographic affinity to prioritize matching users and workers that are geographically closest to each other. This ensures the lowest possible latency between users and agents.
Worker availability
Worker availability is defined by the
load_fnc
and
load_threshold
parameters in the
WorkerOptions
configuration.
The
load_fnc
must return a value between 0 and 1, indicating how busy the worker is.
load_threshold
is the load value above which the worker stops accepting new jobs.
The default
load_fnc
is overall CPU utilization, and the default
load_threshold
is
0.75
.
Autoscaling
To handle variable traffic patterns, add an autoscaling strategy to your deployment platform. Your autoscaler should use the same underlying metrics as your
load_fnc
(the default is CPU utilization) but should scale up at a
lower
threshold than your worker's
load_threshold
. This ensures continuity of service by adding new workers before existing ones go out of service. For example, if your
load_threshold
is
0.75
, you should scale up at
0.50
.
Since voice agents are typically long running tasks (relative to typical web requests), rapid increases in load are more likely to be sustained. In technical terms: spikes are less spikey. For your autoscaling configuration, you should consider
reducing
cooldown/stabilization periods when scaling up. When scaling down, consider
increasing
cooldown/stabilization periods because workers take time to drain.
For example, if deploying on Kubernetes using a Horizontal Pod Autoscaler,
see
stabilizationWindowSeconds
.
On this page
Overview
Where to deploy
Networking
Environment variables
Storage
Memory and CPU
Rollout
Load balancing
Worker availability
Autoscaling


Content from https://docs.livekit.io/agents/ops/recording:

On this page
Overview
Video or audio recording
Example
Text transcripts
Example
Copy page
See more page options
Overview
There are many reasons to record or persist the sessions that occur in your app, from quality monitoring to regulatory compliance. LiveKit allows you to record the video and audio from agent sessions or save the text transcripts.
Video or audio recording
Use the
Egress feature
to record audio and/or video. The simplest way to do this is to start a
room composite recorder
in your agent's entrypoint. This starts recording when the agent enters the room and automatically captures all audio and video shared in the room. Recording ends when all participants leave. Recordings are stored in the cloud storage provider of your choice.
Example
This example shows how to modify the
Voice AI quickstart
to record sessions. It uses Amazon S3, but you can also save files to any Amazon S3-compatible storage provider, Google Cloud Storage or Azure Blob Storage.
For additional egress examples using Google and Azure, see the
Egress examples
.
To modify the
Voice AI quickstart
to record sessions, add the following code:
agent.py
from
livekit
import
api
async
def
entrypoint
(
ctx
:
JobContext
)
:
# Add the following code to the top, before calling ctx.connect()
# Set up recording
req
=
api
.
RoomCompositeEgressRequest
(
room_name
=
ctx
.
room
.
name
,
audio_only
=
True
,
file_outputs
=
[
api
.
EncodedFileOutput
(
file_type
=
api
.
EncodedFileType
.
OGG
,
filepath
=
"livekit/my-room-test.ogg"
,
s3
=
api
.
S3Upload
(
bucket
=
"<S3 bucket name>"
,
region
=
"<S3 region>"
,
access_key
=
"<S3 access key>"
,
secret
=
"<S3 secret key>"
,
)
,
)
]
,
)
lkapi
=
api
.
LiveKitAPI
(
)
res
=
await
lkapi
.
egress
.
start_room_composite_egress
(
req
)
await
lkapi
.
aclose
(
)
# .. The rest of your entrypoint code follows ...
Text transcripts
Text transcripts are available in realtime via the
llm_node
or the
transcription_node
as detailed in the docs on
Pipeline nodes
. You can use this along with other events and callbacks to record your session and any other data you need.
Additionally, you can access the
session.history property
at any time to get the entire conversation history. Using the
add_shutdown_callback
method, you can save the conversation history to a file after the user leaves and the room closes.
For more immediate access to conversation as it happens, you can listen for related
events
. A
conversation_item_added
event is emitted whenever an item is added to the chat history. The
user_input_transcribed
event is emitted whenever user input is transcribed. These results might differ from the final transcription.
Example
This example shows how to modify the
Voice AI quickstart
to save the conversation history to a JSON file.
agent.py
from
datetime
import
datetime
import
json
def
entrypoint
(
ctx
:
JobContext
)
:
# Add the following code to the top, before calling ctx.connect()
async
def
write_transcript
(
)
:
current_date
=
datetime
.
now
(
)
.
strftime
(
"%Y%m%d_%H%M%S"
)
# This example writes to the temporary directory, but you can save to any location
filename
=
f"/tmp/transcript_
{
ctx
.
room
.
name
}
_
{
current_date
}
.json"
with
open
(
filename
,
'w'
)
as
f
:
json
.
dump
(
session
.
history
.
to_dict
(
)
,
f
,
indent
=
2
)
print
(
f"Transcript for
{
ctx
.
room
.
name
}
saved to
{
filename
}
"
)
ctx
.
add_shutdown_callback
(
write_transcript
)
# .. The rest of your entrypoint code follows ...
On this page
Overview
Video or audio recording
Example
Text transcripts
Example


Content from https://docs.livekit.io/agents/ops/cost:

Copy page
See more page options
Article coming soon
While docs are still under development, the best place to get the most current reference is on
GitHub
.


Content from https://docs.livekit.io/agents/integrations/openai:

On this page
OpenAI ecosystem support
Getting started
LiveKit Agents overview
Realtime API
OpenAI plugin documentation
Copy page
See more page options
Try LiveKit.fm
Chat with OpenAI's latest models in a LiveKit demo inspired by OpenAI.fm
OpenAI ecosystem support
OpenAI
provides some of the most powerful AI models and services today, which integrate into LiveKit Agents in the following ways:
Realtime API
: The original production-grade speech-to-speech model. Build lifelike voice assistants with just one model.
GPT 4o, o1-mini, and more
: Smart and creative models for voice AI.
STT models
: From industry-standard
whisper-1
to leading-edge
gpt-4o-transcribe
.
TTS models
: Use OpenAI's latest
gpt-4o-mini-tts
to generate lifelike speech in a voice pipeline.
LiveKit Agents supports OpenAI models through the
OpenAI developer platform
as well as
Azure OpenAI Service
. See the
Azure AI integration guide
for more information on Azure OpenAI.
Getting started
Use the following guide to speak to your own OpenAI-powered voice AI agent in less than 10 minutes.
Voice AI quickstart
Build your first voice AI app with the OpenAI Realtime API or GPT-4o.
Realtime playground
Experiment with the OpenAI Realtime API and personalities like
the
Snarky Teenager
or
Opera Singer
.
LiveKit Agents overview
LiveKit Agents is an open source framework for building realtime AI apps in Python and Node.js. It supports complex voice AI
workflows
with multiple agents and discrete processing steps, and includes built-in load balancing.
LiveKit provides SIP support for
telephony integration
and full-featured
frontend SDKs
in multiple languages. It uses
WebRTC
transport for end-user devices, enabling high-quality, low-latency realtime experiences. To learn more, see
LiveKit Agents
.
Realtime API
LiveKit Agents serves as a bridge between your frontend — connected over WebRTC — and the OpenAI Realtime API — connected over WebSockets. LiveKit automatically converts Realtime API audio response buffers to WebRTC audio streams synchronized with text, and handles business logic like interruption handling automatically. You can add your own logic within your agent, and use LiveKit features for realtime state and data to coordinate with your frontend.
Additional benefits of LiveKit Agents for the Realtime API include:
Noise cancellation
: One line of code to remove background noise and speakers from your input audio.
Telephony
: Inbound and outbound calling using SIP trunks.
Interruption handling
: Automatically handles context truncation on interruption.
Transcription sync
: Realtime API text output is synced to audio playback automatically.
Loading diagram…
Realtime API quickstart
Use the Voice AI quickstart with the Realtime API to get up and running in less than 10 minutes.
Web and mobile frontends
Put your agent in your pocket with a custom web or mobile app.
Telephony integration
Your agent can place and receive calls with LiveKit's SIP integration.
Building voice agents
Comprehensive documentation to build advanced voice AI apps with LiveKit.
Recipes
Get inspired by LiveKit's collection of recipes and example apps.
OpenAI plugin documentation
The following links provide more information on each available OpenAI component in LiveKit Agents.
Realtime API
LiveKit Agents docs for the OpenAI Realtime API.
OpenAI Models
LiveKit Agents docs for
gpt-4o
,
o1-mini
, and other OpenAI LLMs.
OpenAI STT
LiveKit Agents docs for
whisper-1
,
gpt-4o-transcribe
, and other OpenAI STT models.
OpenAI TTS
LiveKit Agents docs for
tts-1
,
gpt-4o-mini-tts
, and other OpenAI TTS models.
On this page
OpenAI ecosystem support
Getting started
LiveKit Agents overview
Realtime API
OpenAI plugin documentation


Content from https://docs.livekit.io/agents/integrations/google:

On this page
Google AI ecosystem support
Getting started
LiveKit Agents overview
Google plugin documentation
Copy page
See more page options
Gemini playground
Play with the Gemini Live API in this LiveKit-powered playground
Google AI ecosystem support
Google AI
provides some of the most powerful AI models and services today, which integrate into LiveKit Agents in the following ways:
Gemini
: A family of general purpose high-performance LLMs.
Google Cloud STT and TTS
: Affordable, production-grade models for transcription and speech synthesis.
Gemini Live API
: A speech-to-speech realtime model with live video input.
LiveKit Agents supports Google AI through the
Gemini API
and
Vertex AI
.
Getting started
Use the Voice AI quickstart to build a voice AI app with Gemini. Select an STT-LLM-TTS pipeline model type and add the following components to build on Gemini.
Voice AI quickstart
Build your first voice AI app with Google Gemini.
Install the Google plugin:
pip
install
"livekit-agents[google]~=1.0"
Add your Google API key to your
.env.
file:
.env
GOOGLE_API_KEY
=
<
your-google-api-key
>
Use the Google LLM component to initialize your
AgentSession
:
agent.py
from
livekit
.
plugins
import
google
# ...
# in your entrypoint function
session
=
AgentSession
(
llm
=
google
.
LLM
(
model
=
"gemini-2.0-flash"
,
)
,
# ... stt, tts,vad, turn_detection, etc.
)
LiveKit Agents overview
LiveKit Agents is an open source framework for building realtime AI apps in Python and Node.js. It supports complex voice AI
workflows
with multiple agents and discrete processing steps, and includes built-in load balancing.
LiveKit provides SIP support for
telephony integration
and full-featured
frontend SDKs
in multiple languages. It uses
WebRTC
transport for end-user devices, enabling high-quality, low-latency realtime experiences. To learn more, see
LiveKit Agents
.
Google plugin documentation
The following links provide more information on each available Google component in LiveKit Agents.
Gemini LLM
LiveKit Agents docs for Google Gemini models.
Gemini Live API
LiveKit Agents docs for the Gemini Live API.
Google Cloud STT
LiveKit Agents docs for Google Cloud STT.
Google Cloud TTS
LiveKit Agents docs for Google Cloud TTS.
On this page
Google AI ecosystem support
Getting started
LiveKit Agents overview
Google plugin documentation


Content from https://docs.livekit.io/agents/integrations/azure:

On this page
Azure AI ecosystem support
Getting started
LiveKit Agents overview
Azure plugin documentation
Copy page
See more page options
Azure AI ecosystem support
Microsoft's
Azure AI Services
is a large collection of cutting-edge production-ready AI services, which integrate with LiveKit in the following ways:
Azure OpenAI
: Run OpenAI models, including the Realtime API, with the security and reliability of Azure.
Azure Speech
: Speech-to-text and text-to-speech services.
The LiveKit Agents OpenAI plugin supports Azure OpenAI, and the Azure plugin supports Azure Speech.
Getting started
Use the voice AI quickstart to build a voice AI app with Azure OpenAI. Select a realtime model type and add the following components to use the Azure OpenAI Realtime API:
Voice AI quickstart
Build your first voice AI app with Azure OpenAI.
Install the OpenAI plugin:
pip
install
"livekit-agents[openai]~=1.0"
Add your Azure OpenAI endpoint and API key to your
.env.
file:
.env
AZURE_OPENAI_ENDPOINT
=
<
your-azure-openai-endpoint
>
AZURE_OPENAI_API_KEY
=
<
your-azure-openai-api-key
>
Use the
with_azure
method to connect to Azure OpenAI:
agent.py
from
livekit
.
plugins
import
openai
# ...
# in your entrypoint function
session
=
AgentSession
(
llm
=
openai
.
realtime
.
RealtimeModel
.
with_azure
(
azure_deployment
=
"<model-deployment>"
,
api_version
=
"2024-10-01-preview"
,
voice
=
"alloy"
,
)
,
# ... vad, turn_detection, etc.
)
LiveKit Agents overview
LiveKit Agents is an open source framework for building realtime AI apps in Python and Node.js. It supports complex voice AI
workflows
with multiple agents and discrete processing steps, and includes built-in load balancing.
LiveKit provides SIP support for
telephony integration
and full-featured
frontend SDKs
in multiple languages. It uses
WebRTC
transport for end-user devices, enabling high-quality, low-latency realtime experiences. To learn more, see
LiveKit Agents
.
Azure plugin documentation
Azure OpenAI Realtime API
LiveKit Agents docs for Azure OpenAI Realtime API.
Azure OpenAI LLM
LiveKit Agents docs for Azure OpenAI LLMs.
Azure OpenAI STT
LiveKit Agents docs for Azure OpenAI STT.
Azure OpenAI TTS
LiveKit Agents docs for Azure OpenAI TTS.
Azure Speech STT
LiveKit Agents docs for Azure Speech STT.
Azure Speech TTS
LiveKit Agents docs for Azure Speech TTS.
On this page
Azure AI ecosystem support
Getting started
LiveKit Agents overview
Azure plugin documentation


Content from https://docs.livekit.io/agents/integrations/aws:

On this page
AWS AI ecosystem support
Getting started
AWS plugin documentation
Copy page
See more page options
AWS AI ecosystem support
Amazon's
AWS AI
is a comprehensive collection of production-ready AI services, which integrate with LiveKit in the following ways:
Amazon Bedrock
: Access to foundation models from leading AI companies.
Amazon Polly
: Text-to-speech service with lifelike voices.
Amazon Transcribe
: Speech-to-text service with high accuracy.
Amazon Nova Sonic
: Realtime, speech-to-speech model that uses a bidirectional streaming API for streaming events.
The LiveKit Agents AWS plugin supports these services for building voice AI applications.
Getting started
Use the voice AI quickstart to build a voice AI app with AWS services. Select a pipeline model type and add the following components to use AWS AI services:
Voice AI quickstart
Build your first voice AI app with AWS AI services.
Install the AWS plugin:
pip
install
"livekit-agents[aws]~=1.0"
Add your AWS credentials to your
.env
file:
.env
AWS_ACCESS_KEY_ID
=
<
your-aws-access-key-id
>
AWS_SECRET_ACCESS_KEY
=
<
your-aws-secret-access-key
>
AWS_REGION
=
<
your-aws-region
>
Use the AWS services in your application:
agent.py
from
livekit
.
plugins
import
aws
# ...
# in your entrypoint function
session
=
AgentSession
(
llm
=
aws
.
LLM
(
model
=
"anthropic.claude-3-5-sonnet-20240620-v1:0"
,
)
,
tts
=
aws
.
TTS
(
voice
=
"Ruth"
,
speech_engine
=
"generative"
,
language
=
"en-US"
,
)
,
stt
=
aws
.
STT
(
session_id
=
"my-session-id"
,
language
=
"en-US"
,
)
,
# ... vad, turn_detection, etc.
)
Or use Amazon Nova Sonic, a state of the art speech-to-speech model:
pip
install
"livekit-agents-aws[realtime]~=1.0"
agent.py
from
livekit
.
plugins
import
aws
# ...
# in your entrypoint function
session
=
AgentSession
(
llm
=
aws
.
realtime
.
RealtimeModel
(
)
)
AWS plugin documentation
Amazon Bedrock LLM
LiveKit Agents docs for Amazon Bedrock LLM.
Amazon Polly TTS
LiveKit Agents docs for Amazon Polly TTS.
Amazon Transcribe STT
LiveKit Agents docs for Amazon Transcribe STT.
Amazon Nova Sonic
LiveKit Agents docs for the Amazon Nova Sonic speech-to-speech model.
On this page
AWS AI ecosystem support
Getting started
AWS plugin documentation


Content from https://docs.livekit.io/agents/integrations/groq:

On this page
Groq ecosystem support
Getting started
LiveKit Agents overview
Groq plugin documentation
Copy page
See more page options
Try out Groq Talk
See our voice assistant playground using Groq STT, LLM, and TTS
Groq ecosystem support
Groq
provides fast AI inference in the cloud and on-prem AI compute centers. LiveKit Agents can integrate with the following Groq services:
STT
: Fast and cost-effective English or multilingual transcription based on
whisper-large-v3
.
TTS
: Fast English and Arabic text-to-speech based on
playai-tts
.
LLM
: Fast inference for open models like
llama-3.1-8b-instant
and more.
Getting started
Use the Voice AI quickstart to build a voice AI app with Groq. Select an STT-LLM-TTS pipeline model type and add the following components to build on Groq.
Voice AI quickstart
Build your first voice AI app with Groq.
Install the Groq plugin:
pip
install
"livekit-agents[groq]~=1.0"
Add your Groq API key to your
.env.
file:
.env
GROQ_API_KEY
=
<
Your Groq API Key
>
Use Groq components to initialize your
AgentSession
:
agent.py
from
livekit
.
plugins
import
groq
# ...
# in your entrypoint function
session
=
AgentSession
(
stt
=
groq
.
STT
(
model
=
"whisper-large-v3-turbo"
,
language
=
"en"
,
)
,
llm
=
groq
.
LLM
(
model
=
"llama3-8b-8192"
)
,
tts
=
groq
.
TTS
(
model
=
"playai-tts"
,
voice
=
"Arista-PlayAI"
,
)
,
# ... vad, turn_detection, etc.
)
LiveKit Agents overview
LiveKit Agents is an open source framework for building realtime AI apps in Python and Node.js. It supports complex voice AI
workflows
with multiple agents and discrete processing steps, and includes built-in load balancing.
LiveKit provides SIP support for
telephony integration
and full-featured
frontend SDKs
in multiple languages. It uses
WebRTC
transport for end-user devices, enabling high-quality, low-latency realtime experiences. To learn more, see
LiveKit Agents
.
Groq plugin documentation
The following links provide more information on each available Groq component in LiveKit Agents.
Groq STT
LiveKit Agents docs for Groq transcription models.
Groq TTS
LiveKit Agents docs for Groq speech models.
Groq LLM
LiveKit Agents docs for Groq LLM models including Llama 3, DeepSeek, and more.
On this page
Groq ecosystem support
Getting started
LiveKit Agents overview
Groq plugin documentation


Content from https://docs.livekit.io/agents/integrations/cerebras:

On this page
Cerebras ecosystem support
Getting started
LiveKit Agents overview
Further reading
Copy page
See more page options
Try Cerebras AI
Experience Cerebras's fast inference in a LiveKit-powered voice AI playground
Cerebras ecosystem support
Cerebras
provides high-throughput, low-latency AI inference for open models like Llama and DeepSeek. Cerebras is an OpenAI-compatible LLM provider and LiveKit Agents provides full support for Cerebras inference via the OpenAI plugin.
Getting started
Use the Voice AI quickstart to build a voice AI app with Cerebras. Select an STT-LLM-TTS pipeline model type and add the following components to build on Cerebras.
Voice AI quickstart
Build your first voice AI app with Cerebras.
Install the OpenAI plugin:
pip
install
"livekit-agents[openai]~=1.0"
Add your Cerebras API key to your
.env
file:
.env
CEREBRAS_API_KEY
=
<
your-cerebras-api-key
>
Use the Cerebras LLM to initialize your
AgentSession
:
agent.py
from
livekit
.
plugins
import
openai
# ...
# in your entrypoint function
session
=
AgentSession
(
llm
=
openai
.
LLM
.
with_cerebras
(
model
=
"llama-3.3-70b"
,
)
,
)
For a full list of supported models, including DeepSeek, see the
Cerebras docs
.
LiveKit Agents overview
LiveKit Agents is an open source framework for building realtime AI apps in Python and Node.js. It supports complex voice AI
workflows
with multiple agents and discrete processing steps, and includes built-in load balancing.
LiveKit provides SIP support for
telephony integration
and full-featured
frontend SDKs
in multiple languages. It uses
WebRTC
transport for end-user devices, enabling high-quality, low-latency realtime experiences. To learn more, see
LiveKit Agents
.
Further reading
More information about integrating Llama is available in the following article:
Cerebras integration guide
LiveKit docs on Cerebras integration.
On this page
Cerebras ecosystem support
Getting started
LiveKit Agents overview
Further reading


Content from https://docs.livekit.io/agents/integrations/llama:

On this page
Overview
Providers
Copy page
See more page options
Overview
Llama
is a family of open source LLMs developed by
Meta AI
. These models are widely deployed by a large number of cloud inference providers, and also support your own local or private deployments.
Providers
LiveKit Agents has out-of-the-box support for all of these Llama inference providers:
Cerebras
Fast text-only inference for the latest Llama models.
Groq
Fast inference for the latest Llama models.
Fireworks
Inference, fine-tuning, and multimodal support for the latest Llama models.
Perplexity
The Sonar family of models are based on Llama 3.1, fine-tuned for search.
Telnyx
Hosted inference for the latest Llama models.
Together AI
Inference, fine-tuning, and multimodal support for the latest Llama models.
Ollama
Run Llama and other open source models locally.
On this page
Overview
Providers


Content from https://docs.livekit.io/agents/integrations/realtime/:

On this page
Overview
How to use
Usage with separate TTS
Available providers
Considerations and limitations
Turn detection and VAD
Delayed transcription
Scripted speech output
Loading conversation history
Copy page
See more page options
Overview
Realtime models are capable of consuming and producing speech directly, bypassing the need for a voice pipeline with speech-to-text and text-to-speech components. They can be better at understanding the emotional context of input speech, as well as other verbal cues that may not translate well to text transcription. Additionally, the generated speech can include similar emotional aspects and other improvements over what a text-to-speech model can produce.
You can also use supported realtime models in tandem with a
TTS
plugin of your choice, to gain the benefits of realtime speech comprehension while maintaining complete control over speech output
The agents framework includes plugins for popular realtime models out of the box. This is a new area in voice AI and LiveKit aims to support new providers as they emerge.
LiveKit is open source and welcomes
new plugin contributions
.
How to use
Realtime model plugins have a constructor method to create a
RealtimeModel
instance. This instance can be passed directly to an
AgentSession
or
Agent
in its constructor, in place of an
LLM plugin
.
from
livekit
.
agents
import
AgentSession
from
livekit
.
plugins
import
openai
session
=
AgentSession
(
llm
=
openai
.
realtime
.
RealtimeModel
(
)
)
For additional information about installing and using plugins, see the
plugins overview
.
Usage with separate TTS
To use a realtime model with a different
TTS provider
, configure the realtime model to use a text-only response modality and include a TTS plugin in your
AgentSession
configuration.
session
=
AgentSession
(
llm
=
openai
.
realtime
.
RealtimeModel
(
modalities
=
[
"text"
]
)
,
# Or other realtime model plugin
tts
=
cartesia
.
TTS
(
)
# Or other TTS plugin of your choice
)
This feature requires support for a text-only response modality. Consult the following table for information about which providers support this feature.
Available providers
The following table lists the available realtime model providers. All providers support fast and expressive full speech-to-speech generation, tool calling, image understanding, and simple VAD-based
turn detection
. Support for other features is noted in the following table.
Provider
Live Video
Semantic VAD
Text Only Output
Amazon Nova Sonic
—
—
—
Azure OpenAI Realtime API
—
✓
✓
Gemini Live API
✓
—
✓
OpenAI Realtime API
—
✓
✓
Considerations and limitations
Realtime models bring great benefits due to their wider range of audio understanding and expressive output. However, they also have some limitations and considerations to keep in mind.
Turn detection and VAD
In general, LiveKit recommends using the built-in turn detection capabilities of the realtime model whenever possible. Accurate turn detection relies on both VAD and context gained from realtime speech-to-text, which, as discussed in the following section, isn't available with realtime models. If you need to use the LiveKit
turn detector model
, you must also add a separate STT plugin to provide the necessary interim transcripts.
Delayed transcription
Realtime models don't provide interim transcription results, and in general the user input transcriptions can be considerably delayed and often arrive after the agent's response. If you need realtime transcriptions, you should consider an STT-LLM-TTS pipeline or add a separate STT plugin for realtime transcription.
Scripted speech output
Realtime models don't offer a method to directly generate speech from a text script, such as with the
say
method. You can produce a response with
generate_reply(instructions='...')
and include specific instructions but the output isn't guaranteed to precisely follow any provided script. If your application requires the use of specific scripts, consider using the model
with a separate TTS plugin
instead.
Loading conversation history
Current models only support loading call history in text format. This limits their ability to interpret emotional context and other verbal cues that may not translate well to text transcription. Additionally, the OpenAI Realtime API becomes more likely to respond in text only after loading extensive history, even if configured to use speech. For OpenAI, it's recommended that you use a
separate TTS plugin
if you need to load conversation history.


Content from https://docs.livekit.io/agents/integrations/llm/:

On this page
Overview
Available providers
How to use
Usage in  AgentSession
Standalone usage
Tool usage
Vision usage
Further reading
Copy page
See more page options
Overview
Large language models (LLMs) are a type of AI model that can generate text output from text input. In voice AI apps, they fit between speech-to-text (STT) and text-to-speech (TTS) and are responsible for tool calls and generating the agent's text response.
Available providers
The agents framework includes plugins for the following LLM providers out-of-the-box. Choose a provider from the list below for a step-by-step guide. You can also implement the
LLM node
to provide custom behavior or an alternative provider. All providers support high-performance, low-latency streaming and tool calls. Support for other features is noted in the following table.
Provider
Notes
Vision
Structured Output
Custom Models
Amazon Bedrock
Wide range of models from Llama, DeepSeek, Mistral, and more.
✓
—
✓
Anthropic
Claude family of models.
✓
—
—
Baseten
✓
—
—
Google Gemini
✓
✓
—
Groq
Models from Llama, DeepSeek, and more.
✓
—
—
LangChain
Use a LangGraph workflow for your agent LLM.
✓
✓
✓
OpenAI
✓
✓
—
Azure OpenAI
✓
✓
—
Cerebras
Models from Llama and DeepSeek.
✓
✓
—
DeepSeek
✓
—
—
Fireworks
Wide range of models from Llama, DeepSeek, Mistral, and more.
✓
—
✓
Letta
Stateful API with memory features.
✓
—
—
Ollama
Self-hosted models from Llama, DeepSeek, and more.
✓
—
✓
Perplexity
✓
✓
—
Telnyx
Models from Llama, DeepSeek, OpenAI, and Mistral, and more.
✓
✓
—
Together AI
Models from Llama, DeepSeek, Mistral, and more.
✓
✓
✓
xAI
Grok family of models.
✓
✓
—
Have another provider in mind? LiveKit is open source and welcomes
new plugin contributions
.
Realtime models
Realtime models like the OpenAI Realtime API, Gemini Live, and Amazon Nova Sonic are capable of consuming and producing speech directly. LiveKit Agents supports them as an alternative to using an LLM plugin, without the need for STT and TTS. To learn more, see
Realtime models
.
How to use
The following sections describe high-level usage only.
For more detailed information about installing and using plugins, see the
plugins overview
.
Usage in
AgentSession
Construct an
AgentSession
or
Agent
with an
LLM
instance created by your desired plugin:
from
livekit
.
agents
import
AgentSession
from
livekit
.
plugins
import
openai
session
=
AgentSession
(
llm
=
openai
.
LLM
(
model
=
"gpt-4o-mini"
)
)
Standalone usage
You can also use an
LLM
instance in a standalone fashion with its simple streaming interface. It expects a
ChatContext
object, which contains the conversation history. The return value is a stream of
ChatChunk
s. This interface is the same across all LLM providers, regardless of their underlying API design:
from
livekit
.
agents
import
ChatContext
from
livekit
.
plugins
import
openai
llm
=
openai
.
LLM
(
model
=
"gpt-4o-mini"
)
chat_ctx
=
ChatContext
(
)
chat_ctx
.
add_message
(
role
=
"user"
,
content
=
"Hello, this is a test message!"
)
async
with
llm
.
chat
(
chat_ctx
=
chat_ctx
)
as
stream
:
async
for
chunk
in
stream
:
print
(
"Received chunk:"
,
chunk
.
delta
)
Tool usage
All LLM providers support tools (sometimes called "functions"). LiveKit Agents has full support for them within an
AgentSession
. For more information, see
Tool definition and use
.
Vision usage
All LLM providers support vision within most of their models. LiveKit agents supports vision input from URL or from
realtime video frames
. Consult your model provider for details on compatible image types, external URL support, and other constraints. For more information, see
Vision
.
Further reading
Workflows
How to model repeatable, accurate tasks with multiple agents.
Tool definition and usage
Let your agents call external tools and more.


Content from https://docs.livekit.io/agents/integrations/stt/:

On this page
Overview
Available providers
How to use
Usage in  AgentSession
Standalone usage
Further reading
Copy page
See more page options
Overview
Speech-to-text (STT) models process incoming audio and convert it to text in realtime. In voice AI, this text is then processed by an
LLM
to generate a response which is turn turned backed to speech using a
TTS
model.
Available providers
The agents framework includes plugins for the following STT providers out-of-the-box. Choose a provider from the list for a step-by-step guide. You can also implement the
STT node
to provide custom behavior or an alternative provider.
All STT providers support low-latency multilingual transcription. Support for other features is noted in the following table.
Provider
Prompt
Keywords
Diarization
Translation
Amazon Transcribe
—
✓
✓
—
AssemblyAI
—
—
—
—
Azure AI Speech
—
—
—
—
Azure OpenAI
✓
—
—
—
Baseten
—
—
—
—
Cartesia
—
—
—
—
Clova
—
✓
—
—
Deepgram
—
✓
—
—
fal
—
—
—
—
Gladia
—
—
—
✓
Google Cloud
—
✓
—
—
Groq
✓
—
—
—
OpenAI
✓
—
—
—
Sarvam
—
—
—
—
Speechmatics
—
—
✓
—
Spitch
—
—
—
—
Have another provider in mind? LiveKit is open source and welcomes
new plugin contributions
.
How to use
The following sections describe high-level usage only.
For more detailed information about installing and using plugins, see the
plugins overview
.
Usage in
AgentSession
Construct an
AgentSession
or
Agent
with an
STT
instance created by your desired plugin:
from
livekit
.
agents
import
AgentSession
from
livekit
.
plugins
import
deepgram
session
=
AgentSession
(
stt
=
deepgram
.
STT
(
model
=
"nova-2"
)
)
AgentSession
automatically integrates with VAD to detect user turns and know when to start and stop STT.
Standalone usage
You can also use an
STT
instance in a standalone fashion by creating a stream. You can use
push_frame
to add
realtime audio frames
to the stream, and then consume a stream of
SpeechEvent
as output.
Here is an example of a standalone STT app:
agent.py
import
asyncio
from
dotenv
import
load_dotenv
from
livekit
import
agents
,
rtc
from
livekit
.
agents
.
stt
import
SpeechEventType
,
SpeechEvent
from
typing
import
AsyncIterable
from
livekit
.
plugins
import
(
deepgram
,
)
load_dotenv
(
)
async
def
entrypoint
(
ctx
:
agents
.
JobContext
)
:
@ctx
.
room
.
on
(
"track_subscribed"
)
def
on_track_subscribed
(
track
:
rtc
.
RemoteTrack
)
:
print
(
f"Subscribed to track:
{
track
.
name
}
"
)
asyncio
.
create_task
(
process_track
(
track
)
)
async
def
process_track
(
track
:
rtc
.
RemoteTrack
)
:
stt
=
deepgram
.
STT
(
model
=
"nova-2"
)
stt_stream
=
stt
.
stream
(
)
audio_stream
=
rtc
.
AudioStream
(
track
)
async
with
asyncio
.
TaskGroup
(
)
as
tg
:
# Create task for processing STT stream
stt_task
=
tg
.
create_task
(
process_stt_stream
(
stt_stream
)
)
# Process audio stream
async
for
audio_event
in
audio_stream
:
stt_stream
.
push_frame
(
audio_event
.
frame
)
# Indicates the end of the audio stream
stt_stream
.
end_input
(
)
# Wait for STT processing to complete
await
stt_task
async
def
process_stt_stream
(
stream
:
AsyncIterable
[
SpeechEvent
]
)
:
try
:
async
for
event
in
stream
:
if
event
.
type
==
SpeechEventType
.
FINAL_TRANSCRIPT
:
print
(
f"Final transcript:
{
event
.
alternatives
[
0
]
.
text
}
"
)
elif
event
.
type
==
SpeechEventType
.
INTERIM_TRANSCRIPT
:
print
(
f"Interim transcript:
{
event
.
alternatives
[
0
]
.
text
}
"
)
elif
event
.
type
==
SpeechEventType
.
START_OF_SPEECH
:
print
(
"Start of speech"
)
elif
event
.
type
==
SpeechEventType
.
END_OF_SPEECH
:
print
(
"End of speech"
)
finally
:
await
stream
.
aclose
(
)
if
__name__
==
"__main__"
:
agents
.
cli
.
run_app
(
agents
.
WorkerOptions
(
entrypoint_fnc
=
entrypoint
)
)
VAD and StreamAdapter
Some STT providers or models, such as
Whisper
don't support streaming input.
In these cases, your app must determine when a chunk of audio represents a
complete segment of speech. You can do this using VAD together with the
StreamAdapter
class.
The following example modifies the previous example to use VAD and
StreamAdapter
to buffer user speech until
VAD detects the end of speech:
from
livekit
import
agents
,
rtc
from
livekit
.
plugins
import
openai
,
silero
async
def
process_track
(
ctx
:
agents
.
JobContext
,
track
:
rtc
.
Track
)
:
whisper_stt
=
openai
.
STT
(
)
vad
=
silero
.
VAD
.
load
(
min_speech_duration
=
0.1
,
min_silence_duration
=
0.5
,
)
vad_stream
=
vad
.
stream
(
)
# StreamAdapter will buffer audio until VAD emits END_SPEAKING event
stt
=
agents
.
stt
.
StreamAdapter
(
whisper_stt
,
vad_stream
)
stt_stream
=
stt
.
stream
(
)
.
.
.
Further reading
Text and transcriptions
Integrate realtime text features into your agent.
Pipeline nodes
Learn how to customize the behavior of your agent by overriding nodes in the voice pipeline.


Content from https://docs.livekit.io/agents/integrations/tts/:

On this page
Overview
Available providers
How to use
Usage in  AgentSession
Standalone usage
Further reading
Copy page
See more page options
Overview
Text-to-speech (TTS) models produce realtime synthetic speech from text input. In voice AI, this allows a text-based
LLM
to speak its response to the user.
Available providers
The agents framework includes plugins for the following TTS providers out-of-the-box. Choose a provider from the list for a step-by-step guide. You can also implement the
TTS node
to provide custom behavior or an alternative provider.
All TTS providers support high quality, low-latency, and lifelike multilingual voice synthesis. Support for other features is noted in the following table.
Provider
Prompt
Custom Voices
Pronunciation
Aligned Transcripts
Amazon Polly
—
—
✓
SSML
—
Azure AI Speech
—
—
✓
SSML
—
Azure OpenAI
✓
—
—
—
Baseten
—
—
—
—
Cartesia
—
✓
✓
SSML
✓
Deepgram
—
—
—
—
ElevenLabs
—
✓
✓
SSML
✓
Google Cloud
—
—
✓
SSML
—
Groq
—
—
—
—
Hume
✓
✓
—
—
Inworld
—
✓
—
—
LMNT
—
✓
—
—
Neuphonic
—
✓
—
—
OpenAI
✓
—
—
—
PlayHT
—
✓
—
—
Resemble AI
—
✓
✓
SSML
—
Rime
—
—
✓
Custom
—
Sarvam
—
—
—
—
Speechify
—
✓
✓
SSML
—
Spitch
—
—
—
—
Have another provider in mind? LiveKit is open source and welcomes
new plugin contributions
.
How to use
The following sections describe high-level usage only.
For more detailed information about installing and using plugins, see the
plugins overview
.
Usage in
AgentSession
Construct an
AgentSession
or
Agent
with a
TTS
instance created by your desired plugin:
from
livekit
.
agents
import
AgentSession
from
livekit
.
plugins
import
cartesia
session
=
AgentSession
(
tts
=
cartesia
.
TTS
(
model
=
"sonic-english"
)
)
AgentSession
automatically sends LLM responses to the TTS model, and also supports a
say
method for one-off responses.
Standalone usage
You can also use a
TTS
instance in a standalone fashion by creating a stream. You can use
push_text
to add text to the stream, and then consume a stream of
SynthesizedAudio
as to publish as
realtime audio
to another participant.
Here is an example of a standalone TTS app:
agent.py
from
livekit
import
agents
,
rtc
from
livekit
.
agents
.
tts
import
SynthesizedAudio
from
livekit
.
plugins
import
cartesia
from
typing
import
AsyncIterable
async
def
entrypoint
(
ctx
:
agents
.
JobContext
)
:
text_stream
:
AsyncIterable
[
str
]
=
.
.
.
# you need to provide a stream of text
audio_source
=
rtc
.
AudioSource
(
44100
,
1
)
track
=
rtc
.
LocalAudioTrack
.
create_audio_track
(
"agent-audio"
,
audio_source
)
await
ctx
.
room
.
local_participant
.
publish_track
(
track
)
tts
=
cartesia
.
TTS
(
model
=
"sonic-english"
)
tts_stream
=
tts
.
stream
(
)
# create a task to consume and publish audio frames
ctx
.
create_task
(
send_audio
(
tts_stream
)
)
# push text into the stream, TTS stream will emit audio frames along with events
# indicating sentence (or segment) boundaries.
async
for
text
in
text_stream
:
tts_stream
.
push_text
(
text
)
tts_stream
.
end_input
(
)
async
def
send_audio
(
audio_stream
:
AsyncIterable
[
SynthesizedAudio
]
)
:
async
for
a
in
audio_stream
:
await
audio_source
.
capture_frame
(
e
.
audio
.
frame
)
if
__name__
==
"__main__"
:
agents
.
cli
.
run_app
(
agents
.
WorkerOptions
(
entrypoint_fnc
=
entrypoint
)
)
Further reading
Agent speech docs
Explore the speech capabilities and features of LiveKit Agents.
Pipeline nodes
Learn how to customize the behavior of your agent by overriding nodes in the voice pipeline.


Content from https://docs.livekit.io/agents/integrations/avatar/:

On this page
Overview
Available providers
How it works
Sample code
Avatar workers
Frontend starter apps
Further reading
Copy page
See more page options
Overview
Virtual avatars add lifelike video output for your voice AI agents. You can integrate a variety of providers to LiveKit Agents with just a few lines of code.
Available providers
The following providers are available. Choose a provider from this list for a step-by-step guide:
Provider
Beyond Presence
bitHuman
Hedra
Tavus
Have another provider in mind? LiveKit is open source and welcomes
new plugin contributions
.
How it works
The virtual avatar integrations works with the
AgentSession
class automatically. The plugin adds a separate participant, the avatar worker, to the room. The agent session sends its audio output to the avatar worker instead of to the room, which the avatar worker uses to publish synchronized audio + video tracks to the room and the end user.
To add a virtual avatar:
Install
the selected plugin and API keys
Create an
AgentSession
, as in the
voice AI quickstart
Create an
AvatarSession
and configure it as necessary
Start the avatar session, passing in the
AgentSession
instance
Start the
AgentSession
with audio output disabled (the audio is sent to the avatar session instead)
Sample code
Here is an example using
Tavus
:
from
livekit
import
agents
from
livekit
.
agents
import
AgentSession
,
RoomOutputOptions
from
livekit
.
plugins
import
tavus
async
def
entrypoint
(
ctx
:
agents
.
JobContext
)
:
session
=
AgentSession
(
# ... stt, llm, tts, etc.
)
avatar
=
tavus
.
AvatarSession
(
replica_id
=
"..."
,
# ID of the Tavus replica to use
persona_id
=
"..."
,
# ID of the Tavus persona to use (see preceding section for configuration details)
)
# Start the avatar and wait for it to join
await
avatar
.
start
(
session
,
room
=
ctx
.
room
)
# Start your agent session with the user
await
session
.
start
(
room
=
ctx
.
room
,
room_output_options
=
RoomOutputOptions
(
# Disable audio output to the room. The avatar plugin publishes audio separately.
audio_enabled
=
False
,
)
,
# ... agent, room_input_options, etc....
)
Avatar workers
To minimize latency, the avatar provider joins the LiveKit room directly as a secondary participant to publish synchronized audio and video to the room. In your frontend app, you must distinguish between the agent — your Python program running the
AgentSession
— and the avatar worker.
Loading diagram…
You can identify an avatar worker as a participant of kind
agent
with the attribute
lk.publish_on_behalf
. Check for these values in your frontend code to associate the worker's audio and video tracks with the agent.
const
agent
=
room
.
remoteParticipants
.
find
(
p
=>
p
.
kind
===
Kind
.
Agent
&&
p
.
attributes
[
'lk.publish_on_behalf'
]
===
null
)
;
const
avatarWorker
=
room
.
remoteParticipants
.
find
(
p
=>
p
.
kind
===
Kind
.
Agent
&&
p
.
attributes
[
'lk.publish_on_behalf'
]
===
agent
.
identity
)
;
In React apps, use the
useVoiceAssistant hook
to get the correct audio and video tracks automatically:
const
{
agent
,
// The agent participant
audioTrack
,
// the worker's audio track
videoTrack
,
// the worker's video track
}
=
useVoiceAssistant
(
)
;
Frontend starter apps
The following
frontend starter apps
include out-of-the-box support for virtual avatars.
Swift
SwiftUI Voice Agent
A native iOS, macOS, and visionOS voice AI assistant built in SwiftUI.
GitHub
livekit-examples/agent-starter-swift
Next.js
Next.js Voice Agent
A web voice AI assistant built with React and Next.js.
GitHub
livekit-examples/agent-starter-react
Flutter
Flutter Voice Agent
A cross-platform voice AI assistant app built with Flutter.
GitHub
livekit-examples/agent-starter-flutter
React
React Native Voice Agent
A native voice AI assistant app built with React Native and Expo.
GitHub
livekit-examples/agent-starter-react-native
Android
Android Voice Agent
A native Android voice AI assistant app built with Kotlin and Jetpack Compose.
GitHub
livekit-examples/agent-starter-android
Agents Playground
A virtual workbench to test your multimodal AI agent.
Further reading
Web and mobile frontends
Guide to adding web or mobile frontends to your agent.
Vision
Give your agent the ability to see you, too.
On this page
Overview
Available providers
How it works
Sample code
Avatar workers
Frontend starter apps
Further reading


Content from https://docs.livekit.io/home/cli/cli-setup:

On this page
Install LiveKit CLI
Authenticate with Cloud (optional)
Generate access token
Test with LiveKit Meet
Simulating another publisher
Copy page
See more page options
Install LiveKit CLI
macOS
Linux
Windows
From Source
brew update && brew install livekit-cli
lk
is LiveKit's suite of CLI utilities. It lets you conveniently access server APIs, create tokens, and generate test traffic all from your command line. For more details, refer to the docs in the
livekit-cli
GitHub repo
.
Authenticate with Cloud (optional)
For LiveKit Cloud users, you can authenticate the CLI with your Cloud project to create an API key and secret. This allows you to use the CLI without manually providing credentials each time.
lk cloud auth
Then, follow instructions and log in from a browser.
Tip
If you're looking to explore LiveKit's
Agents
framework, or want to prototype your app against a prebuilt frontend or token server, check out
Sandboxes
.
Generate access token
A participant creating or joining a LiveKit
room
needs an
access token
to do so. For now, let’s generate one via CLI:
Cloud
Localhost
lk token create
\
--api-key
<
PROJECT_KEY
>
--api-secret
<
PROJECT_SECRET
>
\
--join
--room
test_room
--identity
test_user
\
--valid-for 24h
Alternatively, you can
generate tokens from your project's dashboard
.
Test with LiveKit Meet
Tip
If you're testing a LiveKit Cloud instance, you can find your
Project URL
(it starts with
wss://
) in the project settings.
Use a sample app,
LiveKit Meet
, to preview your new LiveKit instance. Enter the token you
previously generated
in the "Custom" tab. Once connected, your microphone and camera will be streamed in realtime to your new LiveKit instance (and any other participant who connects to the same room)!
If interested, here's the
full source
for this example app.
Simulating another publisher
One way to test a multi-user session is by
generating
a second token (ensure
--identity
is unique), opening our example app in another
browser tab
and connecting to the same room.
Another way is to use the CLI as a simulated participant and publish a prerecorded video to the room. Here's how:
Cloud
Localhost
lk room
join
\
--url
<
PROJECT_SECURE_WEBSOCKET_ADDRESS
>
\
--api-key
<
PROJECT_API_KEY
>
--api-secret
<
PROJECT_SECRET_KEY
>
\
--publish-demo
--identity
bot_user
\
my_first_room
This command publishes a looped demo video to
my-first-room
. Due to how the file was encoded, expect a short delay before your browser has sufficient data to render frames.
On this page
Install LiveKit CLI
Authenticate with Cloud (optional)
Generate access token
Test with LiveKit Meet
Simulating another publisher


Content from https://docs.livekit.io/home/cli/templates:

Copy page
See more page options
Note
Before starting, make sure you have created a Cloud account,
installed the LiveKit CLI
, and have authenticated or manually configured your LiveKit project of choice.
The LiveKit CLI can help you bootstrap applications from a number of convenient template repositories, using your project credentials to set up required environment variables and other configuration automatically. To create an application from a template, run the following:
lk app create
--template
<
template_name
>
my-app
Then follow the CLI prompts to finish your setup.
The
--template
flag may be omitted to see a list of all available templates, or can be chosen from a selection of our first-party templates:
Template Name
Language/Framework
Description
agent-starter-python
Python
A starter project for Python, featuring a simple voice agent implementation
voice-assistant-frontend
TypeScript/Next.js
A starter app for Next.js, featuring a flexible voice AI frontend
agent-starter-android
Kotlin/Android
A starter project for Android, featuring a flexible voice AI frontend
agent-starter-swift
Swift
A starter project for Swift, featuring a flexible voice AI frontend
agent-starter-flutter
Flutter
A starter project for Flutter, featuring a flexible voice AI frontend
agent-starter-react-native
React Native/Expo
A starter project for Expo, featuring a flexible voice AI frontend
agent-starter-embed
TypeScript/Next.js
A starter project for a flexible voice AI that can be embedded in any website
token-server
Node.js/TypeScript
A hosted token server to help you prototype your mobile applications faster
meet
TypeScript/Next.js
An open source video conferencing app built on LiveKit Components and Next.js
multi-agent-python
Python
A team of writing coach agents demonstrating multi-agent workflows
outbound-caller-python
Python
An agent that makes outbound calls using LiveKit SIP
Tip
If you're looking to explore LiveKit's
Agents
framework, or want to prototype your app against a prebuilt frontend or token server, check out
Sandboxes
.
For more information on templates, see the
LiveKit Template Index
.


Content from https://docs.livekit.io/home/client/connect:

On this page
Overview
Installing the LiveKit SDK
Connecting to a room
Disconnection
Automatic disconnection
Connection reliability
Network changes and reconnection
Copy page
See more page options
Overview
Your application will connect to LiveKit using the
Room
object, which is the base construct in LiveKit. Think of it like a conference call — multiple participants can join a room and share realtime audio, video, and data with each other.
Depending on your application, each participant might represent a user, an AI agent, a connected device, or some other program you've created. There is no limit on the number of participants in a room and each participant can publish audio, video, and data to the room.
Installing the LiveKit SDK
LiveKit includes open-source SDKs for every major platform including JavaScript, Swift, Android, React Native, Flutter, and Unity.
LiveKit also has SDKs for realtime backend apps in Python, Node.js, Go, and Rust. These are designed to be used with the
Agents framework
for realtime AI applications.
JavaScript
Swift
Android
React Native
Flutter
Install the LiveKit SDK and optional React Components library:
npm
install
livekit-client @livekit/components-react @livekit/components-styles
--save
The SDK is also available using
yarn
or
pnpm
.
Check out the dedicated quickstarts for
React
or
Next.js
if you're using one of those platforms.
If your SDK is not listed above, check out the full list of
platform-specific quickstarts
and
SDK reference docs
for more details.
Connecting to a room
Rooms are identified by their name, which can be any unique string. The room itself is created automatically when the first participant joins, and is closed when the last participant leaves.
You must use a participant identity when you connect to a room. This identity can be any string, but must be unique to each participant.
Connecting to a room always requires two parameters:
wsUrl
: The WebSocket URL of your LiveKit server.
LiveKit Cloud users can find theirs on the
Project Settings page
.
Self-hosted users who followed
this guide
can use
ws://localhost:7880
while developing.
token
: A unique
access token
which each participant must use to connect.
The token encodes the room name, the participant's identity, and their permissions.
For help generating tokens, see
this guide
.
JavaScript
React
Swift
Android
React Native
Flutter
const
room
=
new
Room
(
)
;
await
room
.
connect
(
wsUrl
,
token
)
;
Upon successful connection, the
Room
object will contain two key attributes: a
localParticipant
object, representing the current user, and
remoteParticipants
, an array of other participants in the room.
Once connected, you can
publish
and
subscribe
to realtime media tracks or
exchange data
with other participants.
LiveKit also emits a number of events on the
Room
object, such as when new participants join or tracks are published. For details, see
Handling Events
.
Disconnection
Call
Room.disconnect()
to leave the room. If you terminate the application without calling
disconnect()
, your participant disappears after 15 seconds.
Note
On some platforms, including JavaScript and Swift,
Room.disconnect
is called automatically when the application exits.
Automatic disconnection
Participants might get disconnected from a room due to server-initiated actions. This can happen if the room is closed using the
DeleteRoom
API or if a participant is removed with the
RemoveParticipant
API.
In such cases, a
Disconnected
event is emitted, providing a reason for the disconnection. Common
disconnection reasons
include:
DUPLICATE_IDENTITY: Disconnected because another participant with the same identity joined the room.
ROOM_DELETED: The room was closed via the
DeleteRoom
API.
PARTICIPANT_REMOVED: Removed from the room using the
RemoveParticipant
API.
JOIN_FAILURE: Failure to connect to the room, possibly due to network issues.
ROOM_CLOSED: The room was closed because all
Standard and Ingress participants
left.
Connection reliability
LiveKit enables reliable connectivity in a wide variety of network conditions. It tries the following WebRTC connection types in descending order:
ICE over UDP: ideal connection type, used in majority of conditions
TURN with UDP (3478): used when ICE/UDP is unreachable
ICE over TCP: used when network disallows UDP (i.e. over VPN or corporate firewalls)
TURN with TLS: used when firewall only allows outbound TLS connections
Cloud
Self-hosted
LiveKit Cloud supports all of the above connection types. TURN servers with TLS are provided and maintained by LiveKit Cloud.
Network changes and reconnection
With WiFi and cellular networks, users may sometimes run into network changes that cause the connection to the server to be interrupted. This could include switching from WiFi to cellular or going through spots with poor connection.
When this happens, LiveKit will attempt to resume the connection automatically. It reconnects to the signaling WebSocket and initiates an
ICE restart
for the WebRTC connection.
This process usually results in minimal or no disruption for the user. However, if media delivery over the previous connection fails, users might notice a temporary pause in video, lasting a few seconds, until the new connection is established.
In scenarios where an ICE restart is not feasible or unsuccessful, LiveKit will execute a full reconnection. As full reconnections take more time and might be more disruptive, a
Reconnecting
event is triggered. This allows your application to respond, possibly by displaying a UI element, during the reconnection process.
This sequence goes like the following:
ParticipantDisconnected
fired for other participants in the room
If there are tracks unpublished, you will receive
LocalTrackUnpublished
for them
Emits
Reconnecting
Performs full reconnect
Emits
Reconnected
For everyone currently in the room, you will receive
ParticipantConnected
Local tracks are republished, emitting
LocalTrackPublished
events
In essence, the full reconnection sequence is identical to everyone else having left the room, and came back.
On this page
Overview
Installing the LiveKit SDK
Connecting to a room
Disconnection
Automatic disconnection
Connection reliability
Network changes and reconnection


Content from https://docs.livekit.io/home/client/tracks/:

On this page
Overview
Audio tracks
Video tracks
Example use cases
AI voice agent
Video conference
Livestreaming
AI camera monitoring
Copy page
See more page options
Overview
LiveKit provides realtime media exchange between participants using tracks. Each participant can
publish
and
subscribe
to as many tracks as makes sense for your application.
Audio tracks
Audio tracks are typically published from your microphone and played back on the other participants' speakers. You can also produce custom audio tracks, for instance to add background music or other audio effects.
AI agents can consume an audio track to perform speech-to-text, and can publish their own audio track with synthesized speech or other audio effects.
Video tracks
Video tracks are usually published from a webcam or other video source, and rendered on the other participants' screens within your application's UI. LiveKit also supports screen sharing, which commonly results in two video tracks from the same participant.
AI agents can subscribe to video tracks to perform vision-based tasks, and can publish their own video tracks with synthetic video or other visual effects.
Example use cases
The following examples demonstrate how to model your application for different use cases.
AI voice agent
Each room has two participants: an end-user and an AI agent. They can have a natural conversation with the following setup:
End-user
: publishes their microphone track and subscribes to the AI agent's audio track
AI agent
: subscribes to the user's microphone track and publishes its own audio track with synthesized speech
The UI may be a simple audio visualizer showing that the AI agent is speaking.
Video conference
Each room has multiple users. Each user publishes audio and/or video tracks and subscribes to all tracks published by others. In the UI, the room is typically displayed as a grid of video tiles.
Livestreaming
Each room has one broadcaster and a significant number of viewers. The broadcaster publishes audio and video tracks. The viewers subscribe to the broadcaster's tracks but do not publish their own. Interaction is typically performed with a chat component.
An AI agent may also join the room to publish live captions.
AI camera monitoring
Each room has one camera participant that publishes its video track, and one agent that monitors the camera feed and calls out to an external API to take action based on contents of the video feed (e.g. send an alert).
Alternatively, one room can have multiple cameras and an agent that monitors all of them, or an end-user could also optionally join the room to monitor the feeds alongside the agent.
On this page
Overview
Audio tracks
Video tracks
Example use cases
AI voice agent
Video conference
Livestreaming
AI camera monitoring


Content from https://docs.livekit.io/home/client/data/:

Copy page
See more page options
Sending text
Use text streams to send and receive text data, such as LLM responses or chat messages.
Sending files & bytes
Use byte streams to transfer files, images, or any other binary data.
Remote method calls
Use RPC to execute custom methods on other participants in the room and await a response.
For low-level control over individual packet behavior, LiveKit also includes a simple
data packets
API.


Content from https://docs.livekit.io/home/client/state/:

Participant attributes & metadata
A key-value store for every participant that can be used for managing online status, user preferences, and more.
Room metadata
A freeform string for room-wide state, ideal for room configuration and shared settings.


Content from https://docs.livekit.io/home/client/events:

On this page
Overview
Declarative UI
Events
Copy page
See more page options
Overview
The LiveKit SDKs use events to communicate with the application changes that are taking place in the room.
There are two kinds of events,
room events
and
participant events
. Room events are emitted from the main
Room
object, reflecting any change in the room. Participant events are emitted from each
Participant
, when that specific participant has changed.
Room events are generally a superset of participant events. As you can see, some events are fired on both
Room
and
Participant
; this is intentional. This duplication is designed to make it easier to componentize your application. For example, if you have a UI component that renders a participant, it should only listen to events scoped to that participant.
Declarative UI
Event handling can be quite complicated in a realtime, multi-user system. Participants could be joining and leaving, each publishing tracks or muting them. To simplify this, LiveKit offers built-in support for
declarative UI
for most platforms.
With declarative UI you specify the how the UI should look given a particular state, without having to worry about the sequence of transformations to apply. Modern frameworks are highly efficient at detecting changes and rendering only what's changed.
React
SwiftUI
Android Compose
Flutter
We offer a few hooks and components that makes working with React much simpler.
useParticipant
- maps participant events to state
useTracks
- returns the current state of the specified audio or video track
VideoTrack
- React component that renders a video track
RoomAudioRenderer
- React component that renders the sound of all audio tracks
const
Stage
=
(
)
=>
{
const
tracks
=
useTracks
(
[
Track
.
Source
.
Camera
,
Track
.
Source
.
ScreenShare
]
)
;
return
(
<
LiveKitRoom
{
/* ... */
}
>
// Render all video
{
tracks
.
map
(
(
track
)
=>
{
<
VideoTrack
trackRef
=
{
track
}
/>
;
}
)
}
// ...and all audio tracks.
<
RoomAudioRenderer
/>
</
LiveKitRoom
>
)
;
}
;
function
ParticipantList
(
)
{
// Render a list of all participants in the room.
const
participants
=
useParticipants
(
)
;
<
ParticipantLoop
participants
=
{
participants
}
>
<
ParticipantName
/>
</
ParticipantLoop
>
;
}
Events
This table captures a consistent set of events that are available across platform SDKs. In addition to what's listed here, there may be platform-specific events on certain platforms.
Event
Description
Room Event
Participant Event
ParticipantConnected
A RemoteParticipant joins
after
the local participant.
✔️
ParticipantDisconnected
A RemoteParticipant leaves
✔️
Reconnecting
The connection to the server has been interrupted and it's attempting to reconnect.
✔️
Reconnected
Reconnection has been successful
✔️
Disconnected
Disconnected from room due to the room closing or unrecoverable failure
✔️
TrackPublished
A new track is published to room after the local participant has joined
✔️
✔️
TrackUnpublished
A RemoteParticipant has unpublished a track
✔️
✔️
TrackSubscribed
The LocalParticipant has subscribed to a track
✔️
✔️
TrackUnsubscribed
A previously subscribed track has been unsubscribed
✔️
✔️
TrackMuted
A track was muted, fires for both local tracks and remote tracks
✔️
✔️
TrackUnmuted
A track was unmuted, fires for both local tracks and remote tracks
✔️
✔️
LocalTrackPublished
A local track was published successfully
✔️
✔️
LocalTrackUnpublished
A local track was unpublished
✔️
✔️
ActiveSpeakersChanged
Current active speakers has changed
✔️
IsSpeakingChanged
The current participant has changed speaking status
✔️
ConnectionQualityChanged
Connection quality was changed for a Participant
✔️
✔️
ParticipantAttributesChanged
A participant's attributes were updated
✔️
✔️
ParticipantMetadataChanged
A participant's metadata was updated
✔️
✔️
RoomMetadataChanged
Metadata associated with the room has changed
✔️
DataReceived
Data received from another participant or server
✔️
✔️
TrackStreamStateChanged
Indicates if a subscribed track has been paused due to bandwidth
✔️
✔️
TrackSubscriptionPermissionChanged
One of subscribed tracks have changed track-level permissions for the current participant
✔️
✔️
ParticipantPermissionsChanged
When the current participant's permissions have changed
✔️
✔️
On this page
Overview
Declarative UI
Events


Content from https://docs.livekit.io/home/quickstarts/nextjs:

On this page
Voice AI quickstart
Getting started guide
Create a Next.js app
Install LiveKit SDK
Keys and Configuration
Create token endpoint
Make a page in your web app
Load the page and connect
Next steps
Copy page
See more page options
Note
This guide is compatible with Next.js 13 or later. On an older version? Check out the
quickstart for Next.js 12
.
Voice AI quickstart
To build your first voice AI app for Next.js, use the following quickstart and the starter app. Otherwise follow the getting started guide below.
Voice AI quickstart
Create a voice AI agent in less than 10 minutes.
Next.js Voice Agent
A web voice AI assistant built with React and Next.js.
GitHub
livekit-examples/agent-starter-react
Getting started guide
This guide walks you through the steps to build a video-conferencing application using NextJS. It uses the
LiveKit React components library
to render the UI and communicate with LiveKit servers via WebRTC. By the end, you will have a basic video-conferencing application you can run with multiple participants.
Create a Next.js app
If you're working with an existing app, skip to the next step.
npx create-next-app
<
your_app_name
>
Change directory into your app directory:
cd
<
your_app_name
>
Install LiveKit SDK
Install both frontend and backend LiveKit SDKs:
yarn
npm
yarn
add
livekit-server-sdk @livekit/components-react @livekit/components-styles
Keys and Configuration
To start, your app needs an LiveKit API key and secret, as well as your LiveKit project URL.
In your project root create the file
.env.local
with the following contents. Do not commit this file because it contains your secrets!
LIVEKIT_API_KEY
=
<
your API Key
>
LIVEKIT_API_SECRET
=
<
your API Secret
>
LIVEKIT_URL
=
<
your LiveKit server URL
>
Reveal API Key and Secret
Create token endpoint
Create a new file at
/app/api/token/route.ts
with the following content:
import
{
NextRequest
,
NextResponse
}
from
'next/server'
;
import
{
AccessToken
}
from
'livekit-server-sdk'
;
// Do not cache endpoint result
export
const
revalidate
=
0
;
export
async
function
GET
(
req
:
NextRequest
)
{
const
room
=
req
.
nextUrl
.
searchParams
.
get
(
'room'
)
;
const
username
=
req
.
nextUrl
.
searchParams
.
get
(
'username'
)
;
if
(
!
room
)
{
return
NextResponse
.
json
(
{
error
:
'Missing "room" query parameter'
}
,
{
status
:
400
}
)
;
}
else
if
(
!
username
)
{
return
NextResponse
.
json
(
{
error
:
'Missing "username" query parameter'
}
,
{
status
:
400
}
)
;
}
const
apiKey
=
process
.
env
.
LIVEKIT_API_KEY
;
const
apiSecret
=
process
.
env
.
LIVEKIT_API_SECRET
;
const
wsUrl
=
process
.
env
.
LIVEKIT_URL
;
if
(
!
apiKey
||
!
apiSecret
||
!
wsUrl
)
{
return
NextResponse
.
json
(
{
error
:
'Server misconfigured'
}
,
{
status
:
500
}
)
;
}
const
at
=
new
AccessToken
(
apiKey
,
apiSecret
,
{
identity
:
username
}
)
;
at
.
addGrant
(
{
room
,
roomJoin
:
true
,
canPublish
:
true
,
canSubscribe
:
true
}
)
;
return
NextResponse
.
json
(
{
token
:
await
at
.
toJwt
(
)
}
,
{
headers
:
{
"Cache-Control"
:
"no-store"
}
}
,
)
;
}
Make a page in your web app
Make a new file at
/app/room/page.tsx
with the following content:
'use client'
;
import
{
ControlBar
,
GridLayout
,
ParticipantTile
,
RoomAudioRenderer
,
useTracks
,
RoomContext
,
}
from
'@livekit/components-react'
;
import
{
Room
,
Track
}
from
'livekit-client'
;
import
'@livekit/components-styles'
;
import
{
useEffect
,
useState
}
from
'react'
;
export
default
function
Page
(
)
{
// TODO: get user input for room and name
const
room
=
'quickstart-room'
;
const
name
=
'quickstart-user'
;
const
[
roomInstance
]
=
useState
(
(
)
=>
new
Room
(
{
// Optimize video quality for each participant's screen
adaptiveStream
:
true
,
// Enable automatic audio/video quality optimization
dynacast
:
true
,
}
)
)
;
useEffect
(
(
)
=>
{
let
mounted
=
true
;
(
async
(
)
=>
{
try
{
const
resp
=
await
fetch
(
`
/api/token?room=
${
room
}
&username=
${
name
}
`
)
;
const
data
=
await
resp
.
json
(
)
;
if
(
!
mounted
)
return
;
if
(
data
.
token
)
{
await
roomInstance
.
connect
(
process
.
env
.
NEXT_PUBLIC_LIVEKIT_URL
,
data
.
token
)
;
}
}
catch
(
e
)
{
console
.
error
(
e
)
;
}
}
)
(
)
;
return
(
)
=>
{
mounted
=
false
;
roomInstance
.
disconnect
(
)
;
}
;
}
,
[
roomInstance
]
)
;
if
(
token
===
''
)
{
return
<
div
>
Getting token...
</
div
>
;
}
return
(
<
RoomContext.Provider
value
=
{
roomInstance
}
>
<
div
data-lk-theme
=
"
default
"
style
=
{
{
height
:
'100dvh'
}
}
>
{
/* Your custom component with basic video conferencing functionality. */
}
<
MyVideoConference
/>
{
/* The RoomAudioRenderer takes care of room-wide audio for you. */
}
<
RoomAudioRenderer
/>
{
/* Controls for the user to start/stop audio, video, and screen share tracks */
}
<
ControlBar
/>
</
div
>
</
RoomContext.Provider
>
)
;
}
function
MyVideoConference
(
)
{
// `useTracks` returns all camera and screen share tracks. If a user
// joins without a published camera track, a placeholder track is returned.
const
tracks
=
useTracks
(
[
{
source
:
Track
.
Source
.
Camera
,
withPlaceholder
:
true
}
,
{
source
:
Track
.
Source
.
ScreenShare
,
withPlaceholder
:
false
}
,
]
,
{
onlySubscribed
:
false
}
,
)
;
return
(
<
GridLayout
tracks
=
{
tracks
}
style
=
{
{
height
:
'calc(100vh - var(--lk-control-bar-height))'
}
}
>
{
/* The GridLayout accepts zero or one child. The child is used
as a template to render all passed in tracks. */
}
<
ParticipantTile
/>
</
GridLayout
>
)
;
}
Load the page and connect
Start your server with:
yarn
npm
yarn
dev
And then open
localhost:3000/room
in your browser.
Next steps
The following resources are useful for getting started with LiveKit on Next.js.
Generating tokens
Guide to generating authentication tokens for your users.
Realtime media
Complete documentation for live video and audio tracks.
Realtime data
Send and receive realtime data between clients.
JavaScript SDK
LiveKit JavaScript SDK on GitHub.
React components
LiveKit React components on GitHub.
JavaScript SDK reference
LiveKit JavaScript SDK reference docs.
React components reference
LiveKit React components reference docs.
On this page
Voice AI quickstart
Getting started guide
Create a Next.js app
Install LiveKit SDK
Keys and Configuration
Create token endpoint
Make a page in your web app
Load the page and connect
Next steps


Content from https://docs.livekit.io/home/quickstarts/nextjs-12:

On this page
Getting started guide
Install LiveKit SDK
Keys and Configuration
Create token endpoint
Make a page in your web app
Load the page and connect
Next steps
Copy page
See more page options
Tip
Using a newer version? See the
updated quickstart for Next.js 13+
.
Getting started guide
This guide walks you through the steps to build a video-conferencing application using NextJS. It uses the
LiveKit React components library
to render the UI and communicate with LiveKit servers via WebRTC. By the end, you will have a basic video-conferencing application you can run with multiple participants.
Install LiveKit SDK
Install the necessary LiveKit SDKs for the web frontend and for the server APIs:
yarn
npm
yarn
add
livekit-server-sdk @livekit/components-react @livekit/components-styles
Keys and Configuration
To start, your app needs an LiveKit API key and secret, as well as your LiveKit project URL.
Create a new file at
.env.development.local
with the following content. Do not commit this file!
LIVEKIT_API_KEY
=
<
your API Key
>
LIVEKIT_API_SECRET
=
<
your API Secret
>
NEXT_PUBLIC_LIVEKIT_URL
=
<
your LiveKit server URL
>
Reveal API Key and Secret
Create token endpoint
Create a new file at
/api/get_lk_token.ts
with the following content:
import
{
NextApiRequest
,
NextApiResponse
}
from
'next'
;
import
{
AccessToken
}
from
'livekit-server-sdk'
;
// Do not cache endpoint result
// See https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config#revalidate
export
const
revalidate
=
0
;
export
default
async
function
handler
(
req
:
NextApiRequest
,
res
:
NextApiResponse
)
{
const
room
=
req
.
query
.
room
as
string
;
const
username
=
req
.
query
.
username
as
string
;
if
(
req
.
method
!==
'GET'
)
{
return
res
.
status
(
400
)
.
json
(
{
error
:
'Invalid method'
}
)
;
}
else
if
(
!
room
)
{
return
res
.
status
(
400
)
.
json
(
{
error
:
'Missing "room" query parameter'
}
)
;
}
else
if
(
!
username
)
{
return
res
.
status
(
400
)
.
json
(
{
error
:
'Missing "username" query parameter'
}
)
;
}
const
apiKey
=
process
.
env
.
LIVEKIT_API_KEY
;
const
apiSecret
=
process
.
env
.
LIVEKIT_API_SECRET
;
const
wsUrl
=
process
.
env
.
NEXT_PUBLIC_LIVEKIT_URL
;
if
(
!
apiKey
||
!
apiSecret
||
!
wsUrl
)
{
return
res
.
status
(
500
)
.
json
(
{
error
:
'Server misconfigured'
}
)
;
}
const
at
=
new
AccessToken
(
apiKey
,
apiSecret
,
{
identity
:
username
}
)
;
at
.
addGrant
(
{
room
,
roomJoin
:
true
,
canPublish
:
true
,
canSubscribe
:
true
}
)
;
res
.
setHeader
(
{
'Cache-Control'
:
'no-store'
}
)
;
res
.
status
(
200
)
.
json
(
{
token
:
await
at
.
toJwt
(
)
}
)
;
}
Make a page in your web app
Make a new file at
/pages/room.tsx
with the following content:
import
{
ControlBar
,
GridLayout
,
ParticipantTile
,
RoomAudioRenderer
,
useTracks
,
RoomContext
,
}
from
'@livekit/components-react'
;
import
{
Room
,
Track
}
from
'livekit-client'
;
import
'@livekit/components-styles'
;
import
{
useEffect
,
useState
}
from
'react'
;
export
default
(
)
=>
{
// TODO: get user input for room and name
const
room
=
'quickstart-room'
;
const
name
=
'quickstart-user'
;
const
[
token
,
setToken
]
=
useState
(
''
)
;
const
[
roomInstance
]
=
useState
(
(
)
=>
new
Room
(
{
// Optimize video quality for each participant's screen
adaptiveStream
:
true
,
// Enable automatic audio/video quality optimization
dynacast
:
true
,
}
)
)
;
useEffect
(
(
)
=>
{
let
mounted
=
true
;
(
async
(
)
=>
{
try
{
const
resp
=
await
fetch
(
`
/api/get_lk_token?room=
${
room
}
&username=
${
name
}
`
)
;
const
data
=
await
resp
.
json
(
)
;
if
(
!
mounted
)
return
;
setToken
(
data
.
token
)
;
if
(
data
.
token
)
{
await
roomInstance
.
connect
(
process
.
env
.
NEXT_PUBLIC_LIVEKIT_URL
,
data
.
token
)
;
}
}
catch
(
e
)
{
console
.
error
(
e
)
;
}
}
)
(
)
;
return
(
)
=>
{
mounted
=
false
;
roomInstance
.
disconnect
(
)
;
}
;
}
,
[
roomInstance
]
)
;
if
(
token
===
''
)
{
return
<
div
>
Getting token...
</
div
>
;
}
return
(
<
RoomContext.Provider
value
=
{
roomInstance
}
>
<
div
data-lk-theme
=
"
default
"
style
=
{
{
height
:
'100dvh'
}
}
>
{
/* Your custom component with basic video conferencing functionality. */
}
<
MyVideoConference
/>
{
/* The RoomAudioRenderer takes care of room-wide audio for you. */
}
<
RoomAudioRenderer
/>
{
/* Controls for the user to start/stop audio, video, and screen share tracks */
}
<
ControlBar
/>
</
div
>
</
RoomContext.Provider
>
)
;
}
;
function
MyVideoConference
(
)
{
// `useTracks` returns all camera and screen share tracks. If a user
// joins without a published camera track, a placeholder track is returned.
const
tracks
=
useTracks
(
[
{
source
:
Track
.
Source
.
Camera
,
withPlaceholder
:
true
}
,
{
source
:
Track
.
Source
.
ScreenShare
,
withPlaceholder
:
false
}
,
]
,
{
onlySubscribed
:
false
}
,
)
;
return
(
<
GridLayout
tracks
=
{
tracks
}
style
=
{
{
height
:
'calc(100vh - var(--lk-control-bar-height))'
}
}
>
{
/* The GridLayout accepts zero or one child. The child is used
as a template to render all passed in tracks. */
}
<
ParticipantTile
/>
</
GridLayout
>
)
;
}
Load the page and connect
Start your server with:
yarn
npm
yarn
dev
And then open
localhost:8000/room
in your browser.
Next steps
The following resources are useful for getting started with LiveKit on Next.js.
Generating tokens
Guide to generating authentication tokens for your users.
Realtime media
Complete documentation for live video and audio tracks.
Realtime data
Send and receive realtime data between clients.
JavaScript SDK
LiveKit JavaScript SDK on GitHub.
React components
LiveKit React components on GitHub.
JavaScript SDK reference
LiveKit JavaScript SDK reference docs.
React components reference
LiveKit React components reference docs.
On this page
Getting started guide
Install LiveKit SDK
Keys and Configuration
Create token endpoint
Make a page in your web app
Load the page and connect
Next steps


Content from https://docs.livekit.io/home/quickstarts/react:

On this page
Voice AI quickstart
Getting started guide
Install LiveKit SDK
Join a room
Next steps
Copy page
See more page options
Voice AI quickstart
To build your first voice AI app for Next.js, use the following quickstart and the starter app. Otherwise follow the getting started guide below.
Voice AI quickstart
Create a voice AI agent in less than 10 minutes.
Next.js Voice Agent
A web voice AI assistant built with React and Next.js.
GitHub
livekit-examples/agent-starter-react
Getting started guide
This guide walks you through the steps to build a video-conferencing application using React. It uses the
LiveKit React components library
to render the UI and communicate with LiveKit servers via WebRTC. By the end, you will have a basic video-conferencing application you can run with multiple participants.
Install LiveKit SDK
Install the LiveKit SDK:
yarn
npm
yarn
add
@livekit/components-react @livekit/components-styles livekit-client
Join a room
Update the
serverUrl
and
token
values and copy and paste the following into your
src/App.tsx
file. To generate a
token for this example, see
Creating a token
.
Note
This example hardcodes a token. In a real app, your server generates a token for you.
import
{
ControlBar
,
GridLayout
,
ParticipantTile
,
RoomAudioRenderer
,
useTracks
,
RoomContext
,
}
from
'@livekit/components-react'
;
import
{
Room
,
Track
}
from
'livekit-client'
;
import
'@livekit/components-styles'
;
import
{
useState
}
from
'react'
;
const
serverUrl
=
'<your LiveKit server URL>'
;
const
token
=
'<generate a token>'
;
export
default
function
App
(
)
{
const
[
room
]
=
useState
(
(
)
=>
new
Room
(
{
// Optimize video quality for each participant's screen
adaptiveStream
:
true
,
// Enable automatic audio/video quality optimization
dynacast
:
true
,
}
)
)
;
// Connect to room
useEffect
(
(
)
=>
{
let
mounted
=
true
;
const
connect
=
async
(
)
=>
{
if
(
mounted
)
{
await
room
.
connect
(
serverUrl
,
token
)
;
}
}
;
connect
(
)
;
return
(
)
=>
{
mounted
=
false
;
room
.
disconnect
(
)
;
}
;
}
,
[
room
]
)
;
return
(
<
RoomContext.Provider
value
=
{
room
}
>
<
div
data-lk-theme
=
"
default
"
style
=
{
{
height
:
'100vh'
}
}
>
{
/* Your custom component with basic video conferencing functionality. */
}
<
MyVideoConference
/>
{
/* The RoomAudioRenderer takes care of room-wide audio for you. */
}
<
RoomAudioRenderer
/>
{
/* Controls for the user to start/stop audio, video, and screen share tracks */
}
<
ControlBar
/>
</
div
>
</
RoomContext.Provider
>
)
;
}
function
MyVideoConference
(
)
{
// `useTracks` returns all camera and screen share tracks. If a user
// joins without a published camera track, a placeholder track is returned.
const
tracks
=
useTracks
(
[
{
source
:
Track
.
Source
.
Camera
,
withPlaceholder
:
true
}
,
{
source
:
Track
.
Source
.
ScreenShare
,
withPlaceholder
:
false
}
,
]
,
{
onlySubscribed
:
false
}
,
)
;
return
(
<
GridLayout
tracks
=
{
tracks
}
style
=
{
{
height
:
'calc(100vh - var(--lk-control-bar-height))'
}
}
>
{
/* The GridLayout accepts zero or one child. The child is used
as a template to render all passed in tracks. */
}
<
ParticipantTile
/>
</
GridLayout
>
)
;
}
Next steps
The following resources are useful for getting started with LiveKit on React.
Generating tokens
Guide to generating authentication tokens for your users.
Realtime media
Complete documentation for live video and audio tracks.
Realtime data
Send and receive realtime data between clients.
JavaScript SDK
LiveKit JavaScript SDK on GitHub.
React components
LiveKit React components on GitHub.
JavaScript SDK reference
LiveKit JavaScript SDK reference docs.
React components reference
LiveKit React components reference docs.
On this page
Voice AI quickstart
Getting started guide
Install LiveKit SDK
Join a room
Next steps


Content from https://docs.livekit.io/home/quickstarts/javascript:

On this page
Voice AI quickstart
Getting started guide
Install LiveKit SDK
Join a room
Next steps
Copy page
See more page options
Tip
Check out the dedicated quickstarts for
React
or
Next.js
if you're using one of those platforms.
Voice AI quickstart
To build your first voice AI app for web, use the following quickstart and the starter app. Otherwise follow the getting started guide below.
Voice AI quickstart
Create a voice AI agent in less than 10 minutes.
Next.js Voice Agent
A web voice AI assistant built with React and Next.js.
GitHub
livekit-examples/agent-starter-react
Getting started guide
This guide covers the basics to connect to LiveKit from a JavaScript app.
Install LiveKit SDK
Install the LiveKit SDK:
yarn
npm
yarn
add
livekit-client
Join a room
Note that this example hardcodes a token. In a real app, you’ll need your server to generate a token for you.
import
{
Room
}
from
'livekit-client'
;
const
wsURL
=
'<your LiveKit server URL>'
;
const
token
=
'<generate a token>'
;
const
room
=
new
Room
(
)
;
await
room
.
connect
(
wsURL
,
token
)
;
console
.
log
(
'connected to room'
,
room
.
name
)
;
// Publish local camera and mic tracks
await
room
.
localParticipant
.
enableCameraAndMicrophone
(
)
;
Next steps
The following resources are useful for getting started with LiveKit in a JavaScript app.
Generating tokens
Guide to generating authentication tokens for your users.
Realtime media
Complete documentation for live video and audio tracks.
Realtime data
Send and receive realtime data between clients.
JavaScript SDK
LiveKit JavaScript SDK on GitHub.
React components
LiveKit React components on GitHub.
JavaScript SDK reference
LiveKit JavaScript SDK reference docs.
React components reference
LiveKit React components reference docs.
On this page
Voice AI quickstart
Getting started guide
Install LiveKit SDK
Join a room
Next steps


Content from https://docs.livekit.io/home/quickstarts/unity-web:

On this page
1. Install LiveKit SDK
2. Connect to a room
3. Publish video & audio
4. Display a video on a RawImage
5. Next Steps
Copy page
See more page options
1. Install LiveKit SDK
Click the Add
+
menu in the Package Manager toolbar, select
Add package from git URL
, and enter:
https://github.com/livekit/client-sdk-unity-web.git
For more details, see the
Unity docs on installing packages from Git URLs
.
2. Connect to a room
Note that this example hardcodes a token. In a real app, you’ll need your server to generate a token for you.
public
class
MyObject
:
MonoBehaviour
{
public
Room
Room
;
IEnumerator
Start
(
)
{
Room
=
new
Room
(
)
;
var
c
=
Room
.
Connect
(
"<your LiveKit server URL>"
,
"<generate a token>"
)
;
yield
return
c
;
if
(
!
c
.
IsError
)
{
// Connected
}
}
}
3. Publish video & audio
yield
return
Room
.
LocalParticipant
.
EnableCameraAndMicrophone
(
)
;
4. Display a video on a RawImage
RawImage
image
=
GetComponent
<
RawImage
>
(
)
;
Room
.
TrackSubscribed
+=
(
track
,
publication
,
participant
)
=>
{
if
(
track
.
Kind
==
TrackKind
.
Video
)
{
var
video
=
track
.
Attach
(
)
as
HTMLVideoElement
;
video
.
VideoReceived
+=
tex
=>
{
// VideoReceived is called every time the video resolution changes
image
.
texture
=
tex
;
}
;
}
}
;
5. Next Steps
Set up a server to generate tokens for your app at runtime by following this guide:
Generating Tokens
.
View the
full SDK reference
and
GitHub repository
for more documentation and examples.
Happy coding!
On this page
1. Install LiveKit SDK
2. Connect to a room
3. Publish video & audio
4. Display a video on a RawImage
5. Next Steps


Content from https://docs.livekit.io/home/quickstarts/swift:

On this page
Voice AI quickstart
Getting started guide
SDK installation
Permissions and entitlements
Connecting to LiveKit
Next steps
Copy page
See more page options
Voice AI quickstart
To build your first voice AI app for SwiftUI, use the following quickstart and the starter app. Otherwise follow the getting started guide below.
Voice AI quickstart
Create a voice AI agent in less than 10 minutes.
SwiftUI Voice Agent
A native iOS, macOS, and visionOS voice AI assistant built in SwiftUI.
GitHub
livekit-examples/agent-starter-swift
Getting started guide
This guide uses the Swift Components library for the easiest way to get started on iOS.
LiveKit also supports macOS, tvOS, and visionOS. More documentation for the core Swift SDK is
on GitHub
.
Otherwise follow this guide to build your first LiveKit app with SwiftUI.
SDK installation
Package.swift
Xcode
let
package
=
Package
(
...
dependencies
:
[
.
package
(
url
:
"https://github.com/livekit/client-sdk-swift.git"
,
from
:
"2.5.0"
)
,
// Core SDK
.
package
(
url
:
"https://github.com/livekit/components-swift.git"
,
from
:
"0.1.0"
)
,
// UI Components
]
,
targets
:
[
.
target
(
name
:
"MyApp"
,
dependencies
:
[
.
product
(
name
:
"LiveKitComponents"
,
package
:
"components-swift"
)
,
]
)
]
)
Permissions and entitlements
You must add privacy strings for both camera and microphone usage to your
Info.plist
file, even if you don't plan to use both in your app.
<
dict
>
...
<
key
>
NSCameraUsageDescription
</
key
>
<
string
>
$(PRODUCT_NAME) uses your camera
</
string
>
<
key
>
NSMicrophoneUsageDescription
</
key
>
<
string
>
$(PRODUCT_NAME) uses your microphone
</
string
>
...
</
dict
>
To continue audio sessions in the background add the
Audio, AirPlay, and Picture in Picture
background mode to the Capabilities tab of your app target in Xcode.
Your
Info.plist
should have the following entries:
<
dict
>
...
<
key
>
UIBackgroundModes
</
key
>
<
array
>
<
string
>
audio
</
string
>
</
array
>
Connecting to LiveKit
This simple example uses a hardcoded token that expires in 2 hours. In a real app, you’ll need to
generate a token
with your server.
ContentView.swift
// !! Note !!
// This sample hardcodes a token which expires in 2 hours.
let
wsURL
=
"<your LiveKit server URL>"
let
token
=
"<generate a token>"
// In production you should generate tokens on your server, and your client
// should request a token from your server.
@preconcurrency
import
LiveKit
import
LiveKitComponents
import
SwiftUI
struct
ContentView
:
View
{
@StateObject
private
var
room
:
Room
init
(
)
{
let
room
=
Room
(
)
_room
=
StateObject
(
wrappedValue
:
room
)
}
var
body
:
some
View
{
Group
{
if
room
.
connectionState
==
.
disconnected
{
Button
(
"Connect"
)
{
Task
{
do
{
try
await
room
.
connect
(
url
:
wsURL
,
token
:
token
,
connectOptions
:
ConnectOptions
(
enableMicrophone
:
true
)
)
try
await
room
.
localParticipant
.
setCamera
(
enabled
:
true
)
}
catch
{
print
(
"Failed to connect to LiveKit:
\(
error
)
"
)
}
}
}
}
else
{
LazyVStack
{
ForEachParticipant
{
_
in
VStack
{
ForEachTrack
(
filter
:
.
video
)
{
trackReference
in
VideoTrackView
(
trackReference
:
trackReference
)
.
frame
(
width
:
500
,
height
:
500
)
}
}
}
}
}
}
.
padding
(
)
.
environmentObject
(
room
)
}
}
For more details, you can reference
the components example app
.
Next steps
The following resources are useful for getting started with LiveKit on iOS.
Generating tokens
Guide to generating authentication tokens for your users.
Realtime media
Complete documentation for live video and audio tracks.
Realtime data
Send and receive realtime data between clients.
Swift SDK
LiveKit Swift SDK on GitHub.
SwiftUI Components
LiveKit SwiftUI Components on GitHub.
Swift SDK reference
LiveKit Swift SDK reference docs.
SwiftUI components reference
LiveKit SwiftUI components reference docs.
On this page
Voice AI quickstart
Getting started guide
SDK installation
Permissions and entitlements
Connecting to LiveKit
Next steps


Content from https://docs.livekit.io/home/quickstarts/android-compose:

On this page
Voice AI quickstart
Getting started guide
SDK installation
Permissions
Connecting to LiveKit
Next steps
Copy page
See more page options
Voice AI quickstart
To build your first voice AI app for Android, use the following quickstart and the starter app. Otherwise follow the getting started guide below.
Voice AI quickstart
Create a voice AI agent in less than 10 minutes.
Android Voice Agent
A native Android voice AI assistant app built with Kotlin and Jetpack Compose.
GitHub
livekit-examples/agent-starter-android
Getting started guide
This guide uses the Android Components library for the easiest way to get started on Android.
If you are using the traditional view-based system, check out the
Android quickstart
.
Otherwise follow this guide to build your first LiveKit app with Android Compose.
SDK installation
LiveKit Components for Android Compose is available as a Maven package.
...
dependencies {
implementation "io.livekit:livekit-android-compose-components:<current version>"
}
See the
releases page
for information on the latest version of the SDK.
You'll also need JitPack as one of your repositories. In your
settings.gradle
file:
dependencyResolutionManagement {
repositories {
google()
mavenCentral()
//...
maven { url 'https://jitpack.io' }
}
}
Permissions
LiveKit relies on the
RECORD_AUDIO
and
CAMERA
permissions to use the microphone and camera.
These permission must be requested at runtime, like so:
/**
* Checks if the RECORD_AUDIO and CAMERA permissions are granted.
*
* If not granted, will request them. Will call onPermissionGranted if/when
* the permissions are granted.
*/
fun
ComponentActivity
.
requireNeededPermissions
(
onPermissionsGranted
:
(
(
)
->
Unit
)
?
=
null
)
{
val
requestPermissionLauncher
=
registerForActivityResult
(
ActivityResultContracts
.
RequestMultiplePermissions
(
)
)
{
grants
->
// Check if any permissions weren't granted.
for
(
grant
in
grants
.
entries
)
{
if
(
!
grant
.
value
)
{
Toast
.
makeText
(
this
,
"Missing permission:
${
grant
.
key
}
"
,
Toast
.
LENGTH_SHORT
)
.
show
(
)
}
}
// If all granted, notify if needed.
if
(
onPermissionsGranted
!=
null
&&
grants
.
all
{
it
.
value
}
)
{
onPermissionsGranted
(
)
}
}
val
neededPermissions
=
listOf
(
Manifest
.
permission
.
RECORD_AUDIO
,
Manifest
.
permission
.
CAMERA
)
.
filter
{
ContextCompat
.
checkSelfPermission
(
this
,
it
)
==
PackageManager
.
PERMISSION_DENIED
}
.
toTypedArray
(
)
if
(
neededPermissions
.
isNotEmpty
(
)
)
{
requestPermissionLauncher
.
launch
(
neededPermissions
)
}
else
{
onPermissionsGranted
?
.
invoke
(
)
}
}
Connecting to LiveKit
Note that this example hardcodes a token we generated for you that expires in 2 hours. In a real app, you’ll need your server to generate a token for you.
// !! Note !!
// This sample hardcodes a token which expires in 2 hours.
const
val
wsURL
=
"<your LiveKit server URL>"
const
val
token
=
"<generate a token>"
// In production you should generate tokens on your server, and your frontend
// should request a token from your server.
class
MainActivity
:
ComponentActivity
(
)
{
override
fun
onCreate
(
savedInstanceState
:
Bundle
?
)
{
super
.
onCreate
(
savedInstanceState
)
requireNeededPermissions
{
setContent
{
RoomScope
(
url
=
wsURL
,
token
=
token
,
audio
=
true
,
video
=
true
,
connect
=
true
,
)
{
// Get all the tracks in the room.
val
trackRefs
=
rememberTracks
(
)
// Display the video tracks.
// Audio tracks are automatically played.
LazyColumn
(
modifier
=
Modifier
.
fillMaxSize
(
)
)
{
items
(
trackRefs
.
size
)
{
index
->
VideoTrackView
(
trackReference
=
trackRefs
[
index
]
,
modifier
=
Modifier
.
fillParentMaxHeight
(
0.5f
)
)
}
}
}
}
}
}
}
(For more details, you can reference
the complete quickstart app
.)
Next steps
The following resources are useful for getting started with LiveKit on Android.
Generating tokens
Guide to generating authentication tokens for your users.
Realtime media
Complete documentation for live video and audio tracks.
Realtime data
Send and receive realtime data between clients.
Android SDK
LiveKit Android SDK on GitHub.
Android components
LiveKit Android components on GitHub.
Android SDK reference
LiveKit Android SDK reference docs.
Android components reference
LiveKit Android components reference docs.
On this page
Voice AI quickstart
Getting started guide
SDK installation
Permissions
Connecting to LiveKit
Next steps


Content from https://docs.livekit.io/home/quickstarts/android:

On this page
Voice AI quickstart
Getting started guide
Install LiveKit SDK
Permissions
Connect to LiveKit
Next steps
Copy page
See more page options
Voice AI quickstart
To build your first voice AI app for Android, use the following quickstart and the starter app. Otherwise follow the getting started guide below.
Voice AI quickstart
Create a voice AI agent in less than 10 minutes.
Android Voice Agent
A native Android voice AI assistant app built with Kotlin and Jetpack Compose.
GitHub
livekit-examples/agent-starter-android
Getting started guide
This guide is for Android apps using the traditional view-based system. If you are using Jetpack Compose, check out the
Compose quickstart guide
.
Install LiveKit SDK
LiveKit for Android is available as a Maven package.
...
dependencies {
implementation "io.livekit:livekit-android:<current version>"
}
See the
releases page
for information on the latest version of the SDK.
You'll also need JitPack as one of your repositories. In your
settings.gradle
file:
dependencyResolutionManagement {
repositories {
google()
mavenCentral()
//...
maven { url 'https://jitpack.io' }
}
}
Permissions
LiveKit relies on the
RECORD_AUDIO
and
CAMERA
permissions to use the microphone and camera.
These permission must be requested at runtime, like so:
private
fun
requestPermissions
(
)
{
val
requestPermissionLauncher
=
registerForActivityResult
(
ActivityResultContracts
.
RequestMultiplePermissions
(
)
)
{
grants
->
for
(
grant
in
grants
.
entries
)
{
if
(
!
grant
.
value
)
{
Toast
.
makeText
(
this
,
"Missing permission:
${
grant
.
key
}
"
,
Toast
.
LENGTH_SHORT
)
.
show
(
)
}
}
}
val
neededPermissions
=
listOf
(
Manifest
.
permission
.
RECORD_AUDIO
,
Manifest
.
permission
.
CAMERA
)
.
filter
{
ContextCompat
.
checkSelfPermission
(
this
,
it
)
==
PackageManager
.
PERMISSION_DENIED
}
.
toTypedArray
(
)
if
(
neededPermissions
.
isNotEmpty
(
)
)
{
requestPermissionLauncher
.
launch
(
neededPermissions
)
}
}
Connect to LiveKit
Use the following code to connect and publish audio/video to a room, while rendering the video from other connected participants.
LiveKit uses
SurfaceViewRenderer
to render video tracks. A
TextureView
implementation is also
provided through
TextureViewRenderer
. Subscribed audio tracks are automatically played.
Note that this example hardcodes a token we generated for you that expires in 2 hours. In a real app, you’ll need your server to generate a token for you.
// !! Note !!
// This sample hardcodes a token which expires in 2 hours.
const
val
wsURL
=
"<your LiveKit server URL>"
const
val
token
=
"<generate a token>"
// In production you should generate tokens on your server, and your frontend
// should request a token from your server.
class
MainActivity
:
AppCompatActivity
(
)
{
lateinit
var
room
:
Room
override
fun
onCreate
(
savedInstanceState
:
Bundle
?
)
{
super
.
onCreate
(
savedInstanceState
)
setContentView
(
R
.
layout
.
activity_main
)
// Create Room object.
room
=
LiveKit
.
create
(
applicationContext
)
// Setup the video renderer
room
.
initVideoRenderer
(
findViewById
<
SurfaceViewRenderer
>
(
R
.
id
.
renderer
)
)
connectToRoom
(
)
}
private
fun
connectToRoom
(
)
{
lifecycleScope
.
launch
{
// Setup event handling.
launch
{
room
.
events
.
collect
{
event
->
when
(
event
)
{
is
RoomEvent
.
TrackSubscribed
->
onTrackSubscribed
(
event
)
else
->
{
}
}
}
}
// Connect to server.
room
.
connect
(
wsURL
,
token
,
)
// Publish audio/video to the room
val
localParticipant
=
room
.
localParticipant
localParticipant
.
setMicrophoneEnabled
(
true
)
localParticipant
.
setCameraEnabled
(
true
)
}
}
private
fun
onTrackSubscribed
(
event
:
RoomEvent
.
TrackSubscribed
)
{
val
track
=
event
.
track
if
(
track
is
VideoTrack
)
{
attachVideo
(
track
)
}
}
private
fun
attachVideo
(
videoTrack
:
VideoTrack
)
{
videoTrack
.
addRenderer
(
findViewById
<
SurfaceViewRenderer
>
(
R
.
id
.
renderer
)
)
findViewById
<
View
>
(
R
.
id
.
progress
)
.
visibility
=
View
.
GONE
}
}
(For more details, you can reference
the complete sample app
.)
Next steps
The following resources are useful for getting started with LiveKit on Android.
Generating tokens
Guide to generating authentication tokens for your users.
Realtime media
Complete documentation for live video and audio tracks.
Realtime data
Send and receive realtime data between clients.
Android SDK
LiveKit Android SDK on GitHub.
Android components
LiveKit Android components on GitHub.
Android SDK reference
LiveKit Android SDK reference docs.
Android components reference
LiveKit Android components reference docs.
On this page
Voice AI quickstart
Getting started guide
Install LiveKit SDK
Permissions
Connect to LiveKit
Next steps


Content from https://docs.livekit.io/home/quickstarts/flutter:

On this page
Voice AI quickstart
Getting started guide
Install LiveKit SDK
Permissions and entitlements
Connect to LiveKit
Next steps
Copy page
See more page options
Voice AI quickstart
To build your first voice AI app for Flutter, use the following quickstart and the starter app. Otherwise follow the getting started guide below.
Voice AI quickstart
Create a voice AI agent in less than 10 minutes.
Flutter Voice Agent
A cross-platform voice AI assistant app built with Flutter.
GitHub
livekit-examples/agent-starter-flutter
Getting started guide
This guide covers the basic setup for a new Flutter app for iOS, Android, or web using LiveKit.
Install LiveKit SDK
flutter pub
add
livekit_client
Permissions and entitlements
You'll need to request camera and/or microphone permissions (depending on your use case). This must be done within your platform-specific code:
Android
Web
Windows
iOS
macOS
Permissions are configured in
AppManifest.xml
. In addition to camera and microphone, you may need to add networking and bluetooth permissions.
<
uses-feature
android:
name
=
"
android.hardware.camera
"
/>
<
uses-feature
android:
name
=
"
android.hardware.camera.autofocus
"
/>
<
uses-permission
android:
name
=
"
android.permission.CAMERA
"
/>
<
uses-permission
android:
name
=
"
android.permission.RECORD_AUDIO
"
/>
<
uses-permission
android:
name
=
"
android.permission.ACCESS_NETWORK_STATE
"
/>
<
uses-permission
android:
name
=
"
android.permission.CHANGE_NETWORK_STATE
"
/>
<
uses-permission
android:
name
=
"
android.permission.MODIFY_AUDIO_SETTINGS
"
/>
<
uses-permission
android:
name
=
"
android.permission.BLUETOOTH
"
android:
maxSdkVersion
=
"
30
"
/>
<
uses-permission
android:
name
=
"
android.permission.BLUETOOTH_ADMIN
"
android:
maxSdkVersion
=
"
30
"
/>
Connect to LiveKit
Add the following code to connect and publish audio/video to a room:
final
roomOptions
=
RoomOptions
(
adaptiveStream
:
true
,
dynacast
:
true
,
// ... your room options
)
final
room
=
Room
(
)
;
await
room
.
connect
(
url
,
token
,
roomOptions
:
roomOptions
)
;
try
{
// video will fail when running in ios simulator
await
room
.
localParticipant
.
setCameraEnabled
(
true
)
;
}
catch
(
error
)
{
print
(
'Could not publish video, error:
$
error
'
)
;
}
await
room
.
localParticipant
.
setMicrophoneEnabled
(
true
)
;
Next steps
The following resources are useful for getting started with LiveKit on Android.
Generating tokens
Guide to generating authentication tokens for your users.
Realtime media
Complete documentation for live video and audio tracks.
Realtime data
Send and receive realtime data between clients.
Flutter SDK
LiveKit Flutter SDK on GitHub.
Flutter components
LiveKit Flutter components on GitHub.
Flutter SDK reference
LiveKit Flutter SDK reference docs.
On this page
Voice AI quickstart
Getting started guide
Install LiveKit SDK
Permissions and entitlements
Connect to LiveKit
Next steps


Content from https://docs.livekit.io/home/quickstarts/react-native:

On this page
Voice AI quickstart
Getting started guide
Install LiveKit SDK
Integrate into your project
Connect to a room, publish video & audio
Create a backend server to generate tokens
Next steps
Copy page
See more page options
Note
If you're planning to integrate LiveKit into an Expo app, see the
quickstart guide for Expo instead
.
Voice AI quickstart
To build your first voice AI app for React Native, use the following quickstart and the starter app. Otherwise follow the getting started guide below.
Voice AI quickstart
Create a voice AI agent in less than 10 minutes.
React Native Voice Agent
A native voice AI assistant app built with React Native and Expo.
GitHub
livekit-examples/agent-starter-react-native
Getting started guide
The following guide walks you through the steps to build a video-conferencing application using React Native. It uses the
LiveKit React Native SDK
to render the UI and communicate with LiveKit servers via WebRTC. By the end, you will have a basic video-conferencing application you can run with multiple participants.
Install LiveKit SDK
Install the LiveKit SDK:
npm
install
@livekit/react-native @livekit/react-native-webrtc livekit-client
Integrate into your project
Android
Swift
This library depends on @livekit/react-native-webrtc, which has additional installation instructions for
Android
.
Once the @livekit/react-native-webrtc dependency is installed, one last step is required. In your MainApplication.java file:
import
com
.
livekit
.
reactnative
.
LiveKitReactNative
;
import
com
.
livekit
.
reactnative
.
audio
.
AudioType
;
public
class
MainApplication
extends
Application
implements
ReactApplication
{
@Override
public
void
onCreate
(
)
{
// Place this above any other RN related initialization
// When the AudioType is omitted, it'll default to CommunicationAudioType.
// Use AudioType.MediaAudioType if user is only consuming audio, and not publishing
LiveKitReactNative
.
setup
(
this
,
new
AudioType
.
CommunicationAudioType
(
)
)
;
//...
}
}
If you are using Expo, LiveKit is available on Expo through development builds.
See the instructions found here
.
Finally, in your index.js file, setup the LiveKit SDK by calling
registerGlobals()
. This sets up the required WebRTC libraries for use in Javascript, and is needed for LiveKit to work.
import
{
registerGlobals
}
from
'@livekit/react-native'
;
// ...
registerGlobals
(
)
;
Connect to a room, publish video & audio
import
*
as
React
from
'react'
;
import
{
StyleSheet
,
View
,
FlatList
,
ListRenderItem
,
}
from
'react-native'
;
import
{
useEffect
}
from
'react'
;
import
{
AudioSession
,
LiveKitRoom
,
useTracks
,
TrackReferenceOrPlaceholder
,
VideoTrack
,
isTrackReference
,
registerGlobals
,
}
from
'@livekit/react-native'
;
import
{
Track
}
from
'livekit-client'
;
// !! Note !!
// This sample hardcodes a token which expires in 2 hours.
const
wsURL
=
"<your LiveKit server URL>"
const
token
=
"<generate a token>"
export
default
function
App
(
)
{
// Start the audio session first.
useEffect
(
(
)
=>
{
let
start
=
async
(
)
=>
{
await
AudioSession
.
startAudioSession
(
)
;
}
;
start
(
)
;
return
(
)
=>
{
AudioSession
.
stopAudioSession
(
)
;
}
;
}
,
[
]
)
;
return
(
<
LiveKitRoom
serverUrl
=
{
wsURL
}
token
=
{
token
}
connect
=
{
true
}
options
=
{
{
// Use screen pixel density to handle screens with differing densities.
adaptiveStream
:
{
pixelDensity
:
'screen'
}
,
}
}
audio
=
{
true
}
video
=
{
true
}
>
<
RoomView
/>
</
LiveKitRoom
>
)
;
}
;
const
RoomView
=
(
)
=>
{
// Get all camera tracks.
const
tracks
=
useTracks
(
[
Track
.
Source
.
Camera
]
)
;
const
renderTrack
:
ListRenderItem
<
TrackReferenceOrPlaceholder
>
= ({item}) =>
{
// Render using the VideoTrack component.
if
(
isTrackReference
(
item
)
)
{
return
(
<
VideoTrack
trackRef
=
{
item
}
style
=
{
styles
.
participantView
}
/>
)
}
else
{
return
(
<
View
style
=
{
styles
.
participantView
}
/>
)
}
}
;
return (
<
View
style
=
{
styles
.
container
}
>
<
FlatList
data
=
{
tracks
}
renderItem
=
{
renderTrack
}
/>
</
View
>
);
};
const styles = StyleSheet.create(
{
container
:
{
flex
:
1
,
alignItems
:
'stretch'
,
justifyContent
:
'center'
,
}
,
participantView
:
{
height
:
300
,
}
,
}
);
Create a backend server to generate tokens
Set up a server to generate tokens for your app at runtime by following this guide:
Generating Tokens
.
Next steps
The following resources are useful for getting started with LiveKit on React Native.
Generating tokens
Guide to generating authentication tokens for your users.
Realtime media
Complete documentation for live video and audio tracks.
Realtime data
Send and receive realtime data between clients.
React Native SDK
LiveKit React Native SDK on GitHub.
React Native SDK reference
LiveKit React Native SDK reference docs.
On this page
Voice AI quickstart
Getting started guide
Install LiveKit SDK
Integrate into your project
Connect to a room, publish video & audio
Create a backend server to generate tokens
Next steps


Content from https://docs.livekit.io/home/quickstarts/expo:

On this page
Voice AI quickstart
Getting started guide
Install LiveKit SDK
Configure Expo
Connect to a room, publish video & audio
Create a backend server to generate tokens
Next steps
Copy page
See more page options
Voice AI quickstart
To build your first voice AI app for Expo, use the following quickstart and the starter app. Otherwise follow the getting started guide below.
Voice AI quickstart
Create a voice AI agent in less than 10 minutes.
React Native Voice Agent
A native voice AI assistant app built with React Native and Expo.
GitHub
livekit-examples/agent-starter-react-native
Getting started guide
The following guide walks you through the steps to build a video-conferencing application using Expo. It uses the
LiveKit React Native SDK
to render the UI and communicate with LiveKit servers via WebRTC. By the end, you will have a basic video-conferencing application you can run with multiple participants.
Install LiveKit SDK
LiveKit provides a
React Native SDK
and corresponding Expo config plugin. Install the packages and dependencies with:
npm
install
@livekit/react-native @livekit/react-native-expo-plugin @livekit/react-native-webrtc @config-plugins/react-native-webrtc livekit-client
Note
The LiveKit SDK is not compatible with the Expo Go app due to the native code required. Using
expo-dev-client
and
building locally
will allow you to create development builds compatible with LiveKit.
Configure Expo
In your root folder, add the Expo plugins to the
app.json
file:
{
"expo"
:
{
"plugins"
:
[
"@livekit/react-native-expo-plugin"
,
"@config-plugins/react-native-webrtc"
]
}
}
Finally, in your App.js file, setup the LiveKit SDK by calling
registerGlobals()
. This sets up the required WebRTC libraries for use in Javascript, and is needed for LiveKit to work.
import
{
registerGlobals
}
from
'@livekit/react-native'
;
registerGlobals
(
)
;
Connect to a room, publish video & audio
import
*
as
React
from
'react'
;
import
{
StyleSheet
,
View
,
FlatList
,
ListRenderItem
,
}
from
'react-native'
;
import
{
useEffect
}
from
'react'
;
import
{
AudioSession
,
LiveKitRoom
,
useTracks
,
TrackReferenceOrPlaceholder
,
VideoTrack
,
isTrackReference
,
registerGlobals
,
}
from
'@livekit/react-native'
;
import
{
Track
}
from
'livekit-client'
;
registerGlobals
(
)
;
// !! Note !!
// This sample hardcodes a token which expires in 2 hours.
const
wsURL
=
"<your LiveKit server URL>"
const
token
=
"<generate a token>"
export
default
function
App
(
)
{
// Start the audio session first.
useEffect
(
(
)
=>
{
let
start
=
async
(
)
=>
{
await
AudioSession
.
startAudioSession
(
)
;
}
;
start
(
)
;
return
(
)
=>
{
AudioSession
.
stopAudioSession
(
)
;
}
;
}
,
[
]
)
;
return
(
<
LiveKitRoom
serverUrl
=
{
wsURL
}
token
=
{
token
}
connect
=
{
true
}
options
=
{
{
// Use screen pixel density to handle screens with differing densities.
adaptiveStream
:
{
pixelDensity
:
'screen'
}
,
}
}
audio
=
{
true
}
video
=
{
true
}
>
<
RoomView
/>
</
LiveKitRoom
>
)
;
}
;
const
RoomView
=
(
)
=>
{
// Get all camera tracks.
const
tracks
=
useTracks
(
[
Track
.
Source
.
Camera
]
)
;
const
renderTrack
:
ListRenderItem
<
TrackReferenceOrPlaceholder
>
= ({item}) =>
{
// Render using the VideoTrack component.
if
(
isTrackReference
(
item
)
)
{
return
(
<
VideoTrack
trackRef
=
{
item
}
style
=
{
styles
.
participantView
}
/>
)
}
else
{
return
(
<
View
style
=
{
styles
.
participantView
}
/>
)
}
}
;
return (
<
View
style
=
{
styles
.
container
}
>
<
FlatList
data
=
{
tracks
}
renderItem
=
{
renderTrack
}
/>
</
View
>
);
};
const styles = StyleSheet.create(
{
container
:
{
flex
:
1
,
alignItems
:
'stretch'
,
justifyContent
:
'center'
,
}
,
participantView
:
{
height
:
300
,
}
,
}
);
See the
quickstart example repo
for a fully configured app using Expo.
Create a backend server to generate tokens
Set up a server to generate tokens for your app at runtime by following this guide:
Generating Tokens
.
Next steps
The following resources are useful for getting started with LiveKit on React Native and Expo.
Generating tokens
Guide to generating authentication tokens for your users.
Realtime media
Complete documentation for live video and audio tracks.
Realtime data
Send and receive realtime data between clients.
React Native SDK
LiveKit React Native SDK on GitHub.
React Native SDK reference
LiveKit React Native SDK reference docs.
On this page
Voice AI quickstart
Getting started guide
Install LiveKit SDK
Configure Expo
Connect to a room, publish video & audio
Create a backend server to generate tokens
Next steps


Content from https://docs.livekit.io/home/server/generating-tokens:

On this page
1. Install LiveKit Server SDK
2. Keys and Configuration
3. Make an endpoint that returns a token
4. Create a frontend app to connect
Copy page
See more page options
In order for frontend apps to connect to LiveKit rooms, they need a token generated by your backend
server. In this guide, we'll walk through how to set up a server to generate tokens for your frontend.
1. Install LiveKit Server SDK
Go
Node.js
Ruby
Python
Rust
PHP
go get github.com/livekit/server-sdk-go/v2
2. Keys and Configuration
Create a new file at
development.env
and with your API Key and Secret:
export
LIVEKIT_API_KEY
=
<
your API Key
>
export
LIVEKIT_API_SECRET
=
<
your API Secret
>
Reveal API Key and Secret
3. Make an endpoint that returns a token
Create a server:
Go
Node.js
Ruby
Python
Rust
PHP
// server.go
import
(
"net/http"
"log"
"time"
"os"
"github.com/livekit/protocol/auth"
)
func
getJoinToken
(
room
,
identity
string
)
string
{
at
:=
auth
.
NewAccessToken
(
os
.
Getenv
(
"LIVEKIT_API_KEY"
)
,
os
.
Getenv
(
"LIVEKIT_API_SECRET"
)
)
grant
:=
&
auth
.
VideoGrant
{
RoomJoin
:
true
,
Room
:
room
,
}
at
.
AddGrant
(
grant
)
.
SetIdentity
(
identity
)
.
SetValidFor
(
time
.
Hour
)
token
,
_
:=
at
.
ToJWT
(
)
return
token
}
func
main
(
)
{
http
.
HandleFunc
(
"/getToken"
,
func
(
w http
.
ResponseWriter
,
r
*
http
.
Request
)
{
w
.
Write
(
[
]
byte
(
getJoinToken
(
"my-room"
,
"identity"
)
)
)
}
)
log
.
Fatal
(
http
.
ListenAndServe
(
":8080"
,
nil
)
)
}
Load the environment variables and run the server:
Go
Node.js
Ruby
Python
Rust
PHP
$
source
development.env
$ go run server.go
Note
See the
Authentication
page for more information on how to generate tokens with custom permissions.
4. Create a frontend app to connect
Create a frontend app that fetches a token from the server we just made, then uses it to connect to a LiveKit room:
iOS
Android
Flutter
React Native
React
Unity (web)
JavaScript
On this page
1. Install LiveKit Server SDK
2. Keys and Configuration
3. Make an endpoint that returns a token
4. Create a frontend app to connect


Content from https://docs.livekit.io/home/server/managing-rooms:

On this page
Initialize RoomServiceClient
Create a room
List rooms
Delete a room
Copy page
See more page options
Initialize RoomServiceClient
Room management is done with a RoomServiceClient, created like so:
Go
Python
Node.js
import
(
lksdk
"github.com/livekit/server-sdk-go"
livekit
"github.com/livekit/protocol/livekit"
)
// ...
host
:=
"https://my.livekit.host"
roomClient
:=
lksdk
.
NewRoomServiceClient
(
host
,
"api-key"
,
"secret-key"
)
Create a room
Go
Python
Node.js
LiveKit CLI
room
,
_
:=
roomClient
.
CreateRoom
(
context
.
Background
(
)
,
&
livekit
.
CreateRoomRequest
{
Name
:
"myroom"
,
EmptyTimeout
:
10
*
60
,
// 10 minutes
MaxParticipants
:
20
,
}
)
List rooms
Go
Python
Node.js
LiveKit CLI
rooms
,
_
:=
roomClient
.
ListRooms
(
context
.
Background
(
)
,
&
livekit
.
ListRoomsRequest
{
}
)
Delete a room
Deleting a room causes all Participants to be disconnected.
Go
Python
Node.js
LiveKit CLI
_
,
_
=
roomClient
.
DeleteRoom
(
context
.
Background
(
)
,
&
livekit
.
DeleteRoomRequest
{
Room
:
"myroom"
,
}
)
On this page
Initialize RoomServiceClient
Create a room
List rooms
Delete a room


Content from https://docs.livekit.io/home/server/managing-participants:

On this page
Initialize RoomServiceClient
List Participants
Get details on a Participant
Updating permissions
Updating metadata
Remove a Participant
Mute/unmute a Participant's Track
Copy page
See more page options
Initialize RoomServiceClient
Participant management is done with a RoomServiceClient, created like so:
Go
Python
Node.js
import
(
lksdk
"github.com/livekit/server-sdk-go"
livekit
"github.com/livekit/protocol/livekit"
)
// ...
host
:=
"https://my.livekit.host"
roomClient
:=
lksdk
.
NewRoomServiceClient
(
host
,
"api-key"
,
"secret-key"
)
List Participants
Go
Python
Node.js
LiveKit CLI
res
,
err
:=
roomClient
.
ListParticipants
(
context
.
Background
(
)
,
&
livekit
.
ListParticipantsRequest
{
Room
:
roomName
,
}
)
Get details on a Participant
Go
Python
Node.js
LiveKit CLI
res
,
err
:=
roomClient
.
GetParticipant
(
context
.
Background
(
)
,
&
livekit
.
RoomParticipantIdentity
{
Room
:
roomName
,
Identity
:
identity
,
}
)
Updating permissions
You can modify a participant's permissions on-the-fly using
UpdateParticipant
.
When there's a change in permissions, connected clients will be notified through the
ParticipantPermissionChanged
event.
This comes in handy, for instance, when transitioning an audience member to a speaker role within a room.
Note that if you revoke the
CanPublish
permission from a participant, all tracks they've published will be automatically unpublished.
Go
Python
Node.js
LiveKit CLI
// Promotes an audience member to a speaker
res
,
err
:=
c
.
UpdateParticipant
(
context
.
Background
(
)
,
&
livekit
.
UpdateParticipantRequest
{
Room
:
roomName
,
Identity
:
identity
,
Permission
:
&
livekit
.
ParticipantPermission
{
CanSubscribe
:
true
,
CanPublish
:
true
,
CanPublishData
:
true
,
}
,
}
)
// ...and later move them back to audience
res
,
err
:=
c
.
UpdateParticipant
(
context
.
Background
(
)
,
&
livekit
.
UpdateParticipantRequest
{
Room
:
roomName
,
Identity
:
identity
,
Permission
:
&
livekit
.
ParticipantPermission
{
CanSubscribe
:
true
,
CanPublish
:
false
,
CanPublishData
:
true
,
}
,
}
)
Updating metadata
You can modify a Participant's metadata whenever necessary. Once changed, connected
clients will receive a
ParticipantMetadataChanged
event.
Go
Python
Node.js
LiveKit CLI
data
,
err
:=
json
.
Marshal
(
values
)
_
,
err
=
c
.
UpdateParticipant
(
context
.
Background
(
)
,
&
livekit
.
UpdateParticipantRequest
{
Room
:
roomName
,
Identity
:
identity
,
Metadata
:
string
(
data
)
,
}
)
Remove a Participant
RemoteParticipant
will forcibly disconnect the participant from the room. However, this action doesn't invalidate the participant's token.
To prevent the participant from rejoining the same room, consider the following measures:
Generate access tokens with a short TTL (Time-To-Live).
Refrain from providing a new token to the same participant via your application's backend.
Go
Python
Node.js
LiveKit CLI
res
,
err
:=
roomClient
.
RemoveParticipant
(
context
.
Background
(
)
,
&
livekit
.
RoomParticipantIdentity
{
Room
:
roomName
,
Identity
:
identity
,
}
)
Mute/unmute a Participant's Track
To mute a particular Track from a Participant, first get the TrackSid from
GetParticipant
(above), then call
MutePublishedTrack
:
Go
Python
Node.js
LiveKit CLI
res
,
err
:=
roomClient
.
MutePublishedTrack
(
context
.
Background
(
)
,
&
livekit
.
MuteRoomTrackRequest
{
Room
:
roomName
,
Identity
:
identity
,
TrackSid
:
"track_sid"
,
Muted
:
true
,
}
)
You may also unmute the track by setting
muted
to
false
.
Note
Being remotely unmuted can catch users by surprise, so it's disabled by default.
To allow remote unmute, select the
Admins can remotely unmute tracks
option in your
project settings
.
If you're self-hosting, configure
room.enable_remote_unmute: true
in your config YAML.
On this page
Initialize RoomServiceClient
List Participants
Get details on a Participant
Updating permissions
Updating metadata
Remove a Participant
Mute/unmute a Participant's Track


Content from https://docs.livekit.io/home/server/webhooks:

On this page
Overview
Receiving webhooks
Delivery and retries
Events
Room Started
Room Finished
Participant Joined
Participant Left
Track Published
Track Unpublished
Egress Started
Egress Updated
Egress Ended
Ingress Started
Ingress Ended
Copy page
See more page options
Overview
LiveKit can be configured to notify your server when room events take place. This can be helpful for your backend to know when a room has finished, or when a participant leaves.
With Cloud, webhooks can be configured in the Settings section of your project's dashboard.
When self-hosting, webhooks can be enabled by setting the
webhook
section in your config.
For Egress, extra webhooks can also be
configured inside Egress requests
.
webhook
:
# The API key to use in order to sign the message
# This must match one of the keys LiveKit is configured with
api_key
:
'api-key-to-sign-with'
urls
:
-
'https://yourhost'
Receiving webhooks
Webhook requests are HTTP POST requests sent to URLs that you specify in your config or Cloud dashboard. A
WebhookEvent
is encoded as JSON and sent in the body of the request.
The
Content-Type
header of the request is set to
application/webhook+json
. Please ensure your webserver is configured to receive payloads with this content type.
In order to ensure webhook requests are coming from LiveKit, these requests have an
Authorization
header containing a signed JWT token. The token includes a sha256 hash of the payload.
LiveKit's server SDKs provide webhook receiver libraries which should help with validation and decoding of the payload.
Node.js
Go
Java
import
{
WebhookReceiver
}
from
'livekit-server-sdk'
;
const
receiver
=
new
WebhookReceiver
(
'apikey'
,
'apisecret'
)
;
// In order to use the validator, WebhookReceiver must have access to the raw
// POSTed string (instead of a parsed JSON object). If you are using express
// middleware, ensure that `express.raw` is used for the webhook endpoint
// app.use(express.raw({type: 'application/webhook+json'}));
app
.
post
(
'/webhook-endpoint'
,
async
(
req
,
res
)
=>
{
// Event is a WebhookEvent object
const
event
=
await
receiver
.
receive
(
req
.
body
,
req
.
get
(
'Authorization'
)
)
;
}
)
;
Delivery and retries
Webhooks are HTTP requests initiated by LiveKit and sent to your backend. Due to the protocol's push-based nature, there are no guarantees around delivery.
LiveKit aims to mitigate transient failures by retrying a webhook request multiple times. Each message will undergo several delivery attempts before being abandoned. If multiple events are queued for delivery, LiveKit will properly sequence them; only delivering newer events after older ones have been delivered or abandoned.
Events
In addition to the fields below, all webhook events will include the following fields:
id
- a UUID identifying the event
createdAt
- UNIX timestamp in seconds
Room Started
interface
WebhookEvent
{
event
:
'room_started'
;
room
:
Room
;
}
Room Finished
interface
WebhookEvent
{
event
:
'room_finished'
;
room
:
Room
;
}
Participant Joined
interface
WebhookEvent
{
event
:
'participant_joined'
;
room
:
Room
;
participant
:
ParticipantInfo
;
}
Participant Left
interface
WebhookEvent
{
event
:
'participant_left'
;
room
:
Room
;
participant
:
ParticipantInfo
;
}
Track Published
In the Room and Participant objects, only sid, identity, and name are sent.
interface
WebhookEvent
{
event
:
'track_published'
;
room
:
Room
;
participant
:
ParticipantInfo
;
track
:
TrackInfo
;
}
Track Unpublished
In the Room and Participant objects, only sid, identity, and name are sent.
interface
WebhookEvent
{
event
:
'track_unpublished'
;
room
:
Room
;
participant
:
ParticipantInfo
;
track
:
TrackInfo
;
}
Egress Started
interface
WebhookEvent
{
event
:
'egress_started'
;
egressInfo
:
EgressInfo
;
}
Egress Updated
interface
WebhookEvent
{
event
:
'egress_updated'
;
egressInfo
:
EgressInfo
;
}
Egress Ended
interface
WebhookEvent
{
event
:
'egress_ended'
;
egressInfo
:
EgressInfo
;
}
Ingress Started
interface
WebhookEvent
{
event
:
'ingress_started'
;
ingressInfo
:
IngressInfo
;
}
Ingress Ended
interface
WebhookEvent
{
event
:
'ingress_ended'
;
ingressInfo
:
IngressInfo
;
}
On this page
Overview
Receiving webhooks
Delivery and retries
Events
Room Started
Room Finished
Participant Joined
Participant Left
Track Published
Track Unpublished
Egress Started
Egress Updated
Egress Ended
Ingress Started
Ingress Ended


Content from https://docs.livekit.io/home/egress/composite-recording:

On this page
Composite recording
RoomComposite Egress
Default layouts
Output options
Audio-only composite
Web Egress
Examples
Copy page
See more page options
Composite recording
Composite recordings use a web-based recorder to capture a composited view of a room, including all participants, interactions, and any customized UI elements from the application.
We provide two options for composite recording:
RoomComposite
: A composite recording that is tied to a room's lifecycle. When all of the participants leave the room, the recording would stop automatically.
Web
: A standalone composite recording can be started and stopped independently of a room’s lifecycle. Web Egress can be used to record any web-based content, even if it’s not part of a LiveKit room.
RoomComposite Egress
One common requirement when recording a room is to capture all of the participants and interactions that take place.
This can be challenging in a multi-user application, where different users may be joining, leaving, or turning their cameras on and off.
It may also be desirable for the recording to look as close to the actual application experience as possible, capturing the richness and interactivity of your application.
A
RoomComposite
Egress uses a web app to create the composited view, rendering the output with an instance of headless Chromium.
In most cases, your existing LiveKit application can be used as a compositing template with few modifications.
Default layouts
We provide a few default compositing layouts that works out of the box. They'll be used
by default if a custom template URL is not passed in. These templates are deployed
alongside and served by the Egress service (
source
).
While it's a great starting point, you can easily
create your own layout
using standard web technologies that you are already familiar with.
Layout
Preview
grid
speaker
single-speaker
Additionally, you can use a
-light
suffix to change background color to white. i.e.
grid-light
.
Output options
Composite recordings can output to a wide variety of formats and destinations.
The options are described in detail in
Output options
.
Audio-only composite
If your application is audio-only, you can export a mixed audio file containing audio from all participants in the room.
To start an audio-only composite, pass
audio_only=true
when starting an Egress.
Web Egress
Web egress allows you to record or stream any website. Similar to room composite egress, it uses headless Chromium to render output.
Unlike room composite egress, you can supply any url, and the lifecycle of web egress is not attached to a LiveKit room.
Examples
For examples on using composite recordings, see
Egress examples
.
On this page
Composite recording
RoomComposite Egress
Default layouts
Output options
Audio-only composite
Web Egress
Examples


Content from https://docs.livekit.io/home/egress/participant:

On this page
Participant Egress
TrackComposite Egress
Examples
Copy page
See more page options
Some use cases require participants to be recorded individually instead of compositing them. LiveKit offers two options for recording participants individually. Both options support a wide range of
output options
.
See the
Egress examples
page for example usage.
Participant Egress
Participant Egress allows you to record a participant's audio and video tracks by providing the participant's identity. Participant Egress is designed to simplify the workflow of recording participants in a realtime session, and handles the changes in track state, such as when a track is muted.
When a Participant Egress is requested, the Egress service joins the room and waits for the participant to join and publish tracks. Recording begins as soon as either audio or video tracks are published. The service automatically handles muted or unpublished tracks and stops recording when the participant leaves the room.
You can also record a participant’s screen share along with the screen share's audio. To enable this, pass
screen_share=true
when starting the Egress. The Egress service will identify tracks based on their
source
setting.
TrackComposite Egress
TrackComposite combines an audio and video track for output, as the name suggests.
It’s a more advanced version of Participant Egress, allowing you to specify which tracks to record — useful when precise control over track IDs is needed.
One key difference with TrackComposite is that tracks must be published before starting the Egress. As a result, there may be a slight delay between when the track is published and when recording begins.
Examples
For examples on using Participant or Track Composite Egress, please reference
Egress examples
.
On this page
Participant Egress
TrackComposite Egress
Examples


Content from https://docs.livekit.io/home/egress/track:

On this page
Overview
Example
Stream audio to WebSocket
Copy page
See more page options
Overview
TrackEgress is the simplest way to export individual tracks to cloud storage or a server via WebSocket.
Tracks are exported as is, without transcoding. The following containers will be used depending on track codec:
H.264: MP4
VP8: WebM
Opus: Ogg
Note: Track Egress only exports one track, either video or audio. If you want to export video and audio together, use
Track Composite Egress
.
Example
See an example of exporting to Azure Blob Storage on the
example page
.
Stream audio to WebSocket
You can add custom stream processing by starting a TrackEgress to your WebSocket server. This will give you a realtime streaming export of your audio tracks. (WebSocket streaming is only available for audio tracks).
The tracks will be exported as raw PCM data. This format is compatible with most transcription services.
Format:
pcm_s16le
Content-Type:
audio/x-raw
Sample rate: matches incoming, typically 48kHz
When a
TrackEgressRequest
is started with a WebSocket URL, we'll initiate a WebSocket session to the designated URL. We recommend using query parameters in the URL in order to help you identify the track.
For example:
wss://your-server.com/egress?trackID=<trackID>&participant=<participantIdentity>
We'll send a combination of binary and text frames. Binary frames would contain audio data. Text frames will contain end user events on the tracks. For example: if the
track was muted, you will receive the following:
{
"muted"
:
true
}
And when unmuted:
{
"muted"
:
false
}
The WebSocket connection will terminate when the track is unpublished (or if the participant leaves the room).
JavaScript
Go
Ruby
LiveKit CLI
const
info
=
await
egressClient
.
startTrackEgress
(
'my-room'
,
'wss://my-websocket-server.com'
,
trackID
,
)
;
const
egressID
=
info
.
egressId
;
On this page
Overview
Example
Stream audio to WebSocket


Content from https://docs.livekit.io/home/egress/outputs:

On this page
Supported Outputs
Composite and Participant Egress Outputs
RTMP/SRT Streaming
File/Segment outputs
Image output
Cloud storage configurations
S3
Google Cloud Storage
Azure
Copy page
See more page options
Supported Outputs
Egress Type
Transcoded
Pass-through (mp4, webm, ogg)
HLS Segments
RTMP stream
SRT stream
WebSocket stream
Room Composite
✅
✅
✅
✅
Web
✅
✅
✅
✅
Participant
✅
✅
✅
✅
Track Composite
✅
✅
✅
✅
Track
✅
✅ (audio-only)
Note
A very long-running Egress may hit our
Egress time limits
.
Composite and Participant Egress Outputs
Since Composite and Participant Egress are transcoded, they can be output to a wide range of formats and destinations.
Egress is optimized to transcode once while sending output to multiple destinations.
For example, from the same Egress you may simultaneously:
Stream to one or more RTMP endpoints.
Record as HLS.
Record as MP4.
Generate thumbnails.
When creating a new Egress, set one or more of the following configuration fields:
Field
Description
file_outputs
Record to a MP4 file.
stream_outputs
Stream to RTMP or SRT server.
segment_outputs
Record as HLS segments.
image_outputs
Generate thumbnails.
Note
While each output type is a list (
*_outputs
), Egress supports only a single item per type. i.e. It's not possible to output to two different files, but it is possible to output to both a
file
and a HLS
segment
.
LiveKit CLI
JavaScript
Go
Ruby
Python
Java
{
...
// source details
"file_outputs"
:
[
{
"filepath"
:
"my-test-file.mp4"
,
"s3"
:
{
...
}
,
"gcp"
:
{
...
}
,
"azure"
:
{
...
}
,
"aliOSS"
:
{
...
}
}
]
,
"stream_outputs"
:
[
{
"protocol"
:
"rtmp"
,
"urls"
:
[
"rtmp://my-rtmp-endpoint/path/stream-key"
]
}
]
,
"segment_outputs"
:
[
{
"filename_prefix"
:
"my-output"
,
"playlist_name"
:
"my-output.m3u8"
,
// when provided, we'll generate a playlist containing only the last few segments
"live_playlist_name"
:
"my-output-live.m3u8"
,
"segment_duration"
:
2
,
"s3"
:
{
...
}
,
"gcp"
:
{
...
}
,
"azure"
:
{
...
}
,
"aliOSS"
:
{
...
}
}
]
,
"image_outputs"
:
[
{
"capture_interval"
:
5
,
"filename_prefix"
:
"my-image"
,
"filename_suffix"
:
"IMAGE_SUFFIX_INDEX"
,
"s3"
:
{
...
}
,
"gcp"
:
{
...
}
,
"azure"
:
{
...
}
,
"aliOSS"
:
{
...
}
}
]
}
RTMP/SRT Streaming
Choosing RTMP ingest endpoints
RTMP streams do not perform well over long distances. Some stream providers include a region or location as part of your
stream url, while others might use region-based routing.
When self-hosting, choose stream endpoints that are close to where your Egress servers are deployed.
With Cloud Egress, we will route your Egress request to a server closest to your RTMP endpoints.
Adding streams to non-streaming egress
Streams can be added and removed on the fly using the
UpdateStream API
.
To use the UpdateStream API, your initial request must include a
StreamOutput
. If the stream will start later, include a
StreamOutput
in the initial request with the correct
protocol
and an empty
urls
array.
Integration with Mux
Mux is LiveKit's preferred partner for HLS streaming. To start a
Mux
stream, all you need is your stream key. You can then use
mux://<stream_key>
as
a url in your
StreamOutput
.
File/Segment outputs
Filename templating
When outputing to files, the
filepath
and
filename_prefix
fields support templated variables. The below templates can be used in request filename/filepath parameters:
Egress Type
{room_id}
{room_name}
{time}
{publisher_identity}
{track_id}
{track_type}
{track_source}
Room Composite
✔️
✔️
✔️
Web
✔️
Participant
✔️
✔️
✔️
✔️
Track Composite
✔️
✔️
✔️
✔️
Track
✔️
✔️
✔️
✔️
✔️
✔️
✔️
If no filename is provided with a request, one will be generated in the form of
"{room_name}-{time}"
.
If your filename ends with a
/
, a file will be generated in that directory.
If your filename is missing an extension or includes the wrong extension, the correct one will be added.
Examples:
Request filename
Output filename
""
testroom-2022-10-04T011306.mp4
"livekit-recordings/"
livekit-recordings/testroom-2022-10-04T011306.mp4
"{room_name}/{time}"
testroom/2022-10-04T011306.mp4
"{room_id}-{publisher_identity}.mp4"
10719607-f7b0-4d82-afe1-06b77e91fe12-david.mp4
"{track_type}-{track_source}-{track_id}"
audio-microphone-TR_SKasdXCVgHsei.ogg
Image output
Image output allows you to create periodic snapshots from a recording or stream, useful for generating thumbnails or running moderation workflows in your application.
The configuration options are:
Field
Description
capture_interval
The interval in seconds between each snapshot.
filename_prefix
The prefix for each image file.
filename_suffix
The suffix for each image file. This can be a timestamp or an index.
width
and
height
The dimensions of the image. If not provided, the image is the same size as the video frame.
Cloud storage configurations
S3
Egress supports any S3-compatible storage provider, including the following:
MinIO
Oracle Cloud
CloudFlare R2
Digital Ocean
Akamai Linode
Backblaze
When using non-AWS storage, set
force_path_style
to
true
. This ensures the bucket name is used in the path, rather than as a subdomain.
Configuration fields:
Field
Description
access_key
The access key for your S3 account.
secret
The secret key for your S3 account.
region
The region where your S3 bucket is located (required when
endpoint
is not set).
bucket
The name of the bucket where the file will be stored.
endpoint
The endpoint for your S3-compatible storage provider (optional). Must start with
https://
.
metadata
Key/value pair to set as S3 metadata.
content_disposition
Content-Disposition header when the file is downloaded.
proxy
HTTP proxy to use when uploading files. {url: "", username: "", password: ""}.
Note
If the
endpoint
field is left empty, it uses AWS's regional endpoints. The
region
field is required when
endpoint
is not set.
Google Cloud Storage
For Egress to upload to Google Cloud Storage, you'll need to provide credentials in JSON.
This can be obtained by first creating a
service account
that has permissions to create storage objects (i.e.
Storage Object Creator
).
Then
create a key
for that account and export as a JSON file.
We'll refer to this file as
credentials.json
.
Configuration fields:
Field
Description
credentials
Service account credentials serialized in a JSON file named
credentials.json
.
bucket
The name of the bucket where the file will be stored.
proxy
HTTP proxy to use when uploading files. {url: "", username: "", password: ""}.
Azure
In order to upload to Azure Blob Storage, you'll need the account's shared access key.
Configuration fields:
Field
Description
account_name
The name of the Azure account.
account_key
The shared access key for the Azure account.
container_name
The name of the container where the file will be stored.
On this page
Supported Outputs
Composite and Participant Egress Outputs
RTMP/SRT Streaming
File/Segment outputs
Image output
Cloud storage configurations
S3
Google Cloud Storage
Azure


Content from https://docs.livekit.io/home/egress/autoegress:

On this page
Start recordings automatically
Examples
Automatically record all tracks to S3
Record each room to HLS on GCP
Copy page
See more page options
Start recordings automatically
Sometimes it's desirable to record every track published to the room, or to start recording the room as soon as it's created.
Autoegress is designed to simplify these workflows. When a room is created with
CreateRoom
, you can set the
egress
field to have it automatically record the room as a composite as well as each published track separately.
Examples
Automatically record all tracks to S3
curl
-X
POST
<
your-host
>
/twirp/livekit.RoomService/CreateRoom
\
-H
"Authorization: Bearer <token-with-roomCreate>"
\
-H
'Content-Type: application/json'
\
--data-binary @-
<<
EOF
{
"name": "my-room",
"egress": {
"tracks": {
"filepath": "bucket-path/{room_name}-{publisher_identity}-{time}"
"s3": {
"access_key": "",
"secret": "",
"bucket": "mybucket",
"region": "",
}
}
}
}
EOF
Record each room to HLS on GCP
curl
-X
POST
<
your-host
>
/twirp/livekit.RoomService/CreateRoom
\
-H
"Authorization: Bearer <token-with-roomCreate>"
\
-H
'Content-Type: application/json'
\
--data-binary @-
<<
EOF
{
"name": "my-room",
"egress": {
"room": {
"customBaseUrl": "https://your-template-url"
"segments": {
"filename_prefix": "path-in-bucket/myfile",
"segment_duration": 3,
"gcp": {
"credentials": "<json-encoded-credentials>",
"bucket": "mybucket"
}
}
}
}
}
EOF
On this page
Start recordings automatically
Examples
Automatically record all tracks to S3
Record each room to HLS on GCP


Content from https://docs.livekit.io/home/egress/custom-template:

On this page
Overview
Built-in LiveKit Recording View
Building a Custom Recording View
Building your Application
Deploying your Application
Testing Your Application
Using the Custom Recording View in Production
Recording Process
Copy page
See more page options
Overview
LiveKit
Room Composite Egress
enables recording of all participants' tracks in a room. This document explains its functionality and customization options.
Built-in LiveKit Recording View
The recording feature in LiveKit is built on a web-based architecture, utilizing a headless Chrome instance to render and capture output. The default view is built using LiveKit's
React Components
. There are a handful of configuration options available including:
layout
to control how the participants are arranged in the view. (You can set or change the layout using either
StartRoomCompositeEgress()
or
UpdateLayout()
.)
Encoding options
to control the quality of the audio and/or video captured
For more advanced customization, LiveKit supports configuring the URL of the web application that will generate the page to be recorded, allowing full customization of the recording view.
Building a Custom Recording View
While you can use any web framework, it's often easiest to start with the built-in React-based application and modify it to meet your requirements. The source code can be found in the
template-default
folder
of the
LiveKit egress repository
. The main files include:
Room.tsx
: the main component that renders the recording view
SpeakerLayout.tsx
,
SingleSpeakerLayout.tsx
: components used for the
speaker
and
single-speaker
layouts
App.tsx
,
index.tsx
: the main entry points for the application
App.css
,
index.css
: the CSS files for the application
Note
The built-in
Room.tsx
component uses the
template SDK
, for common tasks like:
Retrieving query string arguments (Example:
App.tsx
)
Starting a recording (Example:
Room.tsx
)
Ending a recording (Example:
EgressHelper.setRoom()
)
If you are not using
Room.tsx
as a starting point, be sure to leverage the template SDK to handle these and other common tasks.
Building your Application
Make a copy of the above files and modify tnem to meet your requirements.
Example: Move non-speaking participants to the right side of the Speaker view
By default the
Speaker
view shows the non-speaking participants on the left and the speaker on the right. Let's change this so the speaker is on the left and the non-speaking participants are on the right.
Copy the default components and CSS files into a new location
Modify
SpeakerLayout.tsx
to move the
FocusLayout
above
CarouselLayout
so it looks like this:
return
(
<
div
className
=
"
lk-focus-layout
"
>
<
FocusLayout
trackRef
=
{
mainTrack
as
TrackReference
}
/>
<
CarouselLayout
tracks
=
{
remainingTracks
}
>
<
ParticipantTile
/>
</
CarouselLayout
>
</
div
>
)
;
Modify
App.css
to fix the
grid-template-columns
value (reverse the values). It should look like this:
.lk-focus-layout
{
height
:
100
%
;
grid-template-columns
:
5
fr
1
fr
;
}
Deploying your Application
Once your app is ready for testing or deployment, you'll need to host it on a web server. We recommend using
Vercel
for its simplicity, but many other options are available.
Testing Your Application
We have created the
egress test-egress-template
subcommand in the
LiveKit CLI
to make it easier to test.
The
egress test-egress-template
subcommand:
Creates a room
Adds the desired number of virtual publishers who will publish simulated video streams
Opens a browser instance to your app URL with the correct parameters
Once you have your application deployed, you can use this command to test it out.
Usage
export
LIVEKIT_API_SECRET
=
SECRET
export
LIVEKIT_API_KEY
=
KEY
export
LIVEKIT_URL
=
YOUR_LIVEKIT_URL
lk egress test-template
\
--base-url YOUR_WEB_SERVER_URL
\
--room
ROOM_NAME
\
--layout
LAYOUT
\
--publishers
PUBLISHER_COUNT
This command launches a browser and opens:
YOUR_WEB_SERVER_URL?url=<LIVEKIT_INSTANCE WSS URL>&token=<RECORDER TOKEN>&layout=LAYOUT
Example
export
LIVEKIT_API_SECRET
=
SECRET
export
LIVEKIT_API_KEY
=
KEY
export
LIVEKIT_URL
=
YOUR_LIVEKIT_URL
lk egress test-template
\
--base-url http://localhost:3000/lk-recording-view
\
--room
my-room
\
--layout
grid
\
--publishers
3
This command launches a browser and opens:
http://localhost:3000/lk-recording-view?url=wss%3A%2F%2Ftest-1234567890.livekit.cloud&token=<RECORDER TOKEN>&layout=grid
Using the Custom Recording View in Production
Set the
custom_base_url
parameter on the
StartRoomCompositeEgress()
API to the URL where your custom recording application is deployed.
For additional authentication, most customers attach URL parameters to the
custom_base_url
. For example:
https://your-template-url.example.com/?yourparam={auth_info}
(and set this as your
custom_base_url
).
Recording Process
For those curious about the workflow of a recording, the basic steps are:
The
Egress.StartRoomCompositeEgress()
API is invoked
LiveKit assigns an available egress instance to handle the request
The egress recorder creates necessary connection & authentication details
A URL for the rendering web page is constructed with these parameters:
url
: URL of LiveKit Server
token
: Access token for joining the room as a recorder
layout
: Desired layout passed to
StartRoomCompositeEgress()
The egress recorder launches a headless Chrome instance with the constructed URL
The recorder waits for the web page to log
START_RECORDING
to the console
The recording begins
The recorder waits for the web page to log
END_RECORDING
to the console
The recording is terminated
On this page
Overview
Built-in LiveKit Recording View
Building a Custom Recording View
Building your Application
Deploying your Application
Testing Your Application
Using the Custom Recording View in Production
Recording Process


Content from https://docs.livekit.io/home/egress/api:

On this page
API
StartRoomCompositeEgress
StartTrackCompositeEgress
StartTrackEgress
StartWebEgress
UpdateLayout
UpdateStream
ListEgress
StopEgress
Types
EncodedFileOutput
DirectFileOutput
SegmentedFileOutput
StreamOutput
ImageOutput
S3Upload
GCPUpload
AzureBlobUpload
AliOSSUpload
EncodingOptions
EncodingOptionsPreset
ProxyConfig
WebhookConfig
Copy page
See more page options
API
The Egress API is available within our server SDKs and CLI:
Go Egress Client
JS Egress Client
Ruby Egress Client
Python Egress Client
Java Egress Client
CLI
Important
Requests to the Egress API need the
roomRecord
permission on the
access token
.
You can also use
curl
to interact with the Egress APIs. To do so,
POST
the arguments in JSON format to:
https://<your-livekit-host>/twirp/livekit.Egress/<MethodName>
For example:
%
curl
-X
POST https://
<
your-livekit-host
>
/twirp/livekit.Egress/StartRoomCompositeEgress
\
-H
'Authorization: Bearer <livekit-access-token>'
\
-H
'Content-Type: application/json'
\
-d
'{"room_name": "your-room", "segments": {"filename_prefix": "your-hls-playlist.m3u8", "s3": {"access_key": "<key>", "secret": "<secret>", "bucket": "<bucket>", "region": "<bucket-region>"}}}'
{
"egress_id"
:
"EG_MU4QwhXUhWf9"
,
"room_id"
:
"<room-id>"
,
"room_name"
:
"your-room"
,
"status"
:
"EGRESS_STARTING"
..
.
}
Tip
All RPC definitions and options can be found
here
.
StartRoomCompositeEgress
Starts a new
Composite Recording
using a web browser as the rendering engine.
Parameter
Type
Required
Description
room_name
string
yes
name of room to record
layout
string
layout parameter that is passed to the template
audio_only
bool
true if resulting output should only contain audio
video_only
bool
true if resulting output should only contain video
custom_base_url
string
URL to the page that would composite tracks, uses embedded templates if left blank
file_outputs
EncodedFileOutput
[]
output to MP4 file. currently only supports a single entry
segment_outputs
SegmentedFileOutput
[]
output to HLS segments. currently only supports a single entry
stream_outputs
StreamOutput
[]
output to a stream. currently only supports a single entry, though it could includ multiple destination URLs
image_outputs
ImageOutput
[]
output to a succession of snapshot images taken at a given interval (thumbnails). Currently only supports a single entry.
preset
EncodingOptionsPreset
encoding preset to use. only one of preset or advanced could be set
advanced
EncodingOptions
advanced encoding options. only one of preset or advanced could be set
webhooks
WebhookConfig
[]
extra webhooks to send on egress events for this request
StartTrackCompositeEgress
Starts a new
Track Composite
Parameter
Type
Required
Description
room_name
string
yes
name of room to record
audio_track_id
string
ID of audio track to composite
video_track_id
string
ID of video track to composite
file_outputs
EncodedFileOutput
[]
output to MP4 file. currently only supports a single entry
segment_outputs
SegmentedFileOutput
[]
output to HLS segments. currently only supports a single entry
stream_outputs
StreamOutput
[]
output to a stream. currently only supports a single entry, though it could includ multiple destination URLs
image_outputs
ImageOutput
[]
output to a succession of snapshot images taken at a given interval (thumbnails). Currently only supports a single entry.
preset
EncodingOptionsPreset
encoding preset to use. only one of preset or advanced could be set
advanced
EncodingOptions
advanced encoding options. only one of preset or advanced could be set
webhooks
WebhookConfig
[]
extra webhooks to send on egress events for this request
StartTrackEgress
Starts a new
Track Egress
Parameter
Type
Required
Description
room_name
string
yes
name of room to record
track_id
string
layout parameter that is passed to the template
file
DirectFileOutput
only one of file or websocket_url can be set
websocket_url
string
url to websocket to receive audio output. only one of file or websocket_url can be set
webhooks
WebhookConfig
[]
extra webhooks to send on egress events for this request
StartWebEgress
Starts a new
Web Egress
Parameter
Type
Required
Description
url
string
yes
URL of the web page to record
audio_only
bool
true if resulting output should only contain audio
video_only
bool
true if resulting output should only contain video
file_outputs
EncodedFileOutput
[]
output to MP4 file. currently only supports a single entry
segment_outputs
SegmentedFileOutput
[]
output to HLS segments. currently only supports a single entry
stream_outputs
StreamOutput
[]
output to a stream. currently only supports a single entry, though it could includ multiple destination URLs
image_outputs
ImageOutput
[]
output to a succession of snapshot images taken at a given interval (thumbnails). Currently only supports a single entry.
preset
EncodingOptionsPreset
encoding preset to use. only one of preset or advanced could be set
advanced
EncodingOptions
advanced encoding options. only one of preset or advanced could be set
webhooks
WebhookConfig
[]
extra webhooks to send on egress events for this request
UpdateLayout
Used to change the web layout on an active RoomCompositeEgress.
Parameter
Type
Required
Description
egress_id
string
yes
Egress ID to update
layout
string
yes
layout to update to
JavaScript
Go
Ruby
Java
LiveKit CLI
const
info
=
await
egressClient
.
updateLayout
(
egressID
,
'grid-light'
)
;
UpdateStream
Used to add or remove stream urls from an active stream
Note: you can only add outputs to an Egress that was started with
stream_outputs
set.
Parameter
Type
Required
Description
egress_id
string
yes
Egress ID to update
add_output_urls
string[]
URLs to add to the egress as output destinations
remove_output_urls
string[]
URLs to remove from the egress
JavaScript
Go
Ruby
Java
LiveKit CLI
const
streamOutput
=
new
StreamOutput
(
{
protocol
:
StreamProtocol
.
RTMP
,
urls
:
[
'rtmp://live.twitch.tv/app/<stream-key>'
]
,
}
)
;
var
info
=
await
egressClient
.
startRoomCompositeEgress
(
'my-room'
,
{
stream
:
streamOutput
}
)
;
const
streamEgressID
=
info
.
egressId
;
info
=
await
egressClient
.
updateStream
(
streamEgressID
,
[
'rtmp://a.rtmp.youtube.com/live2/stream-key'
,
]
)
;
ListEgress
Used to list active egress. Does not include completed egress.
JavaScript
Go
Ruby
Java
LiveKit CLI
const
res
=
await
egressClient
.
listEgress
(
)
;
StopEgress
Stops an active egress.
JavaScript
Go
Ruby
Java
LiveKit CLI
const
info
=
await
egressClient
.
stopEgress
(
egressID
)
;
Types
EncodedFileOutput
Field
Type
Description
filepath
string
default {room_name}-{time}
disable_manifest
bool
by default, Egress outputs a {filepath}.json with metadata of the file
s3
S3Upload
set if uploading to S3 compatible storage. only one storage output can be set
gcp
GCPUpload
set if uploading to GCP
azure
AzureBlobUpload
set if uploading to Azure
aliOSS
AliOSSUpload
set if uploading to AliOSS
DirectFileOutput
Field
Type
Description
filepath
string
default {track_id}-{time}
disable_manifest
bool
by default, Egress outputs a {filepath}.json with metadata of the file
s3
S3Upload
set if uploading to S3 compatible storage. only one storage output can be set
gcp
GCPUpload
set if uploading to GCP
azure
AzureBlobUpload
set if uploading to Azure
aliOSS
AliOSSUpload
set if uploading to AliOSS
SegmentedFileOutput
Field
Type
Description
filename_prefix
string
prefix used in each segment (include any paths here)
playlist_name
string
name of the m3u8 playlist. when empty, matches filename_prefix
segment_duration
uint32
length of each segment (defaults to 4s)
filename_suffix
SegmentedFileSuffix
INDEX (1, 2, 3) or TIMESTAMP (in UTC)
disable_manifest
bool
s3
S3Upload
set if uploading to S3 compatible storage. only one storage output can be set
gcp
GCPUpload
set if uploading to GCP
azure
AzureBlobUpload
set if uploading to Azure
aliOSS
AliOSSUpload
set if uploading to AliOSS
StreamOutput
Field
Type
Description
protocol
SreamProtocol
(optional) only RTMP is supported
urls
string[]
list of URLs to send stream to
ImageOutput
Field
Type
Description
capture_interval
uint32
time in seconds between each snapshot
width
int32
width of the snapshot images (optional, the original width will be used if not provided)
height
int32
height of the snapshot images (optional, the original width will be used if not provided)
filename_prefix
string
prefix used in each image filename (include any paths here)
filename_suffix
ImageFileSuffix
INDEX (1, 2, 3) or TIMESTAMP (in UTC)
image_codec
ImageCodec
IC_DEFAULT or IC_JPEG (optional, both options will cause JPEGs to be generated currently)
disable_manifest
bool
by default, Egress outputs a {filepath}.json with a list of exported snapshots
s3
S3Upload
set if uploading to S3 compatible storage. only one storage output can be set
gcp
GCPUpload
set if uploading to GCP
azure
AzureBlobUpload
set if uploading to Azure
aliOSS
AliOSSUpload
set if uploading to AliOSS
S3Upload
Field
Type
Description
access_key
string
secret
string
S3 secret key
bucket
string
destination bucket
region
string
region of the S3 bucket (optional)
endpoint
string
URL to use for S3 (optional)
force_path_style
bool
leave bucket in the path and never to sub-domain (optional)
metadata
map<string, string>
metadata key/value pairs to store (optional)
tagging
string
(optional)
proxy
ProxyConfig
Proxy server to use when uploading(optional)
GCPUpload
Field
Type
Description
credentials
string
Contents of credentials.json
bucket
string
destination bucket
proxy
ProxyConfig
Proxy server to use when uploading(optional)
AzureBlobUpload
Field
Type
Description
account_name
string
account_key
string
container_name
string
destination container
AliOSSUpload
Field
Type
Description
access_key
string
secret
string
bucket
string
region
string
endpoint
string
EncodingOptions
Field
Type
Description
width
int32
height
int32
depth
int32
default 24
framerate
int32
default 30
audio_codec
AudioCodec
default AAC
audio_bitrate
int32
128
audio_frequency
int32
44100
video_codec
VideoCodec
default H264_MAIN
video_bitrate
int32
default 4500
key_frame_interval
int32
default 4s
EncodingOptionsPreset
Enum, valid values:
H264_720P_30: 0
H264_720P_60: 1
H264_1080P_30: 2
H264_1080P_60: 3
PORTRAIT_H264_720P_30: 4
PORTRAIT_H264_720P_60: 5
PORTRAIT_H264_1080P_30: 6
PORTRAIT_H264_1080P_60: 7
ProxyConfig
For S3 and GCP, you can specify a proxy server for Egress to use when uploading files.
This can be helpful to avoid network restrictions on the destination buckets.
Field
Type
Description
url
string
URL of the proxy
username
string
username for basic auth (optional)
password
string
password for basic auth (optional)
WebhookConfig
Extra webhooks can be configured for a specific Egress request. These webhooks are called for Egress lifecycle events in addition to the project wide webhooks. To learn more, see
Webhooks
.
Field
Type
Description
url
string
URL of the webhook
signing_key
string
API key to use to sign the request, must be defined for the project
On this page
API
StartRoomCompositeEgress
StartTrackCompositeEgress
StartTrackEgress
StartWebEgress
UpdateLayout
UpdateStream
ListEgress
StopEgress
Types
EncodedFileOutput
DirectFileOutput
SegmentedFileOutput
StreamOutput
ImageOutput
S3Upload
GCPUpload
AzureBlobUpload
AliOSSUpload
EncodingOptions
EncodingOptionsPreset
ProxyConfig
WebhookConfig


Content from https://docs.livekit.io/home/egress/examples:

On this page
Recording Room Composite as HLS
Recording Web In Portrait
SRT Streaming With Thumbnails
Adding RTMP To Track Composite Egress
Exporting Individual Tracks Without Transcode
Copy page
See more page options
Recording Room Composite as HLS
This example records a room composite layout as HLS segments to a S3 bucket.
LiveKit CLI
JavaScript
Go
Ruby
Python
Java
Note
When
live_playlist_name
is provided, we'll generate a playlist containing only the last few segments.
This can be useful to live-stream the recording via HLS.
{
"room_name"
:
"my-room"
,
"layout"
:
"grid"
,
"preset"
:
"H264_720P_30"
,
"custom_base_url"
:
"https://my-custom-template.com"
,
"audio_only"
:
false
,
"segment_outputs"
:
[
{
"filename_prefix"
:
"path/to/my-output"
,
"playlist_name"
:
"my-output.m3u8"
,
"live_playlist_name"
:
"my-output-live.m3u8"
,
"segment_duration"
:
2
,
"s3"
:
{
"access_key"
:
""
,
"secret"
:
""
,
"region"
:
""
,
"bucket"
:
"my-bucket"
,
"force_path_style"
:
true
}
}
]
}
lk egress start
--type
room-composite egress.json
Recording Web In Portrait
This example records a web page in portrait mode to Google Cloud Storage and streaming to RTMP.
Portrait orientation can be specified by either setting a preset or advanced options. Egress will resize the Chrome compositor to your specified resolution. However, keep in mind:
Chrome has a minimum browser width limit of 500px.
Your application should maintain a portrait layout, even when the browser reports a width larger than typical mobile phones. (e.g., 720px width or higher).
LiveKit CLI
JavaScript
Go
Ruby
Python
Java
{
"url"
:
"https://my-page.com"
,
"preset"
:
"PORTRAIT_H264_720P_30"
,
"audio_only"
:
false
,
"file_outputs"
:
[
{
"filepath"
:
"my-test-file.mp4"
,
"gcp"
:
{
"credentials"
:
"{\"type\": \"service_account\", ...}"
,
"bucket"
:
"my-bucket"
}
}
]
,
"stream_outputs"
:
[
{
"protocol"
:
"RTMP"
,
"urls"
:
[
"rtmps://my-rtmp-server.com/live/stream-key"
]
}
]
}
lk egress start
--type
web egress.json
SRT Streaming With Thumbnails
This examples shows streaming a Participant Egress to a SRT server, and generating thumbnails every 5 seconds. Thumbnails are stored in Azure.
LiveKit CLI
JavaScript
Go
Ruby
Python
Java
{
"room_name"
:
"my-room"
,
"identity"
:
"participant-to-record"
,
"screen_share"
:
false
,
"advanced"
:
{
"width"
:
1280
,
"height"
:
720
,
"framerate"
:
30
,
"audioCodec"
:
"AAC"
,
"audioBitrate"
:
128
,
"videoCodec"
:
"H264_HIGH"
,
"videoBitrate"
:
5000
,
"keyFrameInterval"
:
2
}
,
"stream_outputs"
:
[
{
"protocol"
:
"SRT"
,
"urls"
:
[
"srt://my-srt-server.com:9999"
]
}
]
,
"image_outputs"
:
[
{
"capture_interval"
:
5
,
"width"
:
1280
,
"height"
:
720
,
"filename_prefix"
:
"{room_name}/{publisher_identity}"
,
"filename_suffix"
:
"IMAGE_SUFFIX_TIMESTAMP"
,
"disable_manifest"
:
true
,
"azure"
:
{
"account_name"
:
"my-account"
,
"account_key"
:
"my-key"
,
"container_name"
:
"my-container"
}
}
]
}
lk egress start
--type
participant egress.json
Adding RTMP To Track Composite Egress
This example demonstrates a TrackComposite Egress that starts by saving to HLS, with RTMP output added later.
LiveKit CLI
JavaScript
Go
Ruby
Python
Java
{
"room_name"
:
"my-room"
,
"audio_track_id"
:
"TR_AUDIO_ID"
,
"video_track_id"
:
"TR_VIDEO_ID"
,
"stream_outputs"
:
[
{
"protocol"
:
"RTMP"
,
"urls"
:
[
]
}
]
,
"segment_outputs"
:
[
{
"filename_prefix"
:
"path/to/my-output"
,
"playlist_name"
:
"my-output.m3u8"
,
"segment_duration"
:
2
,
"s3"
:
{
"access_key"
:
""
,
"secret"
:
""
,
"region"
:
""
,
"bucket"
:
"my-bucket"
}
}
]
}
lk egress start
--type
track-composite egress.json
# later, to add a RTMP output
lk egress update-stream
--id
<
egress-id
>
--add-urls rtmp://new-server.com/live/stream-key
# to remove RTMP output
lk egress update-stream
--id
<
egress-id
>
--remove-urls rtmp://new-server.com/live/stream-key
Exporting Individual Tracks Without Transcode
This example exports video tracks to Azure Blob Storage without transcoding.
Note: video and audio tracks must be exported separately using Track Egress.
LiveKit CLI
JavaScript
Go
Ruby
Python
Java
{
"room_name"
:
"my-room"
,
"track_id"
:
"TR_TRACK_ID"
,
"filepath"
:
"{room_name}/{track_id}"
,
"azure"
:
{
"account_name"
:
"my-account"
,
"account_key"
:
"my-key"
,
"container_name"
:
"my-container"
}
}
lk egress start
--type
track egress.json
On this page
Recording Room Composite as HLS
Recording Web In Portrait
SRT Streaming With Thumbnails
Adding RTMP To Track Composite Egress
Exporting Individual Tracks Without Transcode


Content from https://docs.livekit.io/home/ingress/configure-streaming-software:

On this page
OBS
FFmpeg
GStreamer
Copy page
See more page options
The
IngressInfo
object returned by most Ingress APIs contains a full list of the ingress parameters. In particular, the
url
and
stream_key
fields provide the settings required to configure encoders to send media to the Ingress service. Refer to the documentation of any RTMP or WHIP-capable streaming software for more information about how to provide these parameters. Two common examples are OBS and FFmpeg:
OBS
The
OBS Project
releases OBS Studio, a powerful cross platform broadcasting software that can be fully configured through a graphical user interface, and capable of sending complex video compositions to LiveKit WebRTC via Ingress. In order to configure OBS for LiveKit, in the main window, select the
Settings
option, and then the
Stream
tab. In the window, select the
Custom...
Service and enter the URL from the
StreamInfo
in the
Server
field, and the stream key in the
Stream Key
field.
FFmpeg
FFmpeg
is a powerful media processing command-line tool that can be used to stream media to LiveKit Ingress. The following command can be used for that purpose:
% ffmpeg
-re
-i
<
input definition
>
-c:v
libx254
-b:v
3M
-preset
veryfast
-profile
high
-c:a
libfdk_aac
-b:a
128k
-f
flv
"<url from the stream info>/<stream key>"
For instance:
% ffmpeg
-re
-i
my_file.mp4
-c:v
libx264
-b:v
3M
-preset
veryfast
-profile:v
high
-c:a
libfdk_aac
-b:a
128k
-f
flv rtmps://my-project.livekit.cloud/x/1234567890ab
Refer to the
FFmpeg documentation
for a list of the supported inputs, and how to use them.
GStreamer
GStreamer
is multi platform multimedia framework that can be used either directly using command line tools provided as part of the distribution, or integrated in other applications using their API. GStreamer supports streaming media to LiveKit Ingress both over RTMP and WHIP.
For RTMP, the following sample command and pipeline definition can be used:
% gst-launch-1.0 flvmux
name
=
mux
!
rtmp2sink
location
=
"<url from the stream info>/<stream key>"
audiotestsrc
wave
=
sine-table
!
faac
!
mux. videotestsrc is-live
=
true
!
video/x-raw,width
=
1280
,height
=
720
!
x264enc speed-preset
=
3
tune
=
zerolatency
!
mux.
WHIP requires the following GStreamer plugins to be installed:
nicesink
webrtcbin
whipsink
Some these plugins are distributed as part of
libnice
or the
Rust GStreamer plugins package
and may not always be present. This can be verified using the
gst-inspect-1.0
command. LiveKit provides a Docker image based on Ubuntu that includes all the required GStreamer plugins at
livekit/gstreamer:1.22.8-prod-rs
.
gst-launch-1.0 audiotestsrc
wave
=
sine-table
!
opusenc
!
rtpopuspay
!
'application/x-rtp,media=audio,encoding-name=OPUS,payload=96,clock-rate=48000,encoding-params=(string)2'
!
whip.sink_0 videotestsrc is-live
=
true
!
video/x-raw,width
=
1280
,height
=
720
!
x264enc speed-preset
=
3
tune
=
zerolatency
!
rtph264pay
!
'application/x-rtp,media=video,encoding-name=H264,payload=97,clock-rate=90000'
!
whip.sink_1 whipsink
name
=
whip whip-endpoint
=
"<url from the stream info>/<stream key>"
These 2 sample command lines use the
audiotestsrc
and
videotestsrc
sources to generate test audio and video pattern. These can be replaced with other GStreamer sources to stream any media supported by GStreamer.
On this page
OBS
FFmpeg
GStreamer


Content from https://docs.livekit.io/home/cloud/architecture:

On this page
Built with LiveKit SFU
Distributed Mesh Architecture
Multi-home
No SPOF
Globally distributed
Designed to scale
Copy page
See more page options
Built with LiveKit SFU
LiveKit Cloud
builds on our open-source
SFU
. This means it supports the exact same SDKs and APIs as the open-source
stack
.
Maintaining compatibility with LiveKit's Open Source stack (OSS) is important to us. We didn't want any developer locked into using Cloud, or needing to integrate a different set of features, APIs or SDKs for their applications to work with it. Our design goal: a developer should be able to switch between Cloud or self-hosted without changing a line of code.
Distributed Mesh Architecture
In contrast to traditional
WebRTC architectures
, LiveKit Cloud runs multiple SFU instances in a mesh formation. We've developed capabilities for media servers to discover and connect to one another, in order to relay media between servers. This key capability allows us to bypass the single-server limitation that exists in traditional SFU and MCU architectures.
Multi-home
With a multi-home architecture, participants no longer need to connect to the same server. When participants from different regions join the same meeting, they'll each connect to the SFU closest to them, minimizing latency and transmission loss between the participant and SFU.
Each SFU instance establishes connections to other instances over optimized inter-data center networks. Inter-data center networks often run close to internet backbones, delivering high throughput with a minimal number of network hops.
No SPOF
Anything that can fail, will. LiveKit Cloud is designed to anticipate (and recover from) failures in every software and hardware component.
Layers of redundancy are built into the system. A media server failure is recovered from by moving impacted participants to another instance. We isolate shared infrastructure, like our message bus, to individual data centers.
When an entire data center fails, customer traffic is automatically migrated to the next closest data center. LiveKit's SDKs will perform a "session migration": moving existing WebRTC sessions to a different media server without service interruption for your users.
Globally distributed
To serve end users around the world, our infrastructure runs across multiple Cloud vendors and data centers, delivering under 100ms of latency in each region. Today, we have data centers in the following regions:
North America (US East, US Central, US West)
South America (Brazil)
Oceania (Australia)
East Asia (Japan)
Southeast Asia (Singapore)
South Asia (India)
Middle East (Israel, Saudi Arabia, UAE)
Africa (South Africa)
Europe (France, Germany, UK)
Designed to scale
When you need to support many viewers on a media track, such as in a livestream, LiveKit Cloud dynamically manages that capacity by forming a distribution mesh, similar to a CDN. This process occurs automatically as your session scales, with no special configurations required. Every LiveKit Cloud project scales seamlessly to accommodate millions of concurrent users in any session.
For a deeper look into the design decisions we've made for LiveKit Cloud, you can
read more
on our blog.
On this page
Built with LiveKit SFU
Distributed Mesh Architecture
Multi-home
No SPOF
Globally distributed
Designed to scale


Content from https://docs.livekit.io/home/cloud/sandbox:

On this page
Overview
Getting started
Moving to production
Community templates
Copy page
See more page options
Overview
LiveKit Sandboxes
are hosted components that help you prototype your ideas without having to copy and paste code or manage deployments. They're integrated with our CLI, and ready to work with your LiveKit account out of the box. You can use a sandbox to:
Build and customize an AI voice assistant you can share with others, without building and deploying a frontend.
Prototype a mobile or web app without having to set up and deploy a backend server with a token endpoint.
Set up video conferencing rooms with a single click, and share the link with friends and colleagues.
Getting started
Once you've created a LiveKit Cloud account, you can head to the
Sandboxes
page to create a new sandbox, choosing from one of our templates.
Create a LiveKit Cloud account and
Install the LiveKit CLI
.
If you're setting up the CLI for the first time, authenticate with your LiveKit Cloud account:
lk cloud auth
Navigate to the
Sandboxes
page to create a new sandbox, choosing from one of our templates.
Some templates (for example,
Next.js Voice Agent
) require you to run some code on your local machine. This might be an AI agent, a web server, or some other component depending on that template's use case. If present, follow the instructions under the
Code
tab to clone and set up the component:
lk app create
\
--template
<
template-name
>
\
--sandbox
<
my-sandbox-id
>
Moving to production
When you're ready to move on from the prototyping stage and own the code yourself, every sandbox app can be cloned to your local machine, ready for customization. The quickest way to do this is via the
LiveKit CLI
:
lk app create
--template
<
template-name
>
You'll notice this is similar to the process for cloning agents and other local templates. That's because all sandboxes, and many other templates at
github.com/livekit-examples
, are simple git repositories with a few conventions around environment variables and make them ready to work with your LiveKit account and the CLI.
Community templates
If you're interested in creating and sharing your own templates with the larger community of LiveKit users, check out the
Template Index
repository for more information on contributing.
On this page
Overview
Getting started
Moving to production
Community templates


Content from https://docs.livekit.io/home/cloud/quotas-and-limits:

On this page
Free quotas
Connection limits
Build plan
Ship plan
Scale plan
Custom plan
Egress time limits
Media subscription limits
API request rate limits
Copy page
See more page options
Free quotas
Every LiveKit project gets the following for free:
50GB data transfer
5,000 connection minutes
60 minutes of transcoding (for
Stream import (ingress)
or
Composite recording (egress)
)
Connection limits
LiveKit projects have limitations on the number of concurrent connections of various types in order
to ensure the stability of the network and to prevent abuse. This is similar to rate limiting for an HTTP
service, but for a continuous service with long-lived connections. Like rate limiting, the primary
purpose of these connection limits is to prevent abuse.
You can view the current connection limits on your project at any time in the
LiveKit Cloud
dashboard by navigating to
Settings
and selecting
the
Project
tab.
For pricing information for any of the following plans, see the
pricing guide
.
Build plan
Projects on the
Build
(free) plan have the following limits:
100 concurrent participants.
2 concurrent
egress requests
at a time.
2 concurrent
ingress requests
at a time.
When these limits are reached, new connections of the same type fail.
Ship plan
Projects on the
Ship
plan have the following limits:
1,000 concurrent participants.
100 concurrent egress requests.
100 concurrent ingress requests.
When these limits are reached, new connections of the same type fail.
Scale plan
Projects on the
Scale
plan have the following default limits:
5,000 concurrent participants.
100 concurrent egress requests.
100 concurrent ingress requests.
When these limits are reached, new connections of the same type fail.
Tip
Your project admin can request an increase for specific limits in your
project settings
.
Custom plan
LiveKit can work with you to ensure your project has the capacity it needs.
Contact the sales team
with your project details.
Egress time limits
Egress has time limits, depending on the output type:
Egress output
Time limit
File output (MP4, OGG, WebM)
3 hours
HLS segments
12 hours
HLS/RTMP streaming
12 hours
When these time limits are reached, any in-progress egress automatically ends with the status
LIMIT_REACHED
.
You can listen for this status change using the
egress_ended
webhook
.
Media subscription limits
Each participant may subscribe to a limited number of media tracks. Currently, the limits are
as follows:
Up to 100 video tracks.
Up to 100 audio tracks.
For high volume video use cases, consider using pagination and
selective subscriptions
to keep the number of
subscriptions within these limits.
API request rate limits
All projects have a 1000 requests per minute rate limit on API requests. The limit only applies to
Server API
requests (for example,
RoomService
or
EgressService
API requests) and doesn't apply to SDK methods like joining a room or sending data packets.
LiveKit doesn't anticipate any project exceeding this rate limit. However, you can reach out to
support
to request an increase. Include the
Project URL
in your email.
You can find your project URL in the LiveKit Cloud dashboard in your
Project Settings
page.
On this page
Free quotas
Connection limits
Build plan
Ship plan
Scale plan
Custom plan
Egress time limits
Media subscription limits
API request rate limits


Content from https://docs.livekit.io/home/cloud/billing:

On this page
Overview
Billing
How we meter
Billing cycle
Invoices
Downloading invoices
Customizing invoices
Copy page
See more page options
Overview
Refer to our latest
blog post
and
pricing page
for information about our current pricing.
Billing
How we meter
We meter all projects and bill for resources consumed. This table shows the resources we meter and the increments we bill in:
Resource
Unit
Minimum increment
Outbound transfer
GB
0.01 GB
Realtime connection
minute
1 minute
SIP connection
minute
1 minute
Egress Transcode
minute
1 minute
Ingress Transcode
minute
1 minute
Billing cycle
LiveKit Cloud bills monthly. At the end of each month, we calculate the total resources consumed by your project and bill you for the resources consumed.
Invoices
Downloading invoices
Paying projects can download previous months' invoices as PDFs on the project's
billing page
(accessible only to project admins) and clicking the "PDF" link in the "Statements" section.
Customizing invoices
By default, the invoice only lists your project name. Some customers require more information on the invoice, such as a billing address or VAT number. You can add this information to your invoice by clicking the
...
menu to the right of the PDF link, then clicking
Customize “Invoice to:” field
.
This field is a plain text field that accepts any text. Newlines will be preserved on the invoice PDF. For example, you could include your business name and address like so, and the invoice PDF will have line breaks in the same places:
Acme
Inc
.
404
Nowhere
Ln
.
New
York
,
NY
10001
After saving your “Invoice to:” field, you can click the
PDF
link to re-download the invoice PDF and it will include the new information.
On this page
Overview
Billing
How we meter
Billing cycle
Invoices
Downloading invoices
Customizing invoices


Content from https://docs.livekit.io/home/cloud/firewall:

On this page
Corporate firewalls
Minimum requirements
Copy page
See more page options
Corporate firewalls
LiveKit uses WebSocket and WebRTC to transmit data and media. All transmissions are encrypted with
TLS
and
DTLS
.
LiveKit Cloud requires access to a few domains in order to establish a connection. If you are behind a corporate firewall, please ensure outbound traffic is allowed to the following addresses and ports:
Host
Port
Purpose
*.livekit.cloud
TCP: 443
Signal connection over secure WebSocket
*.turn.livekit.cloud
TCP: 443
TURN
/TLS. Used when UDP connection isn't viable
*.host.livekit.cloud
UDP: 3478
TURN/UDP servers that assist in establishing connectivity
all hosts (optional)
UDP: 50000-60000
UDP connection for WebRTC
all hosts (optional)
TCP: 7881
TCP connection for WebRTC
In order to obtain the best audio and video quality, we recommend allowing access to the UDP ports listed above. Additionally, please ensure UDP hole-punching is enabled (or disable symmetric NAT). This helps machines behind the firewall to establish a direct connection to a LiveKit Cloud media server.
Minimum requirements
If wildcard hostnames are not allowed by your firewall or security policy, the following are the mimimum set of hostnames required to connect to LiveKit Cloud:
Host
Port
<your-subdomain>.livekit.cloud
TCP 443
<your-subdomain>.sfo3.production.livekit.cloud
TCP 443
<your-subdomain>.dsfo3a.production.livekit.cloud
TCP 443
<your-subdomain>.dsfo3b.production.livekit.cloud
TCP 443
<your-subdomain>.dfra1a.production.livekit.cloud
TCP 443
<your-subdomain>.dfra1b.production.livekit.cloud
TCP 443
<your-subdomain>.dblr1a.production.livekit.cloud
TCP 443
<your-subdomain>.dblr1b.production.livekit.cloud
TCP 443
<your-subdomain>.dsgp1a.production.livekit.cloud
TCP 443
<your-subdomain>.dsgp1b.production.livekit.cloud
TCP 443
<your-subdomain>.dsyd1a.production.livekit.cloud
TCP 443
<your-subdomain>.dsyd1b.production.livekit.cloud
TCP 443
<your-subdomain>.osaopaulo1a.production.livekit.cloud
TCP 443
<your-subdomain>.osaopaulo1b.production.livekit.cloud
TCP 443
<your-subdomain>.oashburn1a.production.livekit.cloud
TCP 443
<your-subdomain>.oashburn1b.production.livekit.cloud
TCP 443
<your-subdomain>.omarseille1a.production.livekit.cloud
TCP 443
<your-subdomain>.omarseille1b.production.livekit.cloud
TCP 443
<your-subdomain>.otokyo1a.production.livekit.cloud
TCP 443
<your-subdomain>.otokyo1b.production.livekit.cloud
TCP 443
<your-subdomain>.ophoenix1a.production.livekit.cloud
TCP 443
<your-subdomain>.ophoenix1b.production.livekit.cloud
TCP 443
<your-subdomain>.olondon1a.production.livekit.cloud
TCP 443
<your-subdomain>.olondon1b.production.livekit.cloud
TCP 443
<your-subdomain>.ochicago1a.production.livekit.cloud
TCP 443
<your-subdomain>.ochicago1b.production.livekit.cloud
TCP 443
<your-subdomain>.osingapore1a.production.livekit.cloud
TCP 443
<your-subdomain>.osingapore1b.production.livekit.cloud
TCP 443
<your-subdomain>.odubai1a.production.livekit.cloud
TCP 443
<your-subdomain>.odubai1b.production.livekit.cloud
TCP 443
<your-subdomain>.ohyderabad1a.production.livekit.cloud
TCP 443
<your-subdomain>.ohyderabad1b.production.livekit.cloud
TCP 443
<your-subdomain>.ojohannesburg1a.production.livekit.cloud
TCP 443
<your-subdomain>.ojohannesburg1b.production.livekit.cloud
TCP 443
<your-subdomain>.omumbai1a.production.livekit.cloud
TCP 443
<your-subdomain>.omumbai1b.production.livekit.cloud
TCP 443
<your-subdomain>.ofrankfurt1a.production.livekit.cloud
TCP 443
<your-subdomain>.ofrankfurt1b.production.livekit.cloud
TCP 443
<your-subdomain>.ojerusalem1a.production.livekit.cloud
TCP 443
<your-subdomain>.ojerusalem1b.production.livekit.cloud
TCP 443
<your-subdomain>.osydney1a.production.livekit.cloud
TCP 443
<your-subdomain>.osydney1b.production.livekit.cloud
TCP 443
<your-subdomain>.ozurich1a.production.livekit.cloud
TCP 443
<your-subdomain>.ozurich1b.production.livekit.cloud
TCP 443
<your-subdomain>.turn.livekit.cloud
TCP 443
sfo3.turn.livekit.cloud
TCP 443
dsfo3a.turn.livekit.cloud
TCP 443
dsfo3b.turn.livekit.cloud
TCP 443
dfra1a.turn.livekit.cloud
TCP 443
dfra1b.turn.livekit.cloud
TCP 443
dblr1a.turn.livekit.cloud
TCP 443
dblr1b.turn.livekit.cloud
TCP 443
dsgp1a.turn.livekit.cloud
TCP 443
dsgp1b.turn.livekit.cloud
TCP 443
dsyd1a.turn.livekit.cloud
TCP 443
dsyd1b.turn.livekit.cloud
TCP 443
osaopaulo1a.turn.livekit.cloud
TCP 443
osaopaulo1b.turn.livekit.cloud
TCP 443
oashburn1a.turn.livekit.cloud
TCP 443
oashburn1b.turn.livekit.cloud
TCP 443
omarseille1a.turn.livekit.cloud
TCP 443
omarseille1b.turn.livekit.cloud
TCP 443
otokyo1a.turn.livekit.cloud
TCP 443
otokyo1b.turn.livekit.cloud
TCP 443
ophoenix1a.turn.livekit.cloud
TCP 443
ophoenix1b.turn.livekit.cloud
TCP 443
olondon1a.turn.livekit.cloud
TCP 443
olondon1b.turn.livekit.cloud
TCP 443
ochicago1a.turn.livekit.cloud
TCP 443
ochicago1b.turn.livekit.cloud
TCP 443
osingapore1a.turn.livekit.cloud
TCP 443
osingapore1b.turn.livekit.cloud
TCP 443
odubai1a.turn.livekit.cloud
TCP 443
odubai1b.turn.livekit.cloud
TCP 443
ohyderabad1a.turn.livekit.cloud
TCP 443
ohyderabad1b.turn.livekit.cloud
TCP 443
ojohannesburg1a.turn.livekit.cloud
TCP 443
ojohannesburg1b.turn.livekit.cloud
TCP 443
omumbai1a.turn.livekit.cloud
TCP 443
omumbai1b.turn.livekit.cloud
TCP 443
ofrankfurt1a.turn.livekit.cloud
TCP 443
ofrankfurt1b.turn.livekit.cloud
TCP 443
ojerusalem1a.turn.livekit.cloud
TCP 443
ojerusalem1b.turn.livekit.cloud
TCP 443
osydney1a.turn.livekit.cloud
TCP 443
osydney1b.turn.livekit.cloud
TCP 443
ozurich1a.turn.livekit.cloud
TCP 443
ozurich1b.turn.livekit.cloud
TCP 443
Note
This list of domains is subject to change. Last updated 2025-06-27.
On this page
Corporate firewalls
Minimum requirements


Content from https://docs.livekit.io/home/cloud/region-pinning:

On this page
Overview
Protocol-based region pinning
Enabling protocol-based region pinning
Considerations
Available regions
Copy page
See more page options
Overview
Region pinning restricts network traffic to a specific geographical region. Use this feature to comply with local telephony regulations or data residency requirements.
There are two options for restricting traffic to a specific region:
Protocol-based region pinning
Signaling and transport protocols include region selection. Use this option with LiveKit realtime SDKs.
Region-based endpoint
Clients connect to a region-specific endpoint. Use this option for telephony applications. To learn more, see
SIP cloud and region pinning
.
Protocol-based region pinning
In protocol-based region pinning, region selection information is embedded in the initial signaling and transport messages. When pinning is enabled, if the initial connection is routed to a server outside the allowed regions, the request is rejected. The client then retries the connection using a server in one of the pinned regions.
Region pinning is available for customers on the
Scale plan
or higher.
Protocol-based region pinning only works with LiveKit realtime SDKs
For SIP requests, the server rejects the connection and doesn't retry it. Use
region-based endpoints
for SIP.
When to use protocol-based region pinning
When connecting with LiveKit realtime SDKs or when regional data residency (for example, GDPR compliance) is required.
Enabling protocol-based region pinning
LiveKit must enable region pinning for your project. To request region pinning, sign in to
LiveKit Cloud
and select the
Support
option in the menu.
Considerations
When you enable region pinning, you turn off automatic failover to the nearest region in the case of an outage.
Available regions
The following regions are available for region pinning:
Region name
Region locations
africa
South Africa
asia
Japan, Singapore
aus
Australia
eu
France, Germany, Zurich
il
Israel
india
India, India South
me
Saudi Arabia, UAE
sa
Brazil
uk
UK
us
US Central, US East B, US West B
Note
This list of regions is subject to change. Last updated 2025-07-23.
On this page
Overview
Protocol-based region pinning
Enabling protocol-based region pinning
Considerations
Available regions


Content from https://docs.livekit.io/home/cloud/analytics-api:

On this page
Generate an access token for Analytics requests
List sessions
Query parameters
List session details
Copy page
See more page options
Generate an access token for Analytics requests
Analytics API requests are authorized with a LiveKit token. This is generated by a server side SDK,much like
generating a token for joining Rooms
, except that the token needs the
roomList
grant.
Note
Analytics API is only available to LiveKit Cloud customers with a
Scale plan or higher
.
LiveKit CLI
Node.js
lk token create
\
--api-key
$LIVEKIT_API_KEY
\
--api-secret
$LIVEKIT_SECRET_KEY
\
--list
\
--valid-for 24h
Tip
To streamline your workflow with the
CLI
, add your projects using the command
lk project add
. This approach spares you from repeatedly entering your
--url
,
--api-key
, and
--api-secret
for each command you execute.
List sessions
To make a request, you'll need to know your project id, which you can see in the URL for your project dashboard. It's the part after
/projects/
that starts with
p_
.
Node.js
Shell
async
function
listLiveKitSessions
(
)
{
const
endpoint
=
`
https://cloud-api.livekit.io/api/project/
${
PROJECT_ID
}
/sessions/
`
;
try
{
const
response
=
await
fetch
(
endpoint
,
{
method
:
'GET'
,
headers
:
{
Authorization
:
`
Bearer
${
token
}
`
,
'Content-Type'
:
'application/json'
,
}
,
}
)
;
if
(
!
response
.
ok
)
throw
new
Error
(
'Network response was not ok'
)
;
const
data
=
await
response
.
json
(
)
;
console
.
log
(
data
)
;
// or do whatever you like here
}
catch
(
error
)
{
console
.
log
(
'There was a problem:'
,
error
.
message
)
;
}
}
listLiveKitSessions
(
)
;
This will return a JSON object like this:
{
sessions
:
[
{
sessionId
,
// string
roomName
,
// string
createdAt
,
// Timestamp
lastActive
,
// Timestamp
bandwidthIn
,
// bytes of bandwidth uploaded
bandwidthOut
,
// bytes of bandwidth downloaded
egress
,
// 0 = never started, 1 = active, 2 = ended
numParticipants
,
// int
numActiveParticipants
,
// int
}
,
// ...
]
}
Query parameters
limit
int
Required
#
You can limit the number of returned sessions by adding the limit query parameter like
?limit=100
.
Caution
Higher
limit
values may result in a timeout from the Analytics API.
page
int
Required
#
You can page through the results by adding
?page=n&limit=100
to the endpoint URL to get the
n
th page of results with
100
sessions per page. Pagination starts from
0
.
start
string
Required
#
Specify the start date for the request time range in the format
YYYY-MM-DD
. Sessions starting on the specified start date will be included in the response.
Note
The start date must be within 7 days of the current date.
end
string
Required
#
Specify the end date for the request time range using the format
YYYY-MM-DD
. Sessions up to and including this end date will be included in the response.
Examples
# Get the first page and limit the number of sessions to 100.
curl
-H
"Authorization: Bearer
$TOKEN
"
\
"https://cloud-api.livekit.io/api/project/
$PROJECT_ID
/sessions\
?page=0&limit=100"
# Fetch sessions from a specified time range.
curl
-H
"Authorization: Bearer
$TOKEN
"
\
"https://cloud-api.livekit.io/api/project/
$PROJECT_ID
/sessions\
?start=2024-01-12&end=2024-01-13"
List session details
To get more details about a specific session, you can use the session_id returned from the list sessions request.
Node.js
Shell
async
function
getLiveKitSessionDetails
(
)
{
const
endpoint
=
`
https://cloud-api.livekit.io/api/project/
${
PROJECT_ID
}
/sessions/
${
SESSION_ID
}
`
;
try
{
const
response
=
await
fetch
(
endpoint
,
{
method
:
'GET'
,
headers
:
{
Authorization
:
`
Bearer
${
token
}
`
,
'Content-Type'
:
'application/json'
,
}
,
}
)
;
if
(
!
response
.
ok
)
throw
new
Error
(
'Network response was not ok'
)
;
const
data
=
await
response
.
json
(
)
;
console
.
log
(
data
)
;
// or do whatever you like here
}
catch
(
error
)
{
console
.
log
(
'There was a problem:'
,
error
.
message
)
;
}
}
getLiveKitSessionDetails
(
)
;
This will return a JSON object like this:
{
roomId
,
// string
roomName
,
// string
bandwidth
,
// billable bytes of bandwidth used
startTime
,
// Timestamp
endTime
,
// Timestamp
numParticipants
,
// int
participants
:
[
{
participantIdentity
,
// string
participantName
,
// string
roomId
,
// string
joinedAt
,
// Timestamp
leftAt
,
// Timestamp
publishedSources
:
{
cameraTrack
,
// boolean
microphoneTrack
,
// boolean
screenShareTrack
,
// boolean
screenShareAudio
,
// boolean
}
,
sessions
:
[
{
sessionId
,
// string
joinedAt
,
// Timestamp
leftAt
,
// Timestamp
}
,
// ...
]
,
}
,
// ...
]
,
}
Timestamp
objects are
Protobuf Timestamps
.
On this page
Generate an access token for Analytics requests
List sessions
Query parameters
List session details


Content from https://docs.livekit.io/home/cloud/noise-cancellation:

On this page
Overview
Supported platforms
Usage instructions
Copy page
See more page options
Overview
LiveKit Cloud includes advanced models licensed from
Krisp
to remove background noise and ensure the best possible audio quality. The models run locally, with no audio data sent to Krisp servers as part of this process and negligible impact on audio latency or quality.
The feature includes a background voice cancellation (BVC) model, which removes extra background speakers in addition to background noise, providing the best possible experience for voice AI applications. You can also use the standard NC model if desired.
The following comparison shows the effect of the models on the audio as perceived by a user, and also as perceived by a voice AI agent running an STT model (
Deepgram Nova 3
in these samples). The segments marked with a strikethrough indicate unwanted content that would confuse the agent. These samples illustrate that BVC is necessary to achieve clean STT in noisy multi-speaker environments.
Try the free
noise canceller tool
with your LiveKit Cloud account to test your own audio samples.
Original Audio - Noisy Taxi Ride
STT by Deepgram Nova 3
Hi there, can you hear me alright?
pretty bad
Sorry it's pretty noisy in the taxi.
sorry i can hear you pretty soon
Okay so as I was saying I just heard about this platform called LiveKit - it's an all in one platform for voice AI agents with some pretty cool features. Have you heard about it?
With Noise Cancellation (NC)
STT by Deepgram Nova 3
Hi there, can you hear me alright?
pretty bad
Sorry it's pretty noisy in the taxi.
sorry i'll get you there pretty soon
Okay so as I was saying I just heard about this platform called LiveKit - it's an all in one platform for voice AI agents with some pretty cool features. Have you heard about it?
With Background Voice Cancellation (BVC)
STT by Deepgram Nova 3
Hi there, can you hear me alright? Sorry it's pretty noisy in the taxi. Okay so as I was saying I just heard about this platform called LiveKit - it's an all in one platform for voice AI agents with some pretty cool features. Have you heard about it?
Supported platforms
You can apply the filter in the frontend ("outbound") with plugins for JavaScript, Swift, and Android, or directly inside of your agent code ("inbound"). The BVC model is available only within your agent, using the Python or Node.js plugins. LiveKit also offers an NC model for SIP-based telephony, which can be enabled with a flag in the trunk configuration.
The following table shows the support for each platform.
Platform
Outbound
Inbound
BVC
Package
Web
✅
❌
❌
@livekit/krisp-noise-filter
Swift
✅
❌
❌
LiveKitKrispNoiseFilter
Android
✅
❌
❌
io.livekit:krisp-noise-filter
Flutter
✅
❌
❌
livekit_noise_filter
React Native
✅
❌
❌
@livekit/react-native-krisp-noise-filter
Unity
❌
❌
❌
N/A
Python
❌
✅
✅
livekit-plugins-noise-cancellation
Node.js
❌
✅
✅
@livekit/noise-cancellation-node
Telephony
✅
✅
❌
LiveKit SIP documentation
Usage instructions
Use the following instructions to integrate the filter into your app.
Tip
When using server-side noise or background voice cancellation (for example, in agents), client-side noise cancellation should be disabled.
Noise cancellation models are trained on raw audio and may produce unexpected results if the input has already been processed on the client.
Python
Node.js
JavaScript
Android
Swift
React Native
Flutter
SIP
Installation
Install the noise cancellation package from PyPI:
pip
install
"livekit-plugins-noise-cancellation~=0.2"
Usage in LiveKit Agents
Include the filter in
RoomInputOptions
when starting your
AgentSession
:
from
livekit
.
plugins
import
noise_cancellation
# ...
await
session
.
start
(
# ...,
room_input_options
=
room_io
.
RoomInputOptions
(
noise_cancellation
=
noise_cancellation
.
BVC
(
)
,
)
,
)
# ...
Agents v0.12 compatibility
In LiveKit Agents v0.12, pass the
noise_cancellation
parameter to the
VoicePipelineAgent
or
MultimodalAgent
constructor.
Usage with AudioStream
Apply the filter to any individual inbound AudioStream:
stream
=
rtc
.
AudioStream
.
from_track
(
track
=
track
,
noise_cancellation
=
noise_cancellation
.
NC
(
)
,
)
Available models
There are three noise cancellation models available:
# Standard enhanced noise cancellation
noise_cancellation
.
NC
(
)
# Background voice cancellation (NC + removes non-primary voices
# that would confuse transcription or turn detection)
noise_cancellation
.
BVC
(
)
# Background voice cancellation optimized for telephony applications
noise_cancellation
.
BVCTelephony
(
)
On this page
Overview
Supported platforms
Usage instructions


Content from https://docs.livekit.io/home/self-hosting/local:

On this page
Install LiveKit Server
Start the server in dev mode
Copy page
See more page options
Install LiveKit Server
macOS
Linux
Windows
brew update && brew install livekit
Start the server in dev mode
You can start LiveKit in development mode by running:
livekit-server --dev
This will start an instance using the following API key/secret pair:
API key: devkey
API secret: secret
To customize your setup for production, refer to our
deployment guides
.
Tip
By default LiveKit's signal server binds to
127.0.0.1:7880
. If you'd like to access it from other devices on your network, pass in
--bind 0.0.0.0
On this page
Install LiveKit Server
Start the server in dev mode


Content from https://docs.livekit.io/home/self-hosting/deployment:

On this page
Domain, SSL certificates, and load balancer
Improving connectivity with TURN
TURN/TLS
TURN/UDP
Configuration
Resources
Copy page
See more page options
Domain, SSL certificates, and load balancer
In order to have a secure LiveKit deployment, you will need a domain as well as a SSL certificate for that domain. This domain will be used as the primary endpoint for LiveKit SDKs, for example:
wss://livekit.yourhost.com
. The SSL certificate must be signed by a trusted certificate authority; self-signed certs do not work here.
You will also need to set up HTTPS/SSL termination with a load balancer or reverse proxy.
If you are using TURN, then a separate TURN domain and SSL cert will be needed, as well.
Improving connectivity with TURN
Certain corporate firewalls block not only UDP traffic, but non-secure TCP traffic, as well. In those cases, it's helpful to use a TURN server.
Here's
a good resource if you're interested in reading more about how TURN is used.
The good news is LiveKit includes an embedded TURN server. It's a secure TURN implementation that has integrated authentication with the rest of LiveKit. The authentication layer ensures that only clients that have already established a signal connection could connect to our TURN server.
TURN/TLS
To firewalls, TLS traffic looks no different from regular HTTPS traffic to websites. Enabling TURN/TLS gives you the broadest coverage in client connectivity, including those behind corporate firewalls. TURN/TLS can be enabled with:
turn
:
enabled
:
true
tls_port
:
5349
domain
:
turn.myhost.com
cert_file
:
/path/to/turn.crt
key_file
:
/path/to/turn.key
LiveKit will perform TLS termination, so you will have to specify the certificates in the config. When running multiple LiveKit instances, you can place a layer 4 load balancer in front of the TCP port.
If you are not using a load balancer,
turn.tls_port
needs to be set to 443, as that will be the port that's advertised to clients.
TURN/UDP
As QUIC (HTTP/3) gains adoption, some firewalls started allowing UDP traffic to pass through port 443. In those cases, it helps to use TURN/UDP on port 443. UDP is preferred over TCP for WebRTC traffic, as it has better control over congestion and latency. TURN/UDP can be enabled with:
turn
:
enabled
:
true
udp_port
:
443
Configuration
For production deploys, we recommend using a config file. The config file can be passed in via
--config
flag, or the body of the YAML can be set with a
LIVEKIT_CONFIG
environment variable.
Below is a recommended config for a production deploy. To view other customization options, see
config-sample.yaml
port
:
7880
log_level
:
info
rtc
:
tcp_port
:
7881
port_range_start
:
50000
port_range_end
:
60000
# use_external_ip should be set to true for most cloud environments where
# the host has a public IP address, but is not exposed to the process.
# LiveKit will attempt to use STUN to discover the true IP, and advertise
# that IP with its clients
use_external_ip
:
true
redis
:
# redis is recommended for production deploys
address
:
my
-
redis
-
server.name
:
6379
keys
:
# key-value pairs
# your_api_key: <your_api_secret>
# When enabled, LiveKit will expose prometheus metrics on :6789/metrics
#prometheus_port: 6789
turn
:
enabled
:
true
# domain must match tls certificate
domain
:
<turn.myhost.com
>
# defaults to 3478. If not using a load balancer, must be set to 443.
tls_port
:
3478
Resources
The scalability of LiveKit is bound by CPU and bandwidth. We recommend running production setups on 10Gbps ethernet or faster.
When deploying to cloud providers, compute-optimized instance types are the most suitable for LiveKit.
If running in a Dockerized environment, host networking should be used for optimal performance.
On this page
Domain, SSL certificates, and load balancer
Improving connectivity with TURN
TURN/TLS
TURN/UDP
Configuration
Resources


Content from https://docs.livekit.io/home/self-hosting/vm:

On this page
Pre-requisites
Generate configuration
Deploy to a VM
Firewall
DNS
Upgrading
Troubleshooting
Checking TLS certificates
Ensure DNS is pointed at your domain
Instance started before networking
Instance firewall
Copy page
See more page options
This configuration utilizes Docker Compose and Caddy. Your LiveKit server will support a broad spectrum of connectivity options, including those behind VPN and firewalls (via TURN/TLS)
You do not need separate SSL certificates for this set up, we will provision them automatically with Caddy. (by using Let's Encrypt or ZeroSSL)
If desired, the generator can also assist with setting up LiveKit
Ingress
and
Egress
. This gives you the ability to ingest media from other sources, as well as enabling recording capabilities.
Pre-requisites
To start, you'll need:
A domain that you own
The ability to add DNS records for subdomains for your new LiveKit server
Generate configuration
Use our configuration generation tool to create a customized configuration for your domain. This script should be run on your development machine:
docker
pull livekit/generate
docker
run
--rm
-it
-v
$PWD
:/output livekit/generate
It creates a folder with the name of domain you provided, containing the following files:
caddy.yaml
docker-compose.yaml
livekit.yaml
redis.conf
init_script.sh
OR
cloud_init.xxxx.yaml
Deploy to a VM
Depending on your cloud provider, there are a couple of options:
Cloud Init
Startup Script
This is the easiest method for deploying LiveKit Server. AWS, Azure, Digital Ocean, and others support
cloud-init
.
We have tested our scripts on Ubuntu and Amazon Linux, but it's possible the same scripts may work on other platforms.
(Please let us know in Slack or open a PR!)
When starting a VM, paste the contents of the file
cloud-init.xxxx.yaml
into the
User data
field.
That's it! When the machine starts up, it'll execute the cloud-init protocol and install LiveKit.
When the install script is finished, your instance should be set up. It will have installed:
docker
docker-compose
generated configuration to
/opt/livekit
systemd service
livekit-docker
To start/stop the service via systemctl:
systemctl stop livekit-docker
systemctl start livekit-docker
Firewall
Ensure that the following ports are open on your firewall and accessible on the instance:
443
- primary HTTPS and TURN/TLS
80
- TLS issuance
7881
- WebRTC over TCP
3478/UDP
- TURN/UDP
50000-60000/UDP
- WebRTC over UDP
And if Ingress is desired
1935
- RTMP Ingress
7885/UDP
- WebRTC for WHIP Ingress
DNS
Both primary and TURN domains must point to the IP address of your instance.
This is required for Caddy to start provisioning your TLS certificates.
Upgrading
To upgrade your install to new LiveKit releases, edit the docker compose file:
/opt/livekit/docker-compose.yaml
Change the image field under
livekit
to
livekit/livekit-server:v<version>
Alternatively, to always run the latest version, set the image field to
livekit/livekit-server:latest
and run:
docker
pull livekit/livekit-server
Troubleshooting
If something is not working as expected, SSH in to your server and use the following commands to investigate:
systemctl status livekit-docker
cd
/opt/livekit
sudo
docker-compose
logs
Checking TLS certificates
If certificate acquisition process has been successful, you should see the following log entry:
livekit-caddy-1
|
{
"level"
:
"info"
,
"ts"
:1642786068.3883107,
"logger"
:
"tls.obtain"
,
"msg"
:
"certificate obtained successfully"
,
"identifier"
:
"<yourhost>"
}
If you don't see these messages, it means your server could not be reached from the internet.
Ensure DNS is pointed at your domain
Running
host <yourdomain>
should show the IP address of your server. Ensure that it matches the IP address of your server.
Instance started before networking
When using cloud-init, it's possible that the instance started up before networking was available to the machine. This is commonly the case on EC2 instances. When this happens, your cloud-init scripts will be stuck in a bad state. To fix this, you can SSH into the machine and trigger a re-run:
sudo
cloud-init clean
--logs
sudo
reboot
now
Instance firewall
Certain Linux distributions ship with an instance-specific firewall enabled. To check if this is the case, run:
sudo
firewall-cmd --list-all
If firewall is enabled, you could add the following rules to it and restart the firewall:
sudo
firewall-cmd
--zone
public
--permanent
--add-port
80
/tcp
sudo
firewall-cmd
--zone
public
--permanent
--add-port
443
/tcp
sudo
firewall-cmd
--zone
public
--permanent
--add-port
7881
/tcp
sudo
firewall-cmd
--zone
public
--permanent
--add-port
443
/udp
sudo
firewall-cmd
--zone
public
--permanent
--add-port
50000
-60000/udp
sudo
firewall-cmd
--reload
When the ports are successfully opened, running
curl http://<yourdomain>
should return a 404 response. (instead of hanging)
On this page
Pre-requisites
Generate configuration
Deploy to a VM
Firewall
DNS
Upgrading
Troubleshooting
Checking TLS certificates
Ensure DNS is pointed at your domain
Instance started before networking
Instance firewall


Content from https://docs.livekit.io/home/self-hosting/kubernetes:

On this page
Understanding the deployment
Graceful restarts
Using the Helm Chart
Pre-requisites
Importing SSL Certificates
Install & Upgrade
Firewall
Copy page
See more page options
LiveKit streamlines deployment to Kubernetes. We publish a
Helm chart
that help you set up a distributed deployment of LiveKit, along with a Service and Ingress to correctly route traffic. Our Helm chart supports Google GKE, Amazon EKS, and Digital Ocean DOKS out of the box, and can serve as a guide on your custom Kubernetes installations.
Important
LiveKit does not support deployment to serverless and/or private clusters. Private clusters have additional layers of NAT that make it unsuitable for WebRTC traffic.
Understanding the deployment
LiveKit pods requires direct access to the network with host networking. This means that the rtc.udp/tcp ports that are open on those nodes are directly handled by LiveKit server. With that direct requirement of specific ports, it means we'll be limited to one LiveKit pod per node. It's possible to run other workload on those nodes.
Termination of TLS/SSL is left as a responsibility of the Ingress. Our Helm chart will configure TLS termination for GKE and ALB load balancers. To use ALB on EKS, AWS Load Balancer Controller needs to be
installed separately
.
Graceful restarts
During an upgrade deployment, older pods will need to be terminated. This could be extremely disruptive if there are active sessions running on those pods. LiveKit handles this by allowing that instance to drain prior to shutting down.
We also set
terminationGracePeriodSeconds
to 5 hours in the helm chart, ensuring Kubernetes gives sufficient time for the pod to gracefully shut down.
Using the Helm Chart
Pre-requisites
To deploy a multi-node cluster that autoscales, you'll need:
a Redis instance
SSL certificates for primary domain and TURN/TLS
a Kubernetes cluster on AWS, GCloud, or DO
Helm
is installed on your machine.
Then add the LiveKit repo
$ helm repo
add
livekit https://helm.livekit.io
Depending on your cloud provider, the following pre-requisites may be required
AWS
Digital Ocean
On AWS, it's recommended to use ALB Ingress Controller as the main load balancer for LiveKit's signal connection. You can find installation instructions
here
.
With ALB, you could also used ACM to handle TLS termination for the primary domain. However, a SSL certificate is still needed in order to use the embedded TURN/TLS server.
Create a values.yaml for your deployment, using
server-sample.yaml
as a template.
Checkout
Helm examples
for AWS, Google Cloud, and Digital Ocean.
Importing SSL Certificates
In order to set up TURN/TLS and HTTPS on the load balancer, you may need to import your SSL certificate(s) into as a Kubernetes Secret. This can be done with:
kubectl create secret tls
<
NAME
>
--cert
<
CERT-FILE
>
--key
<
KEY-FILE
>
--namespace
<
NAMESPACE
>
Note, please ensure that the secret is created in the same namespace as the deployment.
Install & Upgrade
helm
install
<
INSTANCE_NAME
>
livekit/livekit-server
--namespace
<
NAMESPACE
>
--values
values.yaml
We'll publish new version of the chart with new server releases. To fetch these updates and upgrade your installation, perform
helm repo update
helm upgrade
<
INSTANCE_NAME
>
livekit/livekit-server
--namespace
<
NAMESPACE
>
--values
values.yaml
If any configuration has changed, you may need to trigger a restart of the deployment. Kubernetes triggers a restart only when the pod itself has changed, but does not when the changes took place in the ConfigMap.
Firewall
Ensure that your
firewall
is configured properly to allow traffic into LiveKit ports.
On this page
Understanding the deployment
Graceful restarts
Using the Helm Chart
Pre-requisites
Importing SSL Certificates
Install & Upgrade
Firewall


Content from https://docs.livekit.io/home/self-hosting/distributed:

On this page
Multi-node routing
Downscaling and draining
Multi-region support
Configuration
Copy page
See more page options
Multi-node routing
When Redis is configured, LiveKit automatically switches to a distributed setup by using Redis for room data as well as a message bus. In this mode, each node periodically reports their stats to Redis; this enables them to be aware of the entire cluster and make routing decisions based on availability and load. We recommend this setup for a redundant deployment.
When a new room is created, the node that received this request is able to choose an available node from the cluster to host the room.
When a client establishes a signal connection to LiveKit, it creates a persistent WebSocket connection with one of the instances. That instance will then acts as a signaling bridge, proxying messages between the node where the room is hosted and the client.
In a multi-node setup, LiveKit can support a large number of concurrent rooms. However, there are limits to the number of participants in a room since, for now, a room must fit on a single node.
Downscaling and draining
It's simple to scale up instances, but what about scaling down? Terminating an instance while it's hosting active sessions would be extremely disruptive to the end user.
LiveKit solves this problem by providing connection draining natively. When it receives a request to terminate (via
SIGTERM
,
SIGINT
, or
SIGQUIT
) and there are participants currently connected, it will put itself into draining mode. While draining, the instance would:
allow active rooms to run as usual
accept traffic for new participants to active rooms
reject participants trying to join new rooms
When all participants have disconnected, the server will complete draining and shut down.
Multi-region support
It's possible to deploy LiveKit to multiple data centers, allowing users located in different regions to connect to a server that's closest to them.
LiveKit supports this via a
region-aware, load aware node selector
. It's designed to be used in conjunction with region-aware load balancing of the signal connection.
Here's how it works:
Geo or latency aware DNS service (such as Route53 or Cloudflare) returns IP of load balancer closest to the user
User connects load balancer in that region
Then connects to an instance of LiveKit in that region
If the room doesn't already exist, LiveKit will use node selector to choose an available node
The selection criteria is
node must have lower utilization than
sysload_limit
nodes are in the region closest to the signaling instance
a node satisfying the above is chosen at random
Configuration
node_selector
:
kind
:
regionaware
sysload_limit
:
0.5
# List of regions and their lat/lon coordinates
regions
:
-
name
:
us
-
west
-
2
lat
:
37.64046607830567
lon
:
-120.88026233189062
-
name
:
us
-
east
lat
:
40.68914362140307
lon
:
-74.04445748616385
On this page
Multi-node routing
Downscaling and draining
Multi-region support
Configuration


Content from https://docs.livekit.io/home/self-hosting/ports-firewall:

On this page
Ports
Firewall
Copy page
See more page options
Ports
LiveKit uses several ports to communicate with clients. Exposed ports below need to be open on the firewall.
Port
Default
Config
Exposed
Description
API, WebSocket
7880
port
no
This port should be placed behind a load balancer that can terminate SSL. LiveKit APIs are homogenous: any client could connect to any backend instance, regardless of the room they are in.
ICE/UDP
50000-60000
rtc.port_range_start
,
rtc.port_range_end
yes
LiveKit advertises these ports as WebRTC host candidates (each participant in the room will use two ports)
ICE/TCP
7881
rtc.tcp_port
yes
Used when the client could not connect via UDP (e.g. VPN, corporate firewalls)
ICE/UDP Mux
7882
rtc.udp_port
yes
(optional) It's possible to handle all UDP traffic on a single port. When this is set, rtc.port_range_start/end are not used
TURN/TLS
5349
turn.tls_port
when not using LB
(optional) For a distributed setup, use a network load balancer in front of the port. If not using LB, this port needs to be set to 443.
TURN/UDP
3478
turn.udp_port
yes
(optional) To use the embedded TURN/UDP server. When enabled, it also serves as a STUN server.
SIP/UDP
5060
sip_port
yes
(optional) UDP signaling port for LiveKit SIP. Available in
sip/config.yml
.
SIP/TCP
5060
sip_port
yes
(optional) TCP signaling port for LiveKit SIP. Available in
sip/config.yml
.
SIP/TLS
5061
tls.port
yes
(optional) TLS signaling port for LiveKit SIP. Available in
sip/config.yml
.
SIP RTP/UDP
10000-20000
rtp_port
yes
(optional) RTP media port range for LiveKit SIP. Available in
sip/config.yml
.
Firewall
When hosting in cloud environments, the ports configured above will have to be opened in the firewall.
AWS
Digital Ocean
Google Cloud
Navigate to the VPC dashboard, choose
Security Groups
, and select the security group that LiveKit is deployed to.
Open the
Inbound rules
tab and select
Edit Inbound Rules
Then add the following rules (assuming use of default ports):
On this page
Ports
Firewall


Content from https://docs.livekit.io/home/self-hosting/benchmark:

On this page
Measuring performance
Load testing
Benchmarks
Audio only
Video room
Copy page
See more page options
Measuring performance
LiveKit can scale to many simulteneous rooms by running a distributed setup across multiple nodes. However, each room must fit within a single node. For this reason, benchmarks below will be focused on stressing the number of concurrent users in a room.
With WebRTC SFUs, a few factors determine the amount of work a server could perform:
Number of tracks published
Number of subscribers
Amount of data sent to each subscriber
An SFU needs to perform work to receive every track - this means receiving tens of packets per second. It then needs to forward that received data to every subscriber. That adds up to a significant amount of work in decryption and encryption, packet processing, and data forwarding.
Due to these variations, it can be difficult to understand the capacity of the SFU for an specific application. We provide tooling that help with simulating workload according to your specifications.
Load testing
The LiveKit
CLI
includes the
lk load-test
subcommand, which can simulate real-world loading conditions for various scenarios. It uses the Go SDK to simulate publishers and subscribers in a room.
When publishing, it could send both video and audio tracks:
video: looping video clips at 720p, with keyframes every ~3s (simulcast enabled)
audio: sends blank packets that aren't audible, but would simulate a target bitrate.
As a subscriber, it can simulate an application that takes advantage of adaptive stream, rendering a specified number of remote streams on-screen.
When benchmarking with the load tester, be sure to run it on a machine with plenty of CPU and bandwidth, and ensure it has sufficient file handles (
ulimit -n 65535
). You can also run the load tester from multiple machines.
Caution
Load testing traffic on your cloud instance
will
count toward your
quotas
, and is subject to the limits of your plan.
Benchmarks
We've run benchmarks for a few common scenarios to give a general understanding of performance. All benchmarks below are to demonstrate max number of participants supported in a single room.
All benchmarks were ran with the server running on a 16-core, compute optimized instance on Google Cloud. (
c2-standard-16
)
In the tables below:
Pubs
- Number of publishers
Subs
- Number of subscribers
Audio only
This simulates an audio only experience with a large number of listeners in the room. It uses an average
audio bitrate of 3kbps. In large audio sessions, only a small number of people are usually speaking (while everyone are on mute). We use 10 as the approximate number of speakers here.
Use case
Pubs
Subs
Bytes/s in/out
Packets/s in/out
CPU utilization
Large audio rooms
10
3000
7.3 kBps / 23 MBps
305 / 959,156
80%
Command:
lk load-test
\
--url
<
YOUR-SERVER-URL
>
\
--api-key
<
YOUR-KEY
>
\
--api-secret
<
YOUR-SECRET
>
\
--room
load-test
\
--audio-publishers
10
\
--subscribers
1000
Video room
Default video resolution of 720p was used in the load tests.
Use case
Pubs
Subs
Bytes/s in/out
Packets/s in/out
CPU utilization
Large meeting
150
150
50 MBps / 93 MBps
51,068 / 762,749
85%
Livestreaming
1
3000
233 kBps / 531 MBps
246 / 560,962
92%
To simulate large meeting:
lk load-test
\
--url
<
YOUR-SERVER-URL
>
\
--api-key
<
YOUR-KEY
>
\
--api-secret
<
YOUR-SECRET
>
\
--room
load-test
\
--video-publishers
150
\
--subscribers
150
To simulate livestreaming:
lk load-test
\
--url
<
YOUR-SERVER-URL
>
\
--api-key
<
YOUR-KEY
>
\
--api-secret
<
YOUR-SECRET
>
\
--room
load-test
\
--video-publishers
1
\
--subscribers
3000
\
On this page
Measuring performance
Load testing
Benchmarks
Audio only
Video room


Content from https://docs.livekit.io/home/self-hosting/egress:

On this page
Requirements
Config
Running locally
Helm
Ensuring availability
Autoscaling with Helm
Chrome sandboxing
Copy page
See more page options
When multiple Egress workers are deployed, they will automatically load-balance and ensure requests are distributed across worker instances.
Requirements
Certain kinds of Egress operations can be resource-intensive. We recommend giving each Egress instance at least
4 CPUs
and
4 GB
of memory.
An Egress worker may process one or more jobs at once, depending on their resource requirements. For example, a TrackEgress job consumes minimal resources because it doesn't need to transcode. Consequently, hundreds of simulteneous TrackEgress jobs can run on a single instance.
Note
As of
v1.7.6
, chrome sandboxing is enabled for increased security.
As a result, the service is no longer run as the
root
user inside docker, and all Egress deployments (even local) require
--cap-add=SYS_ADMIN
in your
docker run
command.
Without it, all web and room composite egress will fail with a
chrome failed to start
error.
Config
The Egress service takes a yaml config file:
# Required fields
api_key
:
livekit server api key. LIVEKIT_API_KEY env can be used instead
api_secret
:
livekit server api secret. LIVEKIT_API_SECRET env can be used instead
ws_url
:
livekit server websocket url. LIVEKIT_WS_URL can be used instead
redis
:
address
:
must be the same redis address used by your livekit server
username
:
redis username
password
:
redis password
db
:
redis db
# Optional fields
health_port
:
if used
,
will open an http port for health checks
template_port
:
port used to host default templates (default 7980)
prometheus_port
:
port used to collect prometheus metrics. Used for autoscaling
log_level
:
debug
,
info
,
warn
,
or error (default info)
template_base
:
can be used to host custom templates (default http
:
//localhost
:
<template_port
>
/)
enable_chrome_sandbox
:
if true
,
egress will run Chrome with sandboxing enabled. This requires a specific Docker setup
,
see below.
insecure
:
can be used to connect to an insecure websocket (default false)
# File upload config - only one of the following. Can be overridden per-request
s3
:
access_key
:
AWS_ACCESS_KEY_ID env can be used instead
secret
:
AWS_SECRET_ACCESS_KEY env can be used instead
region
:
AWS_DEFAULT_REGION env can be used instead
endpoint
:
optional custom endpoint
bucket
:
bucket to upload files to
azure
:
account_name
:
AZURE_STORAGE_ACCOUNT env can be used instead
account_key
:
AZURE_STORAGE_KEY env can be used instead
container_name
:
container to upload files to
gcp
:
credentials_json
:
GOOGLE_APPLICATION_CREDENTIALS env can be used instead
bucket
:
bucket to upload files to
The config file can be added to a mounted volume with its location passed in the EGRESS_CONFIG_FILE env var, or its body can be passed in the EGRESS_CONFIG_BODY env var.
Running locally
These changes are
not
recommended for a production setup.
To run against a local livekit server, you'll need to do the following:
open
/usr/local/etc/redis.conf
and comment out the line that says
bind 127.0.0.1
change
protected-mode yes
to
protected-mode no
in the same file
find your IP as seen by docker
ws_url
needs to be set using the IP as Docker sees it
on linux, this should be
172.17.0.1
on mac or windows, run
docker run -it --rm alpine nslookup host.docker.internal
and you should see something like
Name:	host.docker.internal Address: 192.168.65.2
These changes allow the service to connect to your local redis instance from inside the docker container.
Create a directory to mount. In this example, we will use
~/livekit-egress
.
Create a config.yaml in the above directory.
redis
and
ws_url
should use the above IP instead of
localhost
insecure
should be set to true
log_level
:
debug
api_key
:
your
-
api
-
key
api_secret
:
your
-
api
-
secret
ws_url
:
ws
:
//192.168.65.2
:
7880
insecure
:
true
redis
:
address
:
192.168.65.2
:
6379
Then to run the service:
docker
run
--rm
\
--cap-add SYS_ADMIN
\
-e
EGRESS_CONFIG_FILE
=
/out/config.yaml
\
-v
~/livekit-egress:/out
\
livekit/egress
You can then use our
CLI
to submit recording requests to your server.
Helm
If you already deployed the server using our Helm chart, jump to
helm install
below.
Ensure
Helm
is installed on your machine.
Add the LiveKit repo
helm repo
add
livekit https://helm.livekit.io
Create a values.yaml for your deployment, using
egress-sample.yaml
as a template.
Each instance can record one room at a time, so be sure to either enable autoscaling, or set replicaCount >= the number of rooms you'll need to simultaneously record.
Then install the chart
helm
install
<
INSTANCE_NAME
>
livekit/egress
--namespace
<
NAMESPACE
>
--values
values.yaml
We'll publish new version of the chart with new egress releases. To fetch these updates and upgrade your installation, perform
helm repo update
helm upgrade
<
INSTANCE_NAME
>
livekit/egress
--namespace
<
NAMESPACE
>
--values
values.yaml
Ensuring availability
Room Composite egress can use anywhere between 2-6 CPUs. For this reason, it is recommended to use
pods with 4 CPUs if you will be using room composite egress.
The
livekit_egress_available
Prometheus metric is also provided to support autoscaling.
prometheus_port
must be defined in your config.
With this metric, each instance looks at its own CPU utilization and decides whether it is available to accept incoming requests.
This can be more accurate than using average CPU or memory utilization, because requests are long-running and are resource intensive.
To keep at least 3 instances available:
sum
(
livekit_egress_available
)
>
3
To keep at least 30% of your egress instances available:
sum
(
livekit_egress_available
)
/
sum
(
kube_pod_labels
{
label_project
=
~
"^.*egress.*"
}
)
>
0.3
Autoscaling with Helm
There are 3 options for autoscaling:
targetCPUUtilizationPercentage
,
targetMemoryUtilizationPercentage
, and
custom
.
autoscaling
:
enabled
:
false
minReplicas
:
1
maxReplicas
:
5
#  targetCPUUtilizationPercentage: 60
#  targetMemoryUtilizationPercentage: 60
#  custom:
#    metricName: my_metric_name
#    targetAverageValue: 70
To use
custom
, you'll need to install the prometheus adapter. You can then create a kubernetes custom metric based off the
livekit_egress_available
prometheus metric.
Chrome sandboxing
By default, Room Composite and Web egresses run with Chrome sandboxing disabled. This is because the default docker security settings prevent Chrome from
switching to a different kernel namespace, which is needed by Chrome to setup its sandbox.
Chrome sandboxing within Egress can be reenabled by setting the
enable_chrome_sandbox
option to
true
in the egress configuration, and launching docker using the
provided
seccomp security profile
:
docker
run
--rm
\
-e
EGRESS_CONFIG_FILE
=
/out/config.yaml
\
-v
~/egress-test:/out
\
--security-opt
seccomp
=
chrome-sandboxing-seccomp-profile.json
\
livekit/egress
This profile is based on the
default docker seccomp security profile
and allows
the 2 extra system calls (
clone
and
unshare
) that Chrome needs to setup the sandbox.
Note that kubernetes disables seccomp entirely by default, which means that running with Chrome sandboxing enabled is possible on a kubernetes cluster with
the default security settings.
On this page
Requirements
Config
Running locally
Helm
Ensuring availability
Autoscaling with Helm
Chrome sandboxing


Content from https://docs.livekit.io/home/self-hosting/ingress:

On this page
Requirements
Config
Ingress Service
LiveKit Server
Health checks
Running natively on a host
Prerequisites
Configuration
Running the service
Running with Docker
Helm
Ensuring availability
Autoscaling with Helm
Copy page
See more page options
Requirements
If more than one Ingress worker is needed, the service must be setup behind a TCP load balancer (or HTTP reverse proxy for WHIP) to assign an incoming RTMP or WHIP request to an available instance. The load balancer is also responsible for TLS termination and performing Ingress node health checks.
Certain kinds of Ingress operations can be resource-intensive. We recommend giving each Ingress instance at least
4 CPUs
and
4 GB
of memory.
If WHIP support is enabled, the instance will also need access to UDP port 7885.
An Ingress worker may process one or more jobs at once, depending on their resource requirements. For example, a WHIP session with transcoding bypassed consumes minimal resources. For Ingress with transcoding enabled, such as RTMP or WHIP with transcoding bypass disabled, the amount of required resources depend on the video resolution and amount of video layers configured in the Ingress video settings.
Config
Ingress Service
The Ingress service takes a yaml config file:
# Required fields
api_key
:
livekit server api key. LIVEKIT_API_KEY env can be used instead
api_secret
:
livekit server api secret. LIVEKIT_API_SECRET env can be used instead
ws_url
:
livekit server websocket url. LIVEKIT_WS_URL can be used instead
redis
:
address
:
must be the same redis address used by your livekit server
username
:
redis username
password
:
redis password
db
:
redis db
# Optional fields
health_port
:
if used
,
will open an http port for health checks
prometheus_port
:
port used to collect Prometheus metrics. Used for autoscaling
rtmp_port
:
TCP port to listen for RTMP connections on (default 1935)
whip_port
:
TCP port to listen for WHIP connections on (default 8080)
http_relay_port
:
TCP port for communication between the main service process and session handler processes
,
on localhost (default 9090)
logging
:
level
:
debug
,
info
,
warn
,
or error (default info)
rtc_config
:
tcp_port
:
TCP port to use for ICE connections on (default disabled)
udp_port
:
UDP port to use for ICE connections on (default 7885)
use_external_ip
:
whether to use advertise the server public facing IP address for ICE connections
# use_external_ip should be set to true for most cloud environments where
# the host has a public IP address, but is not exposed to the process.
# LiveKit will attempt to use STUN to discover the true IP, and convenient
# that IP with its clients
cpu_cost
:
rtmp_cpu_cost
:
cpu resources to reserve when accepting RTMP sessions
,
in fraction of core count
whip_cpu_cost
:
cpu resources to reserve when accepting WHIP sessions
,
in fraction of core count
whip_bypass_transcoding_cpu_cost
:
cpu resources to reserve when accepting WHIP sessions with transcoding disabled
,
in fraction of core count
The location of the config file can be passed in the INGRESS_CONFIG_FILE env var, or its body can be passed in the INGRESS_CONFIG_BODY env var.
LiveKit Server
LiveKit Server serves as the API endpoint for the CreateIngress API calls.
Therefore, it needs to know the location of the Ingress service to provide the
Ingress URL to clients.
To achieve this, include the following in the LiveKit Server's configuration:
ingress
:
rtmp_base_url
:
'rtmps://my.domain.com/live'
whip_base_url
:
'https://my.domain.com/whip'
Health checks
The Ingress service provides HTTP endpoints for both health and availability checks. The heath check endpoint will always return a 200 status code if the Ingress service is running. The availability endpoint will only return 200 if the server load is low enough that a new request with the maximum cost, as defined in the
cpu_cost
section of the configuration file, can still be handled.
Health and availability check endpoints are exposed in 2 different ways:
A dedicated HTTP server that can be enabled by setting the
health_port
configuration entry. The health check endpoint is running at the root of the HTTP server (
/
), while the availability endpoint is available at
/availability
If enabled, the WHIP server also exposes a heath check endpoint at
/health
and an availability endpoint at
/availability
Running natively on a host
This documents how to run the Ingress service natively on a host server. This setup is convenient for testing and development, but not advised in production.
Prerequisites
The Ingress service can be run natively on any platform supported by GStreamer.
The Ingress service is built in Go. Go >= 1.18 is needed. The following
GStreamer
libraries and headers must be installed:
gstreamer
gst-plugins-base
gst-plugins-good
gst-plugins-bad
gst-plugins-ugly
gst-libav
On MacOS, these can be installed using
Homebrew
by running
mage bootstrap
.
In order to run Ingress against a local LiveKit server, a Redis server must be running on the host.
Building
Build the Ingress service by running:
mage build
Configuration
All servers must be configured to communicate over localhost. Create a file named
config.yaml
with the following content:
logging
:
level
:
debug
api_key
:
<YOUR_API_KEY
>
api_secret
:
<YOUR_API_SECRET
>
ws_url
:
ws
:
//localhost
:
7880
redis
:
address
:
localhost
:
6379
Running the service
On MacOS, if GStreamer was installed using Homebrew, the following environment must be set:
export
GST_PLUGIN_PATH
=
/opt/homebrew/Cellar/gst-plugins-base:/opt/homebrew/Cellar/gst-plugins-good:/opt/homebrew/Cellar/gst-plugins-bad:/opt/homebrew/Cellar/gst-plugins-ugly:/opt/homebrew/Cellar/gst-plugins-bad:/opt/homebrew/Cellar/gst-libav
Then to run the service:
ingress
--config
config.yaml
Running with Docker
To run against a local LiveKit server, a Redis server must be running locally. The Ingress service must be instructed to connect to LiveKit server and Redis on the host. The host network is accessible from within the container on IP:
host.docker.internal on MacOS and Windows
172.17.0.1 on linux
Create a file named
config.yaml
with the following content:
log_level
:
debug
api_key
:
<YOUR_API_KEY
>
api_secret
:
<YOUR_API_SECRET
>
ws_url
:
ws
:
//host.docker.internal
:
7880 (or ws
:
//172.17.0.1
:
7880 on linux)
redis
:
address
:
host.docker.internal
:
6379 (or 172.17.0.1
:
6379 on linux)
In order to be able to use establish WHIP sessions over UDP, the container must be run with host networking enabled.
Then to run the service:
docker
run
--rm
\
-e
INGRESS_CONFIG_BODY
=
"
`
cat
config.yaml
`
"
\
-p
1935
:1935
\
-p
8080
:8080
\
--network
host
\
livekit/ingress
Helm
If you already deployed the server using our Helm chart, jump to
helm install
below.
Ensure
Helm
is installed on your machine.
Add the LiveKit repo
helm repo
add
livekit https://helm.livekit.io
Create a values.yaml for your deployment, using
ingress-sample.yaml
as a template.
Each instance can handle a few transcoding-enabled Ingress at a time, so be sure to either enable autoscaling, or set replicaCount accordingly.
Then install the chart
helm
install
<
INSTANCE_NAME
>
livekit/ingress
--namespace
<
NAMESPACE
>
--values
values.yaml
We'll publish new version of the chart with new Ingress releases. To fetch these updates and upgrade your installation, perform
helm repo update
helm upgrade
<
INSTANCE_NAME
>
livekit/ingress
--namespace
<
NAMESPACE
>
--values
values.yaml
Ensuring availability
An Ingress with transcoding enabled can use anywhere between 2-6 CPU cores. For this reason, it is recommended to use pods with 4 CPUs if you will be transcoding incoming media.
The
livekit_ingress_available
Prometheus metric is also provided to support autoscaling.
prometheus_port
must be defined in your config.
With this metric, each instance looks at its own CPU utilization and decides whether it is available to accept incoming requests.
This can be more accurate than using average CPU or memory utilization, because requests are long-running and are resource intensive.
To keep at least 3 instances available:
sum
(
livekit_ingress_available
)
>
3
To keep at least 30% of your Ingress instances available:
sum
(
livekit_ingress_available
)
/
sum
(
kube_pod_labels
{
label_project
=
~
"^.*ingress.*"
}
)
>
0.3
Autoscaling with Helm
There are 3 options for autoscaling:
targetCPUUtilizationPercentage
,
targetMemoryUtilizationPercentage
, and
custom
.
autoscaling
:
enabled
:
false
minReplicas
:
1
maxReplicas
:
5
#  targetCPUUtilizationPercentage: 60
#  targetMemoryUtilizationPercentage: 60
#  custom:
#    metricName: my_metric_name
#    targetAverageValue: 70
To use
custom
, you'll need to install the Prometheus adapter. You can then create a Kubernetes custom metric based off the
livekit_ingress_available
Prometheus metric.
You can find an example on how to do this
here
.
On this page
Requirements
Config
Ingress Service
LiveKit Server
Health checks
Running natively on a host
Prerequisites
Configuration
Running the service
Running with Docker
Helm
Ensuring availability
Autoscaling with Helm


Content from https://docs.livekit.io/home/self-hosting/sip-server:

On this page
Docker compose
Running natively
1. Install SIP server
2. Create config file
3. Run SIP server:
4. Determine your SIP URI
Copy page
See more page options
Caution
Both SIP signaling port (
5060
) and media port range (
10000-20000
) must be accessible from the Internet.
See
Firewall configuration
for details.
Docker compose
The easiest way to run SIP Server is by using Docker Compose:
wget
https://raw.githubusercontent.com/livekit/sip/main/docker-compose.yaml
docker
compose up
This starts a local LiveKit Server and SIP Server connected to Redis.
Running natively
You may also run SIP server natively without Docker.
1. Install SIP server
Follow instructions
here
.
2. Create config file
Create a file named
config.yaml
with the following content:
api_key
:
<your
-
api
-
key
>
api_secret
:
<your
-
api
-
secret
>
ws_url
:
ws
:
//localhost
:
7880
redis
:
address
:
localhost
:
6379
sip_port
:
5060
rtp_port
:
10000
-
20000
use_external_ip
:
true
logging
:
level
:
debug
3. Run SIP server:
livekit-sip
--config
=
config.yaml
4. Determine your SIP URI
Once your SIP server is running, you would have to determine the publilc IP address of the machine.
Then your SIP URI would be:
<
public-ip-address
>
:5060
On this page
Docker compose
Running natively
1. Install SIP server
2. Create config file
3. Run SIP server:
4. Determine your SIP URI


Content from https://docs.livekit.io/sip/quickstarts/configuring-sip-trunk:

On this page
Overview
External provider setup
SIP endpoint
Provider-specific instructions
LiveKit setup
Inbound trunk setup
Create a dispatch rule
Create an outbound trunk
Next steps
Additional documentation
Copy page
See more page options
Overview
LiveKit is compatible with any SIP trunking provider. This guide provides general instructions for setting up a SIP trunk with an external provider and then associating it with your LiveKit project.
External provider setup
The usual steps to create a SIP trunk are as follows:
Create a SIP trunk with your provider.
Add authentication or limit trunk usage by phone numbers or IP addresses.
Purchase a phone number and associate it with your SIP trunk.
Add your
LiveKit SIP endpoint
to the SIP trunk.
SIP endpoint
Depending on your SIP trunking provider, you might need to use a
SIP endpoint
to configure inbound calls instead of your SIP URI. The SIP endpoint is your LiveKit SIP URI without the
sip:
prefix. You can find your SIP URI on the
Project settings
page.
For example, if your SIP URI is
sip:vjnxecm0tjk.sip.livekit.cloud
, your SIP endpoint is
vjnxecm0tjk.sip.livekit.cloud
.
Region-based endpoints
To restrict calls to a specific region, replace your global LiveKit SIP endpoint with a
region-based endpoint
.
Provider-specific instructions
For step-by-step instructions for Telnyx, Twilio, or Plivo, see the following quickstarts:
Twilio Setup
Step-by-step instructions for setting up a SIP trunk with Twilio.
Telnyx Setup
Step-by-step instructions for setting up a SIP trunk with Telnyx.
Plivo Setup
Step-by-step instructions for setting up a SIP trunk with Plivo.
LiveKit setup
Now you are ready to configure your LiveKit project to use the SIP trunk.
The following steps are common to all SIP trunking providers.
LiveKit CLI
These examples use the
LiveKit Cloud
. For additional examples and full documentation, see the linked documentation for each component.
Inbound trunk setup
An
inbound trunk
allows you to accept incoming phone calls.
Create an inbound trunk using the LiveKit Cloud dashboard.
Sign in to the
Telephony
→
Configuration
page.
Select
Create new
→
Trunk
.
Select the
JSON editor
tab.
Select
Inbound
for
Trunk direction
.
Copy and paste the following text into the editor, replacing the phone number with the number you purchased from your SIP trunk provider:
{
"trunk"
:
{
"name"
:
"My inbound trunk"
,
"numbers"
:
[
"+15105550123"
]
}
}
Select
Create
.
Create a dispatch rule
You must set up at least one
dispatch rule
to accept incoming calls into a LiveKit room.
This example creates a dispatch rule that puts each caller into a randomly generated unique room using the name prefix
call-
. For many applications, this is the only configuration you need.
Sign to the
Telephony
→
Configuration
page.
Select
Create new
→
Dispatch rule
.
Select the
JSON editor
tab.
Copy and paste the following text into the editor:
{
"name"
:
"My dispatch rule"
,
"rule"
:
{
"dispatchRuleIndividual"
:
{
"roomPrefix"
:
"call-"
}
}
}
Select
Create
.
After you create an inbound trunk and dispatch rule, you can create an agent to answer incoming calls. To learn more, see the resources in the
Next steps
section.
Create an outbound trunk
Create an
outbound trunk
to make outgoing phone calls with LiveKit.
This example creates an username and password authenticated outbound trunk with the phone number
+15105550123
and the trunk domain name
my-trunk-domain-name
.
Sign in to the
Telephony
→
Configuration
page.
Select
Create new
→
Trunk
.
Select the
JSON editor
tab.
Select
Outbound
for
Trunk direction
.
Copy and paste the following text into the editor:
{
"name"
:
"My outbound trunk"
,
"address"
:
"<my-trunk-domain-name>"
,
"numbers"
:
[
"+15105550123"
]
,
"authUsername"
:
"<username>"
,
"authPassword"
:
"<password>"
}
Select
Create
.
Now you are ready to
place outgoing calls
.
Next steps
See the following guides to continue building your telephony app.
Telephony agents
Building telephony-based voice AI apps with LiveKit Agents.
Make outbound calls
Detailed instructions for making outbound calls.
Additional documentation
See the following documentation for more details on the topics covered in this guide.
Inbound trunk
Detailed instructions for setting up inbound trunks.
Dispatch rule
Detailed instructions for setting up dispatch rules.
Outbound trunk
Detailed instructions for setting up outbound trunks.
On this page
Overview
External provider setup
SIP endpoint
Provider-specific instructions
LiveKit setup
Inbound trunk setup
Create a dispatch rule
Create an outbound trunk
Next steps
Additional documentation


Content from https://docs.livekit.io/sip/quickstarts/configuring-twilio-trunk:

On this page
Creating a SIP trunk for inbound and outbound calls
Prerequisites
Step 1. Create a SIP trunk
Step 2: Configure your trunk
Step 3: Associate phone number and trunk
Configure a SIP trunk using the Twilio UI
Next steps
Copy page
See more page options
Note
If you're using LiveKit Cloud as your SIP server and you're signed in, your SIP URI is automatically
included in the code blocks where appropriate.
Use the following steps to configure inbound and outbound SIP trunks using
Twilio
.
Creating a SIP trunk for inbound and outbound calls
Create a Twilio SIP trunk for incoming or outgoing calls, or both, using the following steps.
To use the Twilio console, see
Configure a SIP trunk using the Twilio UI
.
Note
For inbound calls, you can use TwiML for Programmable Voice instead of setting up Elastic SIP Trunking. To learn more,
see
Inbound calls with Twilio Voice
.
Prerequisites
Purchase phone number
.
Install the Twilio CLI
.
Create a
Twilio profile
to use the CLI.
Step 1. Create a SIP trunk
The domain name for your SIP trunk  must end in
pstn.twilio.com
. For example to create a trunk named
My test trunk
with the domain name
my-test-trunk.pstn.twilio.com
, run the following command:
twilio api trunking v1 trunks create
\
--friendly-name
"My test trunk"
\
--domain-name
"my-test-trunk.pstn.twilio.com"
The output includes the trunk SID. Copy it for use in the following steps.
Step 2: Configure your trunk
Configure the trunk for inbound calls or outbound calls or both. To create a SIP trunk for both inbound and
outbound calls, follow the steps in both tabs:
Inbound
Outbound
For inbound trunks, configure an
origination URI
. If you're
using LiveKit Cloud and are signed in, your SIP URI is automatically included in the following command:
twilio api trunking v1 trunks origination-urls create
\
--trunk-sid
<
twilio_trunk_sid
>
\
--friendly-name
"LiveKit SIP URI"
\
--sip-url
"sip:<your SIP endpoint>"
\
--weight
1
--priority
1
--enabled
Region-based endpoints
To restrict calls to a specific region, replace your global LiveKit SIP endpoint with a
region-based endpoint
.
Step 3: Associate phone number and trunk
The Twilio trunk SID and phone number SID are included in the output of
previous steps. If you didn't copy the SIDs, you can list them using the following commands:
To list phone numbers:
twilio phone-numbers list
To list trunks:
twilio api trunking v1 trunks list
twilio api trunking v1 trunks phone-numbers create
\
--trunk-sid
<
twilio_trunk_sid
>
\
--phone-number-sid
<
twilio_phone_number_sid
>
Configure a SIP trunk using the Twilio UI
Sign in to the
Twilio console
.
Purchase a phone number
.
Create SIP Trunk
on Twilio:
Select
Elastic SIP Trunking
»
Manage
»
Trunks
.
Create a SIP trunk.
Tip
Using your Twilio API key, you can skip the next two steps by using
this snippet
to set your origination and termination URLs automatically.
For inbound calls:
Navigate to
Voice
»
Manage
»
Origination connection policy
, and create an
Origination Connection Policy
Select the policy you just created and set the
Origination SIP URI
to your LiveKit SIP URI (available on your
Project settings
page). For example,
sip:vjnxecm0tjk.sip.livekit.cloud
.
Region-based endpoints
To restrict calls to a specific region, replace your global LiveKit SIP endpoint with a
region-based endpoint
.
For outbound calls, configure termination and authentication:
Navigate to
Elastic SIP Trunking
»
Manage
»
Trunks
.
Copy the
Termination SIP URI
to use
when you create an
outbound trunk
for LiveKit.
Configure
Authentication
:
Select
Elastic SIP Trunking
»
Manage
»
Credential lists
and create a new credential list
with a username and password of your choice.
Associate your trunk with the credential list:
Select
Elastic SIP Trunking
»
Manage
»
Trunks
and select the outbound trunk created in the
previous steps.
Select
Termination
» *
Authentication
»
Credential Lists
and select the credential list you
just created.
Next steps
Head back to the main setup documentation to finish connecting your SIP trunk to LiveKit.
SIP trunk setup
Configure your Twilio trunk in LiveKit.
On this page
Creating a SIP trunk for inbound and outbound calls
Prerequisites
Step 1. Create a SIP trunk
Step 2: Configure your trunk
Step 3: Associate phone number and trunk
Configure a SIP trunk using the Twilio UI
Next steps


Content from https://docs.livekit.io/sip/quickstarts/configuring-telnyx-trunk:

On this page
Creating a Telnyx SIP trunk using the API
Prerequisite
Step 1: Create an environment variable for API key
Step 2: Create an FQDN connection
Step 3: Associate phone number and trunk
Creating a SIP trunk using the Telnyx UI
Next steps
Copy page
See more page options
Note
If you're using LiveKit Cloud as your SIP server and you're signed in, your SIP endpoint is automatically
included in the code blocks where appropriate.
Creating a Telnyx SIP trunk using the API
You can use
curl
command to make calls to the Telnyx API V2. The commands in the steps below use the
example phone number,
+15105550100
. To use the Telnyx console, see
Creating a SIP trunk using the Telnyx UI
.
Prerequisite
Purchase a
Telnyx phone number
.
Step 1: Create an environment variable for API key
If you don't have a key a Telnyx API V2 key,
see the
Telnyx guide to create one
.
export
TELNYX_API_KEY
=
"<your_api_v2_key>"
Step 2: Create an FQDN connection
The following inbound and outbound commands include the required configuration settings if you plan on using only an inbound or
outbound trunk for your LiveKit telephony app. However, by default, an
FQDN connection
creates both an inbound and outbound trunk.
Creating an FQDN connection. Depending on your use case, select
Inbound
,
Outbound
, or
Inbound and outbound
to
accept calls, make calls, or both:
Inbound
Outbound
Inbound and Outbound
Set the caller's number format to
+E.164
for inbound calls (this identifies the caller's number with a leading
+
):
curl
-L
'https://api.telnyx.com/v2/fqdn_connections'
\
-H
'Content-Type: application/json'
\
-H
'Accept: application/json'
\
-H
"Authorization: Bearer
$TELNYX_API_KEY
"
\
-d
'{
"active": true,
"anchorsite_override": "Latency",
"connection_name": "My LiveKit trunk",
"inbound": {
"ani_number_format": "+E.164",
"dnis_number_format": "+e164"
}
}'
Copy the FQDN connection ID from the output:
{
"data"
:
{
"id"
:
"<connection_id>"
,
...
}
}
Create an FQDN with your
LiveKit SIP endpoint
and your FQDN connection ID:
curl
-L
'https://api.telnyx.com/v2/fqdns'
\
-H
'Content-Type: application/json'
\
-H
'Accept: application/json'
\
-H
"Authorization: Bearer
$TELNYX_API_KEY
"
\
-d
'{
"connection_id": "<connection_id>",
"fqdn": "<your SIP endpoint>",
"port": 5060,
"dns_record_type": "a"
}'
Region-based endpoints
To restrict calls to a specific region, replace your global LiveKit SIP endpoint with a
region-based endpoint
.
Step 3: Associate phone number and trunk
Get the phone number ID for phone number
5105550100
:
curl
-L
-g
'https://api.telnyx.com/v2/phone_numbers?filter[phone_number]=5105550100'
\
-H
'Accept: application/json'
\
-H
"Authorization: Bearer
$TELNYX_API_KEY
"
Copy the phone number ID from the output:
{
"meta"
:
{
"total_pages"
:
1
,
"total_results"
:
1
,
"page_number"
:
1
,
"page_size"
:
100
}
,
"data"
:
[
{
"id"
:
"<phone_number_id>"
,
...
}
]
}
Add the FQDN connection to the phone number:
curl
-L
-X
PATCH
'https://api.telnyx.com/v2/phone_numbers/<phone_number_id>'
\
-H
'Content-Type: application/json'
\
-H
'Accept: application/json'
\
-H
"Authorization: Bearer
$TELNYX_API_KEY
"
\
-d
'{
"id": "<phone_number_id>",
"connection_id": "<connection_id>"
}'
Creating a SIP trunk using the Telnyx UI
Sign in to the
Telnyx portal
.
Purchase a phone number
.
Navigate to
Voice
»
SIP Trunking
.
Create a SIP connection:
For inbound calls:
Select
FQDN
and save.
Select
Add FQDN
and enter your
LiveKit SIP endpoint
into the
FQDN
field.
For example,
vjnxecm0tjk.sip.livekit.cloud
.
Region-based endpoints
To restrict calls to a specific region, replace your global LiveKit SIP endpoint with a
region-based endpoint
.
Select the
Inbound
tab. In the
Destination Number Format
field, select
+E.164
.
In the
SIP Transport Protocol
field, select either
TCP
or
UDP
.
In the
SIP Region
field, select your region.
For outbound calls:
Select the
Outbound
tab.
In the
Outbound Voice Profile
field, select or create an outbound voice profile.
Select the
Settings
tab
Configure
FQDN Authentication
:
Select the
Settings
tab.
In the
Authentication & Routing Configuration
section, select
Outbound Calls Authentication
.
In the
Authentication Method
field, select
Credentials
and enter a username and password.
Select the
Numbers
tab and assign the purchased number to the SIP trunk.
Next steps
Head back to the main setup documentation to finish connecting your SIP trunk to LiveKit.
SIP trunk setup
Configure your Telnyx trunk in LiveKit.
On this page
Creating a Telnyx SIP trunk using the API
Prerequisite
Step 1: Create an environment variable for API key
Step 2: Create an FQDN connection
Step 3: Associate phone number and trunk
Creating a SIP trunk using the Telnyx UI
Next steps


Content from https://docs.livekit.io/sip/quickstarts/configuring-plivo-trunk:

On this page
Creating a SIP trunk using the Plivo Console
Prerequisites
Create a SIP trunk
Next steps
Copy page
See more page options
Use the following steps to configure inbound and outbound SIP trunks using
Plivo
.
Creating a SIP trunk using the Plivo Console
Create a Plivo SIP trunk for incoming or outgoing calls, or both, using the following steps.
Prerequisites
A phone number to make/receive calls
.
Create a SIP trunk
Sign in to the
Plivo Console
.
Navigate to
Zentrunk Dashboard
.
Create a SIP connection:
Inbound
Outbound
Select
Create New Inbound Trunk
and provide a descriptive name for your trunk.
Under
Trunk Authentication
, click
Add New URI
.
Enter your
LiveKit SIP endpoint
. For example,
vjnxecm0tjk.sip.livekit.cloud
Region-based endpoints
To restrict calls to a specific region, replace your global LiveKit SIP endpoint with a
region-based endpoint
.
Select
Create Trunk
to complete your inbound trunk creation.
Navigate to the
Phone Numbers Dashboard
and select the number to route to your inbound trunk.
Under
Number Configuration
, set
Trunk
to your newly created inbound trunk and select
Update
to save.
Next steps
Head back to the main setup documentation to finish connecting your SIP trunk to LiveKit.
SIP trunk setup
Configure your Plivo trunk in LiveKit.
On this page
Creating a SIP trunk using the Plivo Console
Prerequisites
Create a SIP trunk
Next steps


Content from https://docs.livekit.io/agents/v0/voice-agent/voice-pipeline:

On this page
Overview
Example agent
Model options
Modify context before LLM
Altering text before TTS
Turn detection thresholds
VAD settings
Interruption handling
Manual Interruptions
Emitted events
Events example
Copy page
See more page options
Agents 1.0 available for Python
This documentation is for v0.x of the LiveKit Agents framework.
See updated documentation here:
Building voice agents
.
v1.0 for Node.js is coming soon.
Overview
VoicePipelineAgent is a high-level abstraction that orchestrates conversation flow using a pipeline of three main models: STT → LLM → TTS. Additional models, like VAD, are used to enhance the conversation flow.
Realtime APIs
To use a realtime model like the OpenAI Realtime API, see the
Multimodal Agent
guide instead.
Example agent
The following is an example of an AI agent created with the
VoicePipelineAgent
class:
Python
Node.js
from
livekit
.
agents
import
llm
from
livekit
.
agents
.
pipeline
import
VoicePipelineAgent
from
livekit
.
plugins
import
cartesia
,
deepgram
,
openai
,
silero
initial_ctx
=
llm
.
ChatContext
(
)
.
append
(
role
=
"system"
,
text
=
"<your prompt>"
,
)
agent
=
VoicePipelineAgent
(
vad
=
silero
.
VAD
.
load
(
)
,
# flexibility to use any models
stt
=
deepgram
.
STT
(
model
=
"nova-2-general"
)
,
llm
=
openai
.
LLM
(
)
,
tts
=
cartesia
.
TTS
(
)
,
# intial ChatContext with system prompt
chat_ctx
=
initial_ctx
,
# whether the agent can be interrupted
allow_interruptions
=
True
,
# sensitivity of when to interrupt
interrupt_speech_duration
=
0.5
,
interrupt_min_words
=
0
,
# minimal silence duration to consider end of turn
min_endpointing_delay
=
0.5
,
# callback to run before LLM is called, can be used to modify chat context
before_llm_cb
=
None
,
# callback to run before TTS is called, can be used to customize pronounciation
before_tts_cb
=
None
,
)
# start the participant for a particular room, taking audio input from a single participant
agent
.
start
(
room
,
participant
)
Model options
Options on the models can be customized when creating the plugin objects. For example, you can adjust the model and temperature of the LLM like this:
Python
Node.js
llm
=
openai
.
LLM
(
model
=
"gpt-4o-mini"
,
temperature
=
0.5
,
)
Modify context before LLM
The
before_llm_cb
callback allows you to modify the
ChatContext
before it is sent to the LLM model. This is useful for adding extra context or adjusting the context based on the conversation. For example, when the context becomes too long, you can truncate it to optimize the amount of tokens used in inference.
Python
Node.js
async
def
truncate_context
(
assistant
:
VoicePipelineAgent
,
chat_ctx
:
llm
.
ChatContext
)
:
if
len
(
chat_ctx
.
messages
)
>
15
:
chat_ctx
.
messages
=
chat_ctx
.
messages
[
-
15
:
]
agent
=
VoicePipelineAgent
(
.
.
.
before_llm_cb
=
truncate_context
,
)
Altering text before TTS
The
before_tts_cb
callback allows you to modify the text before it is sent to the TTS model. This is useful for customizing pronunciation or adding extra context to the text.
Python
Node.js
from
livekit
.
agents
import
tokenize
from
livekit
.
agents
.
pipeline
import
VoicePipelineAgent
def
replace_words
(
assistant
:
VoicePipelineAgent
,
text
:
str
|
AsyncIterable
[
str
]
)
:
return
tokenize
.
utils
.
replace_words
(
text
=
text
,
replacements
=
{
"livekit"
:
r"<<l|aɪ|v|k|ɪ|t|>>"
}
)
agent
=
VoicePipelineAgent
(
.
.
.
before_tts_cb
=
replace_words
,
)
Turn detection thresholds
The following setting is available for voice activity detection. In addition to VAD, you can enable
LiveKit's turn detection model to work in conjunction with VAD for a more natural conversational flow.
To learn more, see
Turn detection
.
VAD settings
min_endpointing_delay
defines the minimum silence duration to detect the end of a turn. The default
value is
0.5
(500 ms). Increasing this value allows for longer pauses before the agent assumes the
user has finished speaking.
Interruption handling
When the user interrupts, the agent stops speaking and switches to listening mode, storing the position of the speech played so far in its ChatContext.
There are three flags that control the interruption behavior:
allow_interruptions
: set to
False
to disable user interruptions.
interrupt_speech_duration
: the minimum speech duration (detected by VAD) required to consider the interruption intentional.
interrupt_min_words
: the minimum number of transcribed words needed for the interruption to be considered intentional.
Manual Interruptions
You can manually interrupt an agent using the
agent.interrupt()
method. Calling this method immediately ends any agent speech.
To stop the agent from speaking any pending speech that's currently in the pipeline, you can use the
interrupt_all
parameter
to interrupt all pending speech.
Note
This method is currently only available in Python.
# Interrupt the agent's current response whenever someone joins the room
@ctx
.
room
.
on
(
"participant_connected"
)
def
on_participant_connected
(
participant
:
rtc
.
RemoteParticipant
)
:
agent
.
interrupt
(
interrupt_all
=
True
)
Emitted events
An agent emits the following events:
Event
Description
user_started_speaking
User started speaking.
user_stopped_speaking
User stopped speaking.
agent_started_speaking
Agent started speaking.
agent_stopped_speaking
Agent stopped speaking.
user_speech_committed
User's speech was committed to the chat context.
agent_speech_committed
Agent's speech was committed to the chat context.
agent_speech_interrupted
Agent was interrupted while speaking.
function_calls_collected
The complete set of functions to be executed was received.
function_calls_finished
All function calls have been executed.
metrics_collected
Metric was collected. Metrics can include time to first token for STT, LLM, TTS, duration, and usage
metrics.
Events example
For example, when a user's speech is committed to the chat context, save it to a queue for transcription:
Python
Node.js
@agent
.
on
(
"user_speech_committed"
)
def
on_user_speech_committed
(
msg
:
llm
.
ChatMessage
)
:
# convert string lists to strings, drop images
if
isinstance
(
msg
.
content
,
list
)
:
msg
.
content
=
"\n"
.
join
(
"[image]"
if
isinstance
(
x
,
llm
.
ChatImage
)
else
x
for
x
in
msg
)
log_queue
.
put_nowait
(
f"[
{
datetime
.
now
(
)
}
] USER:\n
{
msg
.
content
}
\n\n"
)
The
full example
is available in GitHub.
On this page
Overview
Example agent
Model options
Modify context before LLM
Altering text before TTS
Turn detection thresholds
VAD settings
Interruption handling
Manual Interruptions
Emitted events
Events example


Content from https://docs.livekit.io/agents/v0/voice-agent/multimodal-agent:

On this page
MultimodalAgent class
Usage
Advantages of speech-to-speech agents
Emitted events
Events example
Copy page
See more page options
Agents 1.0 available for Python
This documentation is for v0.x of the LiveKit Agents framework.
See updated documentation here:
Building voice agents
.
v1.0 for Node.js is coming soon.
The
MultimodalAgent
class is an abstraction for building AI agents using OpenAI’s Realtime API with multimodal models. These models accept audio directly, enabling them to 'hear' your voice and capture nuances like emotion, often lost in speech-to-text conversion.
MultimodalAgent class
Unlike
VoicePipelineAgent
, the
MultimodalAgent
class uses a single primary model for the conversation flow. The model is capable of processing both audio and text inputs, generating audio responses.
MultimodalAgent
is responsible for managing the conversation state, including buffering responses from the model and sending them to the user in realtime. It also handles interruptions, indicating to OpenAI's realtime API the point at which the model had been interrupted.
Usage
Python
Node.js
from
__future__
import
annotations
import
logging
from
livekit
import
rtc
from
livekit
.
agents
import
(
AutoSubscribe
,
JobContext
,
WorkerOptions
,
cli
,
llm
,
)
from
livekit
.
agents
.
multimodal
import
MultimodalAgent
from
livekit
.
plugins
import
openai
logger
=
logging
.
getLogger
(
"myagent"
)
logger
.
setLevel
(
logging
.
INFO
)
async
def
entrypoint
(
ctx
:
JobContext
)
:
logger
.
info
(
"starting entrypoint"
)
await
ctx
.
connect
(
auto_subscribe
=
AutoSubscribe
.
AUDIO_ONLY
)
participant
=
await
ctx
.
wait_for_participant
(
)
model
=
openai
.
realtime
.
RealtimeModel
(
instructions
=
"You are a helpful assistant and you love kittens"
,
voice
=
"shimmer"
,
temperature
=
0.8
,
modalities
=
[
"audio"
,
"text"
]
,
)
assistant
=
MultimodalAgent
(
model
=
model
)
assistant
.
start
(
ctx
.
room
)
logger
.
info
(
"starting agent"
)
session
=
model
.
sessions
[
0
]
session
.
conversation
.
item
.
create
(
llm
.
ChatMessage
(
role
=
"assistant"
,
content
=
"Please begin the interaction with the user in a manner consistent with your instructions."
,
)
)
session
.
response
.
create
(
)
if
__name__
==
"__main__"
:
cli
.
run_app
(
WorkerOptions
(
entrypoint_fnc
=
entrypoint
)
)
Advantages of speech-to-speech agents
Speech-to-speech agents offer several advantages over pipeline-based agents:
Natural Interactions
: Callers can speak and hear responses with extremely low latency, mimicking human-to-human conversations.
Voice and Tone
: Speech-to-speech agents are able to dynamically change the intonation and tone of their responses based on the emotions of the caller, making interactions more engaging.
Emitted events
An agent emits the following events:
Python
Node.js
Event
Description
user_started_speaking
User started speaking.
user_stopped_speaking
User stopped speaking.
agent_started_speaking
Agent started speaking.
agent_stopped_speaking
Agent stopped speaking.
user_speech_committed
User's speech was committed to the chat context.
agent_speech_committed
Agent's speech was committed to the chat context.
agent_speech_interrupted
Agent was interrupted while speaking.
Events example
Python
Node.js
When user speech is committed to the chat context, save it to a queue:
@agent
.
on
(
"user_speech_committed"
)
def
on_user_speech_committed
(
msg
:
llm
.
ChatMessage
)
:
# convert string lists to strings, drop images
if
isinstance
(
msg
.
content
,
list
)
:
msg
.
content
=
"\n"
.
join
(
"[image]"
if
isinstance
(
x
,
llm
.
ChatImage
)
else
x
for
x
in
msg
)
log_queue
.
put_nowait
(
f"[
{
datetime
.
now
(
)
}
] USER:\n
{
msg
.
content
}
\n\n"
)
On this page
MultimodalAgent class
Usage
Advantages of speech-to-speech agents
Emitted events
Events example


Content from https://docs.livekit.io/agents/v0/voice-agent/telephony:

On this page
Overview of SIP integration
How it works
Getting started
Copy page
See more page options
Agents 1.0 available for Python
This documentation is for v0.x of the LiveKit Agents framework.
See updated documentation here:
Integrating with telephony
.
v1.0 for Node.js is coming soon.
Using LiveKit, it's easy to integrate AI voice agents with telephony systems using SIP. By connecting your agents to phone calls, you can enable users to interact with AI-powered voice assistants over traditional telephony networks.
Overview of SIP integration
SIP is a signaling protocol used for initiating, maintaining, and terminating realtime sessions that involve voice, video, and messaging applications. In the context of telephony integration with AI voice agents, SIP allows you to bridge phone calls into LiveKit rooms where your agents can interact with callers.
How it works
SIP Trunking
: You'll require a SIP trunk with a provider like
Twilio
or
Telnyx
to obtain a phone number and handle call routing.
Call Routing
: Incoming or outgoing calls are routed through the SIP trunk into a dynamically generated LiveKit room.
Agent Participation
: Your AI voice agent joins the LiveKit room as a participant.
LiveKit Rooms
: The LiveKit room acts as a bridge between the phone call and the agent, allowing them to interact in realtime. It also provides an intuitive backend API for transcriptions, recordings, moderation, and other features.
Getting started
Agent Setup
: The best way to get started with telephony applications is by creating an agent using the
Voice pipeline agent
or a speech-to-speech agent using the
OpenAI Realtime API quickstart
guide.
Inbound Calls
: To handle inbound calls from users to your agent, follow the
Inbound Calls via SIP guide
.
Outbound Calls
: To enable your agent to make outbound calls to users, refer to the
Outbound Calls via SIP guide
.
On this page
Overview of SIP integration
How it works
Getting started


Content from https://docs.livekit.io/agents/v0/voice-agent/client-apps:

On this page
Voice agent components
Example
Testing agent UI
Copy page
See more page options
Agents 1.0 available for Python
This documentation is for v0.x of the LiveKit Agents framework.
See updated documentation here:
Web and mobile frontends
.
v1.0 for Node.js is coming soon.
When building voice agents with the Agents framework, your frontend can leverage LiveKit‘s SDKs to manage media devices and transport audio and video streams.
The following UI components are specifically for voice agents, including speech visualizers and control bars.
Voice agent components
The
useVoiceAssistant
hook returns the voice assistant’s
state
and audio track. You pass them to a visualizer component to give users visual feedback about the agent's current status.
There are two additional components available:
BarVisualizer
: Visualizes audio output with vertical bars. You can optionally set the number of bars and the minimum and maximum height. The visualizer can be customized via CSS styles to fit your application's design.
Tip
When
state
is passed into the
BarVisualizer
component, it visualizes both the agent’s state (like
listening
or
thinking
) and the audio spectrum.
VoiceAssistantControlBar
: A control bar
that includes audio settings and a disconnect button.
Example
Here's a basic example showing how these components work together:
React
Android
Swift
// in next.js, "use client" is needed to indicate it shouldn't be
// server side rendered
"use client"
;
import
"@livekit/components-styles"
;
import
{
RoomContext
,
useVoiceAssistant
,
BarVisualizer
,
RoomAudioRenderer
,
VoiceAssistantControlBar
,
}
from
"@livekit/components-react"
;
export
default
function
MyVoiceAgent
(
)
{
const
[
room
]
=
useState
(
new
Room
(
)
)
;
return
(
<
RoomContext
.
Provider value
=
{
room
}
>
<
button onClick
=
{
(
)
=>
room
.
connect
(
serverUrl
,
token
)
}
>
Connect
<
/
button
>
<
SimpleVoiceAssistant
/
>
<
VoiceAssistantControlBar
/
>
<
RoomAudioRenderer
/
>
<
/
RoomContext
.
Provider
>
)
;
}
function
SimpleVoiceAssistant
(
)
{
const
{
state
,
audioTrack
}
=
useVoiceAssistant
(
)
;
return
(
<
div className
=
"h-80"
>
<
BarVisualizer state
=
{
state
}
barCount
=
{
5
}
trackRef
=
{
audioTrack
}
style
=
{
{
}
}
/
>
<
p className
=
"text-center"
>
{
state
}
<
/
p
>
<
/
div
>
)
;
}
Testing agent UI
You can try out both of these components in the
realtime playground
. These components are also included in the frontend when you create an app using
LiveKit Sandbox
.
On this page
Voice agent components
Example
Testing agent UI


Content from https://docs.livekit.io/agents/v0/voice-agent/function-calling:

On this page
Usage
Forwarding to the frontend
Copy page
See more page options
Agents 1.0 available for Python
This documentation is for v0.x of the LiveKit Agents framework.
See updated documentation here:
Tool definition and use
.
v1.0 for Node.js is coming soon.
Function calling (also known as "tool calling" or "tool use") is a powerful LLM capability that allows AI models to interact with external functions and tools. This allows your agent to retrieve additional context before generating a response or take real-world actions.
For example, when a user says "What's the weather like in New York?", the LLM can intelligently detect the need to call your weather function before it responds. As another example, you could use function calling to trigger an action in the frontend, such as presenting a map to the user or querying their current location.
Both
PipelineVoiceAgent
and
MultimodalAgent
have built-in support for function calling, making it easy to create rich voice-powered applications.
Usage
Python
Node.js
import
aiohttp
from
typing
import
Annotated
from
livekit
.
agents
import
llm
from
livekit
.
agents
.
pipeline
import
VoicePipelineAgent
from
livekit
.
agents
.
multimodal
import
MultimodalAgent
# first define a class that inherits from llm.FunctionContext
class
AssistantFnc
(
llm
.
FunctionContext
)
:
# the llm.ai_callable decorator marks this function as a tool available to the LLM
# by default, it'll use the docstring as the function's description
@llm
.
ai_callable
(
)
async
def
get_weather
(
self
,
# by using the Annotated type, arg description and type are available to the LLM
location
:
Annotated
[
str
,
llm
.
TypeInfo
(
description
=
"The location to get the weather for"
)
]
,
)
:
"""Called when the user asks about the weather. This function will return the weather for the given location."""
logger
.
info
(
f"getting weather for
{
location
}
"
)
url
=
f"https://wttr.in/
{
location
}
?format=%C+%t"
async
with
aiohttp
.
ClientSession
(
)
as
session
:
async
with
session
.
get
(
url
)
as
response
:
if
response
.
status
==
200
:
weather_data
=
await
response
.
text
(
)
# response from the function call is returned to the LLM
# as a tool response. The LLM's response will include this data
return
f"The weather in
{
location
}
is
{
weather_data
}
."
else
:
raise
f"Failed to get weather data, status code:
{
response
.
status
}
"
fnc_ctx
=
AssistantFnc
(
)
# pass the function context to the agent
pipeline_agent
=
VoicePipelineAgent
(
.
.
.
fnc_ctx
=
fnc_ctx
,
)
multimodal_agent
=
MultimodalAgent
(
.
.
.
fnc_ctx
=
fnc_ctx
,
)
Forwarding to the frontend
Function calls can be forwarded to a frontend application using
RPC
. This is useful when the data needed to fulfill the function call is only available at the frontend. It can also be used to trigger actions or UI updates in a structured way.
For instance, here's a function that accesses the user's live location from their web browser:
Agent implementation
Python
Node.js
from
livekit
.
agents
import
llm
from
typing
import
Annotated
class
AssistantFnc
(
llm
.
FunctionContext
)
:
@llm
.
ai_callable
(
)
async
def
get_user_location
(
self
,
high_accuracy
:
Annotated
[
bool
,
llm
.
TypeInfo
(
description
=
"Whether to use high accuracy mode, which is slower"
)
]
=
False
)
:
"""Retrieve the user's current geolocation as lat/lng."""
try
:
return
await
ctx
.
room
.
local_participant
.
perform_rpc
(
destination_identity
=
participant
.
identity
,
method
=
"getUserLocation"
,
payload
=
json
.
dumps
(
{
"highAccuracy"
:
high_accuracy
}
)
,
response_timeout
=
10.0
if
high_accuracy
else
5.0
,
)
except
Exception
:
return
"Unable to retrieve user location"
Frontend implementation
JavaScript
import
{
RpcError
,
RpcInvocationData
}
from
'livekit-client'
;
localParticipant
.
registerRpcMethod
(
'getUserLocation'
,
async
(
data
:
RpcInvocationData
)
=>
{
try
{
let
params
=
JSON
.
parse
(
data
.
payload
)
;
const
position
:
GeolocationPosition
=
await
new
Promise
(
(
resolve
,
reject
)
=>
{
navigator
.
geolocation
.
getCurrentPosition
(
resolve
,
reject
,
{
enableHighAccuracy
:
params
.
highAccuracy
??
false
,
timeout
:
data
.
responseTimeout
,
}
)
;
}
)
;
return
JSON
.
stringify
(
{
latitude
:
position
.
coords
.
latitude
,
longitude
:
position
.
coords
.
longitude
,
}
)
;
}
catch
(
error
)
{
throw
new
RpcError
(
1
,
"Could not retrieve user location"
)
;
}
}
)
;
On this page
Usage
Forwarding to the frontend


Content from https://docs.livekit.io/agents/v0/voice-agent/transcriptions:

On this page
Overview
Frontend integration
Agent integration
Copy page
See more page options
Agents 1.0 available for Python
This documentation is for v0.x of the LiveKit Agents framework.
See updated documentation here:
Text and transcriptions
.
v1.0 for Node.js is coming soon.
Overview
The Agents framework includes the ability to capture and deliver realtime transcriptions of a user's speech and LLM-generated speech or text.
Both
VoicePipelineAgent
and
MultimodalAgent
can forward transcriptions to clients automatically if you implement support for receiving them in your frontend. If you're not using either of these agent classes, you can
add transcription forwarding
to your agent code.
To learn more about creating transcriptions in the agent process, see
Recording agent sessions
.
Frontend integration
You can use a
LiveKit SDK
to receive transcription events in your frontend.
Transcriptions are delivered in segments, each associated with a particular
Participant
and
Track
. Each segment has a unique
id
. Segments might be sent as fragments as they're generated. You can monitor the
final
property to determine when a segment is complete.
JavaScript
Swift
Android
Flutter
This example uses React with TypeScript, but the principles are the same for other frameworks.
Collect
TranscriptionSegment
by listening to
RoomEvent.TranscriptionReceived
:
import
{
useEffect
,
useState
}
from
"react"
;
import
{
TranscriptionSegment
,
Participant
,
TrackPublication
,
RoomEvent
,
}
from
"livekit-client"
;
import
{
useMaybeRoomContext
}
from
"@livekit/components-react"
;
export
default
function
Transcriptions
(
)
{
const
room
=
useMaybeRoomContext
(
)
;
const
[
transcriptions
,
setTranscriptions
]
=
useState
<
{
[
id
:
string
]
:
TranscriptionSegment
}
>
(
{
}
)
;
useEffect
(
(
)
=>
{
if
(
!
room
)
{
return
;
}
const
updateTranscriptions
=
(
segments
:
TranscriptionSegment
[
]
,
participant
?
:
Participant
,
publication
?
:
TrackPublication
)
=>
{
setTranscriptions
(
(
prev
)
=>
{
const
newTranscriptions
=
{
...
prev
}
;
for
(
const
segment
of
segments
)
{
newTranscriptions
[
segment
.
id
]
=
segment
;
}
return
newTranscriptions
;
}
)
;
}
;
room
.
on
(
RoomEvent
.
TranscriptionReceived
,
updateTranscriptions
)
;
return
(
)
=>
{
room
.
off
(
RoomEvent
.
TranscriptionReceived
,
updateTranscriptions
)
;
}
;
}
,
[
room
]
)
;
return
(
<
ul
>
{
Object
.
values
(
transcriptions
)
.
sort
(
(
a
,
b
)
=>
a
.
firstReceivedTime
-
b
.
firstReceivedTime
)
.
map
(
(
segment
)
=>
(
<
li key
=
{
segment
.
id
}
>
{
segment
.
text
}
<
/
li
>
)
)
}
<
/
ul
>
)
}
Agent integration
The
STTSegmentsForwarder
class provides an interface for delivering transcriptions from your custom agent to your
frontend
in realtime. Here's a sample implementation:
from
livekit
.
agents
import
stt
,
transcription
from
livekit
.
plugins
.
deepgram
import
STT
async
def
_forward_transcription
(
stt_stream
:
stt
.
SpeechStream
,
stt_forwarder
:
transcription
.
STTSegmentsForwarder
,
)
:
"""Forward the transcription and log the transcript in the console"""
async
for
ev
in
stt_stream
:
stt_forwarder
.
update
(
ev
)
if
ev
.
type
==
stt
.
SpeechEventType
.
INTERIM_TRANSCRIPT
:
print
(
ev
.
alternatives
[
0
]
.
text
,
end
=
""
)
elif
ev
.
type
==
stt
.
SpeechEventType
.
FINAL_TRANSCRIPT
:
print
(
"\n"
)
print
(
" -> "
,
ev
.
alternatives
[
0
]
.
text
)
async
def
entrypoint
(
job
:
JobContext
)
:
stt
=
STT
(
)
tasks
=
[
]
async
def
transcribe_track
(
participant
:
rtc
.
RemoteParticipant
,
track
:
rtc
.
Track
)
:
audio_stream
=
rtc
.
AudioStream
(
track
)
stt_forwarder
=
transcription
.
STTSegmentsForwarder
(
room
=
job
.
room
,
participant
=
participant
,
track
=
track
)
stt_stream
=
stt
.
stream
(
)
stt_task
=
asyncio
.
create_task
(
_forward_transcription
(
stt_stream
,
stt_forwarder
)
)
tasks
.
append
(
stt_task
)
async
for
ev
in
audio_stream
:
stt_stream
.
push_frame
(
ev
.
frame
)
@job
.
room
.
on
(
"track_subscribed"
)
def
on_track_subscribed
(
track
:
rtc
.
Track
,
publication
:
rtc
.
TrackPublication
,
participant
:
rtc
.
RemoteParticipant
,
)
:
if
track
.
kind
==
rtc
.
TrackKind
.
KIND_AUDIO
:
tasks
.
append
(
asyncio
.
create_task
(
transcribe_track
(
participant
,
track
)
)
)
On this page
Overview
Frontend integration
Agent integration


Content from https://docs.livekit.io/agents/v0/quickstarts/s2s:

On this page
Prerequisites
Steps
Setup a LiveKit account and install the CLI
Bootstrap an agent from template
Bootstrap a frontend from template
Launch your app and talk to your agent
Next steps
Copy page
See more page options
Agents 1.0 available for Python
This documentation is for v0.x of the LiveKit Agents framework.
See updated documentation here:
Voice AI quickstart
.
v1.0 for Node.js is coming soon.
The
MultimodalAgent
class in the LiveKit Agents Framework uses the OpenAI Realtime API for speech-to-speech interactions between AI voice assistants and end users. It is implemented in both our
Python
and
Node
Agents Framework libraries.
Note
If you're not using the OpenAI Realtime API, see the
Voice agent with STT, LLM, TTS quickstart
.
Prerequisites
OpenAI API Key
Python 3.9-3.12
or
Node 20.17.0
Steps
The following steps take you through the process of creating a LiveKit account and using the LiveKit CLI to create an agent from some minimal templates. At the end of the quickstart, you'll have an agent and a frontend you can use to talk to your agent.
Setup a LiveKit account and install the CLI
Create an account or sign in to your
LiveKit Cloud account
.
(Optional)
Install the LiveKit CLI
and authenticate using
lk cloud auth
.
Note
LiveKit's CLI utility
lk
is a convenient way to setup and configure new applications, but if you'd rather do it manually, you can clone the
multimodal-agent-python
(or
node
) and
agent-starter-react
repositories and follow the manual setup instructions in each.
Bootstrap an agent from template
Clone a starter template for your preferred language using the CLI:
Python
Node.js
lk app create
--template
multimodal-agent-python
Enter your
OpenAI API Key
when prompted.
Install dependencies and start your agent:
Python
Node.js
cd
<
agent_dir
>
python3
-m
venv venv
source
venv/bin/activate
python3
-m
pip
install
-r
requirements.txt
python3 agent.py dev
You can edit the
agent.py
file to customize the system prompt and other aspects of your agent.
Bootstrap a frontend from template
Clone the
Next.js Voice Agent
starter template using the CLI:
lk app create
--template
voice-assistant-frontend
Install dependencies and start your frontend application:
cd
<
frontend_dir
>
pnpm
install
pnpm
dev
Launch your app and talk to your agent
Visit your locally-running application (by default,
http://localhost:3000
).
Select
Connect
and start a conversation with your agent.
Next steps
Learn more in the
OpenAI Realtime API integration guide
.
Let your friends and colleagues talk to your agent by connecting it to a LiveKit
Sandbox
.
Create an
agent that accepts incoming calls
using SIP.
Create an
agent that makes outbound calls
using SIP.
On this page
Prerequisites
Steps
Setup a LiveKit account and install the CLI
Bootstrap an agent from template
Bootstrap a frontend from template
Launch your app and talk to your agent
Next steps


Content from https://docs.livekit.io/agents/v0/quickstarts/voice-agent:

On this page
Prerequisites
Steps
Setup a LiveKit account and install the CLI
Bootstrap an agent from template
Bootstrap a frontend from template
Launch your app and talk to your agent
Customizing plugins
Next steps
Copy page
See more page options
Agents 1.0 available for Python
This documentation is for v0.x of the LiveKit Agents framework.
See updated documentation here:
Voice AI quickstart
.
v1.0 for Node.js is coming soon.
This quickstart tutorial walks you through the steps to build a conversational AI application using Python and NextJS. It uses LiveKit's
Agents Framework
and React Components Library to create an AI-powered voice assistant that can engage in realtime conversations with users. By the end, you will have a basic voice assistant application that you can run and interact with.
Note
If you're interested in using the OpenAI Realtime API, see the
Speech-to-speech quickstart
.
Prerequisites
LiveKit Cloud Project
or
open-source LiveKit server
Deepgram API Key
Cartesia API Key
OpenAI API Key
Python 3.9-3.12
Note
By default, the example agent uses Deepgram for STT and OpenAI for TTS and LLM. However, you aren't required
to use these providers.
Steps
The following steps take you through the process of creating a voice assistant using the LiveKit CLI and some minimal templates.
Setup a LiveKit account and install the CLI
Create an account or sign in to your
LiveKit Cloud account
.
Install the LiveKit CLI
and authenticate using
lk cloud auth
.
Bootstrap an agent from template
Clone the starter template for a simple Python voice agent:
lk app create
--template
voice-pipeline-agent-python
Enter your
OpenAI API Key
and
Deepgram API Key
when prompted. If you aren't using Deepgram and OpenAI, see
Customizing plugins
.
Install dependencies and start your agent:
cd
<
agent_dir
>
python3
-m
venv venv
source
venv/bin/activate
python3
-m
pip
install
-r
requirements.txt
python3 agent.py dev
You can edit the
agent.py
file to customize the system prompt and other aspects of your agent.
Bootstrap a frontend from template
Clone the
Next.js Voice Agent
starter template using the CLI:
lk app create
--template
voice-assistant-frontend
Install dependencies and start your frontend application:
cd
<
frontend_dir
>
pnpm
install
pnpm
dev
Launch your app and talk to your agent
Visit your locally-running application (by default,
http://localhost:3000
).
Select
Connect
and start a conversation with your agent.
Customizing plugins
You can change the VAD, STT, TTS, and LLM plugins your agent uses by editing the
agents.py
file. By default,
the sandbox voice assistant is configured to use Silero for VAD, Deepgram for STT, and OpenAI for TTS and LLM
using the
gpt-4o-mini
model:
assistant
=
VoiceAssistant
(
vad
=
silero
.
VAD
.
load
(
)
,
stt
=
deepgram
.
STT
(
)
,
llm
=
openai
.
LLM
(
model
=
"gpt-4o-mini"
)
,
tts
=
openai
.
TTS
(
)
,
chat_ctx
=
initial_ctx
,
)
You can modify your agent to use different providers. For example, to use Cartesia for TTS, use the following
steps:
Edit file
agent.py
and update the imported plugins list to include
cartesia
:
from
livekit
.
plugins
import
cartesia
,
deepgram
,
openai
,
silero
Update the
tts
plugin for your assistant in file
agent.py
:
tts
=
cartesia
.
TTS
(
)
,
Update the
.env.local
file to include your Cartesia API key by adding a
CARTESIA_API_KEY
environment variable:
CARTESIA_API_KEY
=
"<cartesia_api_key>"
Install the plugin locally:
pip
install
"livekit-plugins-cartesia~=0.4"
Start your agent:
python3 agent.py dev
Next steps
For a list of additional plugins you can use, see
Integration guides for LiveKit Agents
.
Let your friends and colleagues talk to your agent by connecting it to a LiveKit
Sandbox
.
Create an
agent that accepts incoming calls
using SIP.
Create an
agent that makes outbound calls
using SIP.
On this page
Prerequisites
Steps
Setup a LiveKit account and install the CLI
Bootstrap an agent from template
Bootstrap a frontend from template
Launch your app and talk to your agent
Customizing plugins
Next steps


Content from https://docs.livekit.io/agents/v0/quickstarts/inbound-calls:

On this page
Prerequisites
Step 1: Set up environment variables
Step 2: Create an AI voice agent
Step 3: Create an inbound LiveKit trunk
Add dispatch rule
Step 4: Call your agent
Next steps
Copy page
See more page options
Agents 1.0 available for Python
This documentation is for v0.x of the LiveKit Agents framework.
See updated documentation here:
Integrating with telephony
.
v1.0 for Node.js is coming soon.
Note
To make calls, see the
Making outgoing calls
quickstart.
This guide walks you through the steps to create an AI voice agent that responds to incoming calls.
Users can call a phone number and interact directly with your AI-powered voice assistant. For example,
your agent can be a resource in the following scenarios:
Call centers
: Automate customer interactions and reduce wait times.
Customer service
: Provide immediate assistance through an AI agent.
Sales inquiries
: Allow customers to ask questions about products or services.
Prerequisites
The following are required to complete the steps in this quickstart:
A phone number purchased from your SIP trunk provider.
A SIP trunk with your provider.
LiveKit CLI
installed.
SIP server:
LiveKit Cloud
or a
self-hosted SIP server
.
Instructions for setting up a SIP trunk are available in the
Create and configure SIP trunk
quickstart.
Step 1: Set up environment variables
Tip
Log in
to see your real credentials populated in many places throughout this page
Set up the following environment variables to configure the LiveKit CLI to use your LiveKit Cloud or
self-hosted LiveKit server
instance:
export
LIVEKIT_URL
=
<
your LiveKit server URL
>
export
LIVEKIT_API_KEY
=
<
your API Key
>
export
LIVEKIT_API_SECRET
=
<
your API Secret
>
Reveal API Key and Secret
Step 2: Create an AI voice agent
The fastest way to create an agent is by using the LiveKit CLI.
Enter your API keys at the prompts. Alternatively, you can skip the prompts and manually edit the
.env.local
file with your
API keys.
Python
Node.js
lk app create
--template
voice-pipeline-agent-python
// or clone it from GitHub
git
clone https://github.com/livekit-examples/voice-pipeline-agent-python.git
Modify the agent to give it an
agent_name
. This will allow you to
dispatch the agent
explicitly when configuring SIP.
Python
Node.js
if
__name__
==
"__main__"
:
cli
.
run_app
(
WorkerOptions
(
entrypoint_fnc
=
entrypoint
,
prewarm_fnc
=
prewarm
,
# giving this agent a name of: "inbound-agent"
agent_name
=
"inbound-agent"
,
)
,
)
Then you can start the agent:
Python
Node.js
python3 agent.py dev
Step 3: Create an inbound LiveKit trunk
An inbound SIP trunk instructs LiveKit to accept calls to one or more numbers that you own. Replace the phone number
with the number purchased from your SIP trunking provider and create a file
inbound-trunk.json
with the
following content:
Twilio
Telnyx
{
"trunk"
:
{
"name"
:
"My inbound trunk"
,
"numbers"
:
[
"+15105550100"
]
}
}
Important
Twilio numbers must start with a leading
+
.
Create the LiveKit inbound SIP trunk using the CLI:
lk sip inbound create inbound-trunk.json
Add dispatch rule
Dispatch rules
route inbound SIP calls to LiveKit rooms. In this example,
the
dispatchRuleIndividual
routes each caller to their own room. Each room will
be named with a prefix of
call
.
Additionally, specify an agent to handle incoming calls. In this example,
inbound-agent
is dispatched to all callers.
Create
dispatch-rule.json
file:
{
"name"
:
"My dispatch rule"
,
"rule"
:
{
"dispatchRuleIndividual"
:
{
"roomPrefix"
:
"call"
}
}
,
"room_config"
:
{
"agents"
:
[
{
"agent_name"
:
"inbound-agent"
}
]
}
}
Apply the dispatch rule for the trunk using the CLI:
lk sip dispatch create dispatch-rule.json
Step 4: Call your agent
Calling the phone number you assigned to the trunk places you in a room with your agent.
Next steps
Using SIP participant attributes
: Modify the agent you created in Step 2
based on a caller's attributes.
To learn more, see the following topics:
Inbound trunks
: You can configure your inbound trunk to limit access to your LiveKit SIP application,
set participant metadata and attributes, and modify call properties.
Dispatch rules
: You can create dispatch rules to control how callers are dispatched to rooms,
configure authentication, and add participant attributes.
SIP participant
: You can manage SIP participants like any other LiveKit participant. You can also
use SIP specific attributes to manage callers based on the number they call or the number they're calling from, and more.
On this page
Prerequisites
Step 1: Set up environment variables
Step 2: Create an AI voice agent
Step 3: Create an inbound LiveKit trunk
Add dispatch rule
Step 4: Call your agent
Next steps


Content from https://docs.livekit.io/agents/v0/quickstarts/outbound-calls:

On this page
Prerequisites
Outbound call flow
Step 1: Set up environment variables
Step 2: Create an outbound trunk
Step 3: Create an agent
Understanding the code
Step 4: Creating a dispatch
Next steps
Copy page
See more page options
Agents 1.0 available for Python
This documentation is for v0.x of the LiveKit Agents framework.
See updated documentation here:
Integrating with telephony
.
v1.0 for Node.js is coming soon.
This guide walks you through the steps to create an AI voice agent that makes outgoing
calls. You can use an agent in the following scenarios:
Call center callbacks
: Automatically return customer calls.
Product check-ins
: Follow up on purchased items.
Sales calls
: Reach out to potential customers.
Appointment reminders
: Notify clients of upcoming appointments.
Surveys
: Collect feedback through automated calls.
Prerequisites
The following are required to complete the steps in this guide:
Phone number purchased from a SIP trunk provider like
Twilio
or
Telnyx
.
SIP provider trunk configured
as an outbound trunk for use with LiveKit SIP.
LiveKit Cloud
project or a
self-hosted instance
of LiveKit server
SIP server
(only required if you're self hosting the LiveKit server).
LiveKit CLI
installed (requires version 2.3.0 or later).
Outbound call flow
The suggested flow for outbound calls is as follows:
Create a dispatch for your agent.
Once your agent is connected, dial the user via CreateSIPParticipant.
The user answers the call, and starts speaking to the agent.
Step 1: Set up environment variables
Tip
Log in
to see your real credentials populated in many places throughout this page
Set up the following environment variables to configure the LiveKit CLI to use your LiveKit Cloud or
self-hosted LiveKit server
instance:
export
LIVEKIT_URL
=
<
your LiveKit server URL
>
export
LIVEKIT_API_KEY
=
<
your API Key
>
export
LIVEKIT_API_SECRET
=
<
your API Secret
>
export
OPENAI_API_KEY
=
your-openai-api-key
Reveal API Key and Secret
Step 2: Create an outbound trunk
To make outgoing calls, the provider's outbound trunk needs to be registered with LiveKit.
SIP trunking providers typically require authentication when accepting outbound SIP requests to ensure only authorized users are making calls with your number.
Note
This setup only needs to be performed once.
If you already have an
outbound trunk
for the SIP provider phone number,
you can skip to
Step 3: Create an agent
.
Create a file named
outbound-trunk.json
using your phone number, trunk domain name,
and
username
and
password
. The following example assumes your provider phone number is
+15105550100
:
Twilio
Telnyx
{
"trunk"
:
{
"name"
:
"My outbound trunk"
,
"address"
:
"<my-trunk>.pstn.twilio.com"
,
"numbers"
:
[
"+15105550100"
]
,
"auth_username"
:
"<username>"
,
"auth_password"
:
"<password>"
}
}
Create the outbound trunk using the CLI:
lk sip outbound create outbound-trunk.json
The output of the command returns the trunk ID. Copy the
<trunk-id>
for step 4:
SIPTrunkID: <trunk-id>
Step 3: Create an agent
Create an agent that makes outbound calls. In this example, create a speech-to-speech agent
using OpenAI's realtime API. The same example can also be used with
VoicePipelineAgent
.
Create a template
outbound caller agent
:
lk app create
--template
=
outbound-caller-python
Follow the instructions in the command output. Enter the outbound trunk ID from Step 2.
Understanding the code
There are a few key points of note in this example detailed in the following sections.
Set
agent_name
for explicit dispatch
if
__name__
==
"__main__"
:
cli
.
run_app
(
WorkerOptions
(
entrypoint_fnc
=
entrypoint
,
# giving this agent a name will allow us to dispatch it via API
# automatic dispatch is disabled when `agent_name` is set
agent_name
=
"outbound-caller"
,
)
)
By setting the
agent_name
field, you can use AgentDispatchService to create a
dispatch for this agent. To learn more, see
agent dispatch
.
Dialing the user
async
def
entrypoint
(
ctx
:
JobContext
)
:
global
_default_instructions
logger
.
info
(
f"connecting to room
{
ctx
.
room
.
name
}
"
)
await
ctx
.
connect
(
auto_subscribe
=
AutoSubscribe
.
AUDIO_ONLY
)
user_identity
=
"phone_user"
# the phone number to dial is provided in the job metadata
phone_number
=
ctx
.
job
.
metadata
logger
.
info
(
f"dialing
{
phone_number
}
to room
{
ctx
.
room
.
name
}
"
)
# look up the user's phone number and appointment details
instructions
=
_default_instructions
+
"The customer's name is Jayden. His appointment is next Tuesday at 3pm."
# `create_sip_participant` starts dialing the user
await
ctx
.
api
.
sip
.
create_sip_participant
(
api
.
CreateSIPParticipantRequest
(
room_name
=
ctx
.
room
.
name
,
sip_trunk_id
=
outbound_trunk_id
,
sip_call_to
=
phone_number
,
participant_identity
=
user_identity
,
)
)
# a participant is created as soon as we start dialing
participant
=
await
ctx
.
wait_for_participant
(
identity
=
user_identity
)
# start either VoicePipelineAgent or MultimodalAgent
#run_voice_pipeline_agent(ctx, participant, instructions)
run_multimodal_agent
(
ctx
,
participant
,
instructions
)
Once dispatched, the agent first connects to the room and then dials the user.
The agent receives instructions on which user to call from the job metadata set during dispatch.
In this example, the user’s phone number is included in the job metadata (details below).
After determining which user to call, you can load additional information about
the user from your own database.
Finally, calling
create_sip_participant
initiates dialing the user.
Monitoring dialing status
start_time
=
perf_counter
(
)
while
perf_counter
(
)
-
start_time
<
30
:
call_status
=
participant
.
attributes
.
get
(
"sip.callStatus"
)
if
call_status
==
"active"
:
logger
.
info
(
"user has picked up"
)
return
elif
call_status
==
"automation"
:
# if DTMF is used in the `sip_call_to` number, typically used to dial
# an extension or enter a PIN.
# during DTMF dialing, the participant will be in the "automation" state
pass
elif
call_status
==
"hangup"
:
# user hung up, we'll exit the job
logger
.
info
(
"user hung up, exiting job"
)
break
await
asyncio
.
sleep
(
0.1
)
logger
.
info
(
"session timed out, exiting job"
)
ctx
.
shutdown
(
)
Once
create_sip_participant
is called, LiveKit begins dialing the user.
The method returns immediately after dialing is initiated and does not wait for the user to answer.
Optionally, you can check the
sip.callStatus
attribute to monitor dialing status.
When the user answers, this attribute updates to
active
.
Handle actions with function calling
Use
@llm.ai_callable()
to prepare functions for the LLM to use as tools.
In this example, the following actions are handled:
Detecting voicemail
Looking up availability
Confirming the appointment
Detecting intent to end the call
class
CallActions
(
llm
.
FunctionContext
)
:
def
__init__
(
self
,
*
,
api
:
api
.
LiveKitAPI
,
participant
:
rtc
.
RemoteParticipant
,
room
:
rtc
.
Room
)
:
super
(
)
.
__init__
(
)
self
.
api
=
api
self
.
participant
=
participant
self
.
room
=
room
async
def
hangup
(
self
)
:
try
:
await
self
.
api
.
room
.
remove_participant
(
api
.
RoomParticipantIdentity
(
room
=
self
.
room
.
name
,
identity
=
self
.
participant
.
identity
,
)
)
except
Exception
as
e
:
# it's possible that the user has already hung up, this error can be ignored
logger
.
info
(
f"received error while ending call:
{
e
}
"
)
@llm
.
ai_callable
(
)
async
def
end_call
(
self
)
:
"""Called when the user wants to end the call"""
logger
.
info
(
f"ending the call for
{
self
.
participant
.
identity
}
"
)
await
self
.
hangup
(
)
@llm
.
ai_callable
(
)
async
def
look_up_availability
(
self
,
date
:
Annotated
[
str
,
"The date of the appointment to check availability for"
]
,
)
:
"""Called when the user asks about alternative appointment availability"""
logger
.
info
(
f"looking up availability for
{
self
.
participant
.
identity
}
on
{
date
}
"
)
asyncio
.
sleep
(
3
)
return
json
.
dumps
(
{
"available_times"
:
[
"1pm"
,
"2pm"
,
"3pm"
]
,
}
)
@llm
.
ai_callable
(
)
async
def
confirm_appointment
(
self
,
date
:
Annotated
[
str
,
"date of the appointment"
]
,
time
:
Annotated
[
str
,
"time of the appointment"
]
,
)
:
"""Called when the user confirms their appointment on a specific date. Use this tool only when they are certain about the date and time."""
logger
.
info
(
f"confirming appointment for
{
self
.
participant
.
identity
}
on
{
date
}
at
{
time
}
"
)
return
"reservation confirmed"
@llm
.
ai_callable
(
)
async
def
detected_answering_machine
(
self
)
:
"""Called when the call reaches voicemail. Use this tool AFTER you hear the voicemail greeting"""
logger
.
info
(
f"detected answering machine for
{
self
.
participant
.
identity
}
"
)
await
self
.
hangup
(
)
.
.
.
agent
=
MultimodalAgent
(
model
=
model
,
fnc_ctx
=
CallActions
(
api
=
ctx
.
api
,
participant
=
participant
,
room
=
ctx
.
room
)
,
)
Step 4: Creating a dispatch
Once your agent is up and running, you can test it by having it make a phone call.
Use LiveKit CLI to create a dispatch. Replace
+15105550100
with the phone number you want to call.
Note
You need version 2.3.0 or later of
LiveKit CLI
to use dispatch commands.
lk dispatch create
\
--new-room
\
--agent-name outbound-caller
\
--metadata
'+15105550100'
Your phone should ring. After you pick up, you're connected to the agent.
Next steps
Create an agent that
accepts inbound calls
.
Learn more about
outbound trunks
.
This guide uses Python. For more information and Node examples, see the following topics:
Dispatching agents
VoicePipelineAgent
Creating a SIP participant
On this page
Prerequisites
Outbound call flow
Step 1: Set up environment variables
Step 2: Create an outbound trunk
Step 3: Create an agent
Understanding the code
Step 4: Creating a dispatch
Next steps


Content from https://docs.livekit.io/agents/v0/quickstarts/vision:

On this page
Prerequisites
Step 1: Setup a LiveKit account and install the CLI
Step 2: Bootstrap an agent from template
Step 3: Add video-related imports
Step 4: Enable video subscription
Step 5: Add video frame handling
Step 6: Add the LLM Callback
Step 7: Update the system prompt
Step 8: Update the assistant configuration
Testing your agent
How it works
Next steps
Copy page
See more page options
Agents 1.0 available for Python
This documentation is for v0.x of the LiveKit Agents framework.
See updated documentation here:
Vision
.
v1.0 for Node.js is coming soon.
This quickstart tutorial walks you through the steps to build an AI application using Python that has access to your camera. It uses LiveKit's
Agents Framework
to create an AI-powered voice assistant that can engage in realtime conversations with users, and analyze images captured from your camera.
Prerequisites
LiveKit Cloud Project
or
open-source LiveKit server
OpenAI API Key
Python 3.9-3.12
Note
By default, the example agent uses Deepgram for STT and OpenAI for TTS and LLM. However, you aren't required
to use these providers.
Step 1: Setup a LiveKit account and install the CLI
You can skip this step if you choose to clone the template from the GitHub repository.
Create an account or sign in to your
LiveKit Cloud account
.
Install the LiveKit CLI
and authenticate using
lk cloud auth
.
Step 2: Bootstrap an agent from template
The template provides a working voice assistant to build on. The template includes:
Basic voice interaction
Audio-only track subscription
Voice activity detection (VAD)
Speech-to-text (STT)
Language model (LLM)
Text-to-speech (TTS)
Clone the starter template for a simple Python voice agent using the CLI:
lk app create
--template
voice-pipeline-agent-python
Alternatively, clone the GitHub
repository
:
git
clone https://github.com/livekit-examples/voice-pipeline-agent-python.git
Enter your
OpenAI API Key
and
Deepgram API Key
when prompted. If you aren't using Deepgram and OpenAI, see
Customizing plugins
.
Note
If you want to use OpenAI for STT as well as TTS and LLM, you can change the
stt
plugin to
openai.STT()
.
Follow the instructions in the output of the
lk app create
command or the
setup instructions
in the
README to install dependencies and start your agent.
Step 3: Add video-related imports
Add the video-related content to our agent. At the top of your
agent.py
file, add these imports
alongside the existing ones:
from
livekit
import
rtc
from
livekit
.
agents
.
llm
import
ChatMessage
,
ChatImage
These new imports include:
rtc
: Access to LiveKit's video functionality
ChatMessage
and
ChatImage
: Classes we'll use to send images to the LLM
Step 4: Enable video subscription
Find the
ctx.connect()
line in the
entrypoint
function. Change
AutoSubscribe.AUDIO_ONLY
to
AutoSubscribe.SUBSCRIBE_ALL
:
await
ctx
.
connect
(
auto_subscribe
=
AutoSubscribe
.
SUBSCRIBE_ALL
)
This enables the assistant to receive video tracks as well as audio.
Step 5: Add video frame handling
Add these two helper functions after your imports but before the
prewarm
function:
async
def
get_video_track
(
room
:
rtc
.
Room
)
:
"""Find and return the first available remote video track in the room."""
for
participant_id
,
participant
in
room
.
remote_participants
.
items
(
)
:
for
track_id
,
track_publication
in
participant
.
track_publications
.
items
(
)
:
if
track_publication
.
track
and
isinstance
(
track_publication
.
track
,
rtc
.
RemoteVideoTrack
)
:
logger
.
info
(
f"Found video track
{
track_publication
.
track
.
sid
}
"
f"from participant
{
participant_id
}
"
)
return
track_publication
.
track
raise
ValueError
(
"No remote video track found in the room"
)
This function searches through all participants to find an available video track. It's used to locate the video feed to process.
Next, add the frame capture function:
async
def
get_latest_image
(
room
:
rtc
.
Room
)
:
"""Capture and return a single frame from the video track."""
video_stream
=
None
try
:
video_track
=
await
get_video_track
(
room
)
video_stream
=
rtc
.
VideoStream
(
video_track
)
async
for
event
in
video_stream
:
logger
.
debug
(
"Captured latest video frame"
)
return
event
.
frame
except
Exception
as
e
:
logger
.
error
(
f"Failed to get latest image:
{
e
}
"
)
return
None
finally
:
if
video_stream
:
await
video_stream
.
aclose
(
)
This function captures a single frame from the video track and ensures proper cleanup of resources. Using
aclose()
releases system resources like memory buffers and video decoder instances, which helps prevent memory leaks.
Step 6: Add the LLM Callback
Inside the
entrypoint
function, add this callback function which will inject the latest video frame just before the LLM generates a response:
async
def
before_llm_cb
(
assistant
:
VoicePipelineAgent
,
chat_ctx
:
llm
.
ChatContext
)
:
"""
Callback that runs right before the LLM generates a response.
Captures the current video frame and adds it to the conversation context.
"""
latest_image
=
await
get_latest_image
(
ctx
.
room
)
if
latest_image
:
image_content
=
[
ChatImage
(
image
=
latest_image
)
]
chat_ctx
.
messages
.
append
(
ChatMessage
(
role
=
"user"
,
content
=
image_content
)
)
logger
.
debug
(
"Added latest frame to conversation context"
)
This callback is the key to efficient context management — it only adds visual information when the assistant is about to respond. If visual information was added to every message, it would quickly fill up the context window.
Step 7: Update the system prompt
Find the
initial_ctx
creation in the
entrypoint
function and update it to include vision capabilities:
initial_ctx
=
llm
.
ChatContext
(
)
.
append
(
role
=
"system"
,
text
=
(
"You are a voice assistant created by LiveKit that can both see and hear. "
"You should use short and concise responses, avoiding unpronounceable punctuation. "
"When you see an image in our conversation, naturally incorporate what you see "
"into your response. Keep visual descriptions brief but informative."
)
,
)
Step 8: Update the assistant configuration
Find the
VoicePipelineAgent
creation in the
entrypoint
function and add the callback:
assistant
=
VoicePipelineAgent
(
vad
=
ctx
.
proc
.
userdata
[
"vad"
]
,
stt
=
deepgram
.
STT
(
)
,
llm
=
openai
.
LLM
(
model
=
"gpt-4o-mini"
)
,
tts
=
openai
.
TTS
(
)
,
chat_ctx
=
initial_ctx
,
before_llm_cb
=
before_llm_cb
)
The key change here is the
before_llm_cb
parameter, which uses the callback created earlier to inject the latest video frame into the conversation context.
Testing your agent
Start your assistant (if your agent is already running, skip this step):
python agent.py dev
Connect to the LiveKit room with a client that publishes both audio and video. The easiest way to do this is by using the
Agents Playground
.
Connect to the room, and try asking your agent some questions like:
"What do you see right now?"
"Can you describe what's happening?"
"Has anything changed in the scene?"
How it works
With these changes, your assistant now:
Connects to both audio and video streams.
Listens for user speech as before.
Just before generating each response:
Captures the current video frame.
Adds it to the conversation context.
Uses it to inform the response.
Keeps the context clean by only adding frames when needed.
Next steps
For a list of additional plugins you can use, see
Integration guides for LiveKit Agents
.
Let your friends and colleagues talk to your agent by connecting it to a LiveKit
Sandbox
.
On this page
Prerequisites
Step 1: Setup a LiveKit account and install the CLI
Step 2: Bootstrap an agent from template
Step 3: Add video-related imports
Step 4: Enable video subscription
Step 5: Add video frame handling
Step 6: Add the LLM Callback
Step 7: Update the system prompt
Step 8: Update the assistant configuration
Testing your agent
How it works
Next steps


Content from https://docs.livekit.io/agents/v0/build/anatomy:

On this page
Agent lifecycle
Worker options
Entrypoint
Request handler
Prewarm function
Permissions
Worker type
Starting the worker
Copy page
See more page options
Agents 1.0 available for Python
This documentation is for v0.x of the LiveKit Agents framework.
See updated documentation here:
Worker lifecycle
.
v1.0 for Node.js is coming soon.
This guide explains the core ideas and components that make up Agents. For a step-by-step guide on building an Agent without the deep dive into its inner workings, check out the
quickstart guide
.
Agent lifecycle
The framework turns your program into an "agent" that can join a LiveKit room and interact with other participants. Here's a high-level overview of the lifecycle:
Worker registration
: When you run
myagent.py start
, it connects to LiveKit server and registers itself as a "worker" via a persistent WebSocket connection. Once registered, the app is on standby, waiting for rooms (sessions with end-users) to be created. It exchanges availability and capacity information with LiveKit server automatically, allowing for correct load balancing of incoming requests.
Agent dispatch
: When an end-user connects to a room, LiveKit server selects an available worker and sends it information about that session. The first worker to accept that request will instantiate your program and join the room. A worker can host multiple instances of your agent simultaneously, running each in its own process for isolation.
Your program
: This is where you take over. Your program can use most features of the LiveKit
Python SDK
. Agents can also leverage the
agent plugin
ecosystem to process or synthesize voice and video data.
Room close
: The room will automatically close when the last non-agent participant has left. Remaining agents will be disconnected.
Worker options
In stateful computing, a worker is similar to a web server process in traditional web architecture. The worker acts as the program's main loop, responsible for deploying and monitoring instances of the Agent on child processes. It can handle many Agent instances with minimal overhead, effectively utilizing machine resources.
When a worker experiences high load, it stops taking on new sessions and notifies LiveKit's server.
As you deploy updates to your program, the worker will gracefully drain existing sessions before shutting down, ensuring no sessions are interrupted mid-call.
The interface for creating a worker is through the
WorkerOptions
class:
Python
Node.js
opts
=
WorkerOptions
(
# entrypoint function is called when a job is assigned to this worker
# this is the only required parameter to WorkerOptions
entrypoint_fnc
,
# inspect the request and decide if the current worker should handle it.
request_fnc
,
# a function to perform any necessary initialization in a new process.
prewarm_fnc
,
# a function that reports the current system load, whether CPU or RAM, etc.
load_fnc
,
# the maximum value of load_fnc, above which new processes will not spawn
load_threshold
,
# whether the agent can subscribe to tracks, publish data, update metadata, etc.
permissions
,
# the type of worker to create, either JT_ROOM or JT_PUBLISHER
worker_type
=
WorkerType
.
ROOM
,
)
# start the worker
cli
.
run_app
(
opts
)
Note
While it is possible to supply API keys and secrets to the worker directly through
WorkerOptions
, for security reasons it is recommended to set them as environmental variables that the worker will then read in.
The full list of worker options, information about them, and their default values can be found in
the source code
.
Entrypoint
This is the main function that is called when a new job is assigned to the worker. It is the entry point for your agent's logic. The entrypoint is called
before
the agent joins the room, and is where you can set up any necessary state or configuration.
Python
Node.js
async
def
entrypoint
(
ctx
:
JobContext
)
:
# connect to the room
await
ctx
.
connect
(
)
# handle the session
.
.
.
For details about the entrypoint function, refer to the
Inside a session
section.
Request handler
The
request_fnc
function is executed each time that the server has a job for the agent. The framework expects workers to explicitly accept or reject each job request. If you accept the request, your entrypoint function will be called. If the request is rejected, it'll be sent to the next available worker.
By default, if left blank, the behavior is to auto-accept all requests dispatched to the worker.
Python
Node.js
async
def
request_fnc
(
req
:
JobRequest
)
:
# accept the job request
await
req
.
accept
(
# the agent's name (Participant.name), defaults to ""
name
=
"agent"
,
# the agent's identity (Participant.identity), defaults to "agent-<jobid>"
identity
=
"identity"
,
# attributes to set on the agent participant upon join
attributes
=
{
"myagent"
:
"rocks"
}
,
)
# or reject it
# await req.reject()
opts
=
WorkerOptions
(
entrypoint_fnc
=
entrypoint
,
request_fnc
=
request_fnc
)
Prewarm function
For isolation and performance reasons, the framework runs each agent session in its own process. Agents often need access to model files that take time to load. To address this, the prewarm function can be used to warm up the process before assigning any jobs to it. You can control the number of processes to keep warm using the
num_idle_processes
parameter.
Python
Node.js
def
prewarm_fnc
(
proc
:
JobProcess
)
:
# load silero weights and store to process userdata
proc
.
userdata
[
"vad"
]
=
silero
.
VAD
.
load
(
)
async
def
entrypoint
(
ctx
:
JobContext
)
:
# access the loaded silero instance
vad
:
silero
.
VAD
=
ctx
.
proc
.
userdata
[
"vad"
]
opts
=
WorkerOptions
(
entrypoint_fnc
=
entrypoint
,
prewarm_fnc
=
prewarm_fnc
)
Permissions
By default, agents are allowed to both publish and subscribe from the others in the same Room. However, you can customize these permissions by setting the
permissions
parameter in
WorkerOptions
.
Python
Node.js
opts
=
WorkerOptions
(
.
.
.
permissions
=
WorkerPermissions
(
can_publish
=
True
,
can_subscribe
=
True
,
# when set to true, the agent won't be visible to others in the room.
# when hidden, it will also not be able to publish tracks to the room as it won't be visible.
hidden
=
False
,
)
,
)
Worker type
You can choose to start a new instance of the agent for each room or for each publisher in the room. This can be set when you register your worker:
Python
Node.js
opts
=
WorkerOptions
(
.
.
.
# when omitted, the default is WorkerType.ROOM
worker_type
=
WorkerType
.
ROOM
,
)
The
WorkerType
enum has two options:
ROOM
: A new instance of the agent is created for each room.
PUBLISHER
: A new instance of the agent is created for each publisher in the room.
If the agent is performing resource-intensive operations in a room that could potentially include multiple publishers (for example, processing incoming video from a set of security cameras), it may be desirable to set
worker_type
to
JT_PUBLISHER
to ensure that each publisher has its own instance of the agent.
For
PUBLISHER
jobs, the
entrypoint
function will be called once for each publisher in the room. The
JobContext.publisher
object will contain a
RemoteParticipant
representing that publisher.
Starting the worker
Finally, to spin up a worker with the configuration defined using
WorkerOptions
, call the CLI:
Python
Node.js
if
__name__
==
"__main__"
:
cli
.
run_app
(
opts
)
The Agents worker CLI provides two subcommands:
start
and
dev
. The former outputs raw JSON data to stdout, and is recommended for production.
dev
is recommended to use for development, as it outputs human-friendly colored logs, and supports hot reloading on Python.
On this page
Agent lifecycle
Worker options
Entrypoint
Request handler
Prewarm function
Permissions
Worker type
Starting the worker


Content from https://docs.livekit.io/agents/v0/build/dispatch/:

On this page
Automatic agent dispatch
Explicit agent dispatch
Dispatch via API
Dispatch from inbound SIP calls
Dispatch on participant connection
Copy page
See more page options
Agents 1.0 available for Python
This documentation is for v0.x of the LiveKit Agents framework.
See updated documentation here:
Agent dispatch
.
v1.0 for Node.js is coming soon.
As part of an
agent's lifecycle
, agent workers register with the LiveKit server and remain idle until they are assigned to a room to interact with end users. The process of assigning an agent to a room is called dispatching an agent.
LiveKit’s dispatch system is optimized for concurrency and low latency. It supports hundreds of thousands of new connections per second and dispatches an agent to a room in under 150ms.
Automatic agent dispatch
By default, agents are automatically dispatched when rooms are created. If a participant connects to LiveKit and the room does not already exist, it is created automatically, and an agent is assigned to it.
Automatic dispatch is the best option if the same agent needs to be assigned to new participants consistently.
Explicit agent dispatch
For greater control over when and how agents join rooms, explicit dispatch is available. This approach leverages the same worker systems, allowing you to run agent workers in the same way.
The key difference is that when registering the agent, the
agent_name
field in
WorkerOptions
needs to be set.
Python
Node.js
opts
=
WorkerOptions
(
.
.
.
agent_name
=
"test-agent"
,
)
Important
The agent will not be automatically dispatched to any newly created rooms when the
agent_name
is set.
Dispatch via API
Agents running with an
agent_name
set can be explicitly dispatched to a room via
AgentDispatchService
.
Python
Node.js
LiveKit CLI
Go
import
asyncio
from
livekit
import
api
room_name
=
"my-room"
agent_name
=
"test-agent"
async
def
create_explicit_dispatch
(
)
:
lkapi
=
api
.
LiveKitAPI
(
)
dispatch
=
await
lkapi
.
agent_dispatch
.
create_dispatch
(
api
.
CreateAgentDispatchRequest
(
agent_name
=
agent_name
,
room
=
room_name
,
metadata
=
"my_job_metadata"
)
)
print
(
"created dispatch"
,
dispatch
)
dispatches
=
await
lkapi
.
agent_dispatch
.
list_dispatch
(
room_name
=
room_name
)
print
(
f"there are
{
len
(
dispatches
)
}
dispatches in
{
room_name
}
"
)
await
lkapi
.
aclose
(
)
asyncio
.
run
(
create_explicit_dispatch
(
)
)
If
my-room
does not exist, it is created automatically during dispatch, and an instance of
test-agent
is assigned to it.
Handling job metadata
The metadata set during dispatch is included in the Job metadata passed to the agent.
Python
Node.js
async
def
entrypoint
(
ctx
:
JobContext
)
:
logger
.
info
(
f"job metadata:
{
ctx
.
job
.
metadata
}
"
)
.
.
.
Dispatch from inbound SIP calls
Agents can be explicitly dispatched for inbound SIP calls.
SIP dispatch rules
can define one or more agents using the
room_config.agents
field.
Explicitly specifying agents with SIP inbound calls is recommended over automatic dispatch, as it allows multiple agents within a single project.
Dispatch on participant connection
A participant’s token can be configured to dispatch one or more agents immediately upon connection.
To dispatch multiple agents, include multiple
RoomAgentDispatch
entries in
RoomConfiguration
.
Python
Node.js
Go
from
livekit
.
api
import
(
AccessToken
,
RoomAgentDispatch
,
RoomConfiguration
,
VideoGrants
,
)
room_name
=
"my-room"
agent_name
=
"test-agent"
def
create_token_with_agent_dispatch
(
)
-
>
str
:
token
=
(
AccessToken
(
)
.
with_identity
(
"my_participant"
)
.
with_grants
(
VideoGrants
(
room_join
=
True
,
room
=
room_name
)
)
.
with_room_config
(
RoomConfiguration
(
agents
=
[
RoomAgentDispatch
(
agent_name
=
"test-agent"
,
metadata
=
"my_metadata"
)
]
,
)
,
)
.
to_jwt
(
)
)
return
token
On this page
Automatic agent dispatch
Explicit agent dispatch
Dispatch via API
Dispatch from inbound SIP calls
Dispatch on participant connection


Content from https://docs.livekit.io/agents/v0/build/session:

On this page
Entrypoint
Customizing for participant
Ending the session
Disconnecting the agent
Disconnecting everyone
Post-processing and cleanup
Copy page
See more page options
Agents 1.0 available for Python
This documentation is for v0.x of the LiveKit Agents framework.
See updated documentation here:
Job lifecycle
.
v1.0 for Node.js is coming soon.
Once the worker accepts the job, the framework starts a new process to run your entrypoint function with the context of that specific job.
Each session runs in a separate process to ensure that agents are isolated from each other. This isolation means that if an agent instance crashes, it will not affect other agents running on the same worker.
Entrypoint
The entrypoint function is called as soon as the job is assigned to the worker. From there, you have full control over the session. The session will continue to run until all human participants have left the room, or when the job is explicitly shut down.
Python
Node.js
async
def
do_something
(
track
:
rtc
.
RemoteAudioTrack
)
:
audio_stream
=
rtc
.
AudioStream
(
track
)
async
for
event
in
audio_stream
:
# Do something here to process event.frame
pass
await
audio_stream
.
aclose
(
)
async
def
entrypoint
(
ctx
:
JobContext
)
:
# an rtc.Room instance from the LiveKit Python SDK
room
=
ctx
.
room
# set up listeners on the room before connecting
@room
.
on
(
"track_subscribed"
)
def
on_track_subscribed
(
track
:
rtc
.
Track
,
*
_
)
:
if
track
.
kind
==
rtc
.
TrackKind
.
KIND_AUDIO
:
asyncio
.
create_task
(
do_something
(
track
)
)
# connect to room
await
ctx
.
connect
(
auto_subscribe
=
AutoSubscribe
.
AUDIO_ONLY
)
# when connected, room.local_participant represents the agent
await
room
.
local_participant
.
publish_data
(
"hello world"
)
# iterate through currently connected remote participants
for
rp
in
room
.
remote_participants
.
values
(
)
:
print
(
rp
.
identity
)
Working examples of LiveKit Agents for Python are available in the
repository
. More on publishing and receiving tracks will be expanded in a later section.
Customizing for participant
The agent can be customized to behave differently based on the connected participant, enabling a personalized experience.
LiveKit provides several ways to identify participants:
ctx.room.name
: the name that the participant is connected to
participant.identity
: the identity of the participant
participant.attributes
:
custom attributes
set on the participant
Here's an example:
Python
Node.js
async
def
entrypoint
(
ctx
:
JobContext
)
:
# connect to the room
await
ctx
.
connect
(
auto_subscribe
=
AutoSubscribe
.
AUDIO_ONLY
)
# wait for the first participant to arrive
participant
=
await
ctx
.
wait_for_participant
(
)
# customize behavior based on the participant
print
(
f"connected to room
{
ctx
.
room
.
name
}
with participant
{
participant
.
identity
}
"
)
# inspect the current value of the attribute
language
=
participant
.
attributes
.
get
(
"user.language"
)
# listen to when the attribute is changed
@ctx
.
room
.
on
(
"participant_attributes_changed"
)
def
on_participant_attributes_changed
(
changed_attrs
:
dict
[
str
,
str
]
,
p
:
rtc
.
Participant
)
:
if
p
==
participant
:
language
=
p
.
attributes
.
get
(
"user.language"
)
print
(
f"participant
{
p
.
identity
}
changed language to
{
language
}
"
)
Ending the session
Disconnecting the agent
When the agent should no longer be in the room, you could disconnect it from the room. This will allow the other participants in the session to continue. After disconnection, your shutdown hooks will also be called.
Python
Node.js
async
def
entrypoint
(
ctx
:
JobContext
)
:
# do some work
.
.
.
# disconnect from the room
ctx
.
shutdown
(
reason
=
"Session ended"
)
Disconnecting everyone
If the session should end for everyone, use the server API
deleteRoom
to end the session.
The
Disconnected
room event
will be sent, and the room will be removed from the server.
Python
Node.js
from
livekit
import
api
async
def
entrypoint
(
ctx
:
JobContext
)
:
# do some work
.
.
.
api_client
=
api
.
LiveKitAPI
(
os
.
getenv
(
"LIVEKIT_URL"
)
,
os
.
getenv
(
"LIVEKIT_API_KEY"
)
,
os
.
getenv
(
"LIVEKIT_API_SECRET"
)
,
)
await
api_client
.
room
.
delete_room
(
api
.
DeleteRoomRequest
(
room
=
ctx
.
job
.
room
.
name
,
)
)
Post-processing and cleanup
After the session has ended, you may want to perform post-processing or cleanup tasks. This could include saving user state in a database. Agents framework supports shutdown hooks that are called when the session ends.
Python
Node.js
async
def
entrypoint
(
ctx
:
JobContext
)
:
async
def
my_shutdown_hook
(
)
:
# save user state
.
.
.
ctx
.
add_shutdown_callback
(
my_shutdown_hook
)
Note
Shutdown hooks are expected to complete within a short amount of time. By default, the framework waits 60 seconds before forcefully terminating the agent process. You can adjust this timeout using the
shutdown_process_timeout
parameter in
WorkerOptions
.
On this page
Entrypoint
Customizing for participant
Ending the session
Disconnecting the agent
Disconnecting everyone
Post-processing and cleanup


Content from https://docs.livekit.io/agents/v0/build/tracks:

On this page
Receiving
Working with video
Publishing
Publishing audio
Publishing video
Copy page
See more page options
Agents 1.0 available for Python
This documentation is for v0.x of the LiveKit Agents framework.
See updated documentation here:
Agent speech and audio
.
v1.0 for Node.js is coming soon.
Receiving
Reading WebRTC tracks via LiveKit is done through streams, which are exposed in Python as AsyncIterators. The LiveKit SDKs provide utilities for working with both audio and video tracks:
Python
Node.js
async
def
do_something
(
track
:
rtc
.
Track
)
:
if
track
.
kind
==
rtc
.
TrackKind
.
KIND_AUDIO
:
audio_stream
=
rtc
.
AudioStream
(
track
)
async
for
event
in
audio_stream
:
# Do something here to process event.frame
pass
await
audio_stream
.
aclose
(
)
elif
track
.
kind
==
rtc
.
TrackKind
.
KIND_VIDEO
:
video_stream
=
rtc
.
VideoStream
(
track
)
async
for
event
in
video_stream
:
# Do something here to process event.frame
pass
await
video_stream
.
aclose
(
)
@ctx
.
room
.
on
(
"track_subscribed"
)
def
on_track_subscribed
(
track
:
rtc
.
Track
,
publication
:
rtc
.
TrackPublication
,
participant
:
rtc
.
RemoteParticipant
,
)
:
if
track
.
kind
==
rtc
.
TrackKind
.
KIND_AUDIO
:
asyncio
.
create_task
(
do_something
(
track
)
)
elif
track
.
kind
==
rtc
.
TrackKind
.
KIND_VIDEO
:
asyncio
.
create_task
(
do_something
(
track
)
)
As in every LiveKit SDK, the
TrackSubscribed
event
is triggered when a track is subscribed to. The agent can be configured to automatically subscribe to tracks when you accept the job. To manage subscriptions manually, set
auto_subscribe
to
AutoSubscribe.SUBSCRIBE_NONE
:
Python
Node.js
async
def
entrypoint_fnc
(
ctx
:
JobContext
)
:
await
ctx
.
connect
(
# valid values are SUBSCRIBE_ALL, SUBSCRIBE_NONE, VIDEO_ONLY, AUDIO_ONLY
# when omitted, it defaults to SUBSCRIBE_ALL
auto_subscribe
=
AutoSubscribe
.
SUBSCRIBE_NONE
,
)
Note
All subscribed tracks will be streamed to the machine. To ensure efficient use of resources, subscribe only to the tracks that your agent needs.
Working with video
Because different applications work with different video buffer encodings, LiveKit supports many and translates between them automatically.
VideoFrame
provides the current video buffer type and a method to convert it to any of the other encodings:
Python
Node.js
async
def
handle_video
(
track
:
rtc
.
Track
)
:
video_stream
=
rtc
.
VideoStream
(
track
)
async
for
event
in
video_stream
:
video_frame
=
event
.
frame
current_type
=
video_frame
.
type
frame_as_bgra
=
video_frame
.
convert
(
rtc
.
VideoBufferType
.
BGRA
)
# [...]
await
video_stream
.
aclose
(
)
@ctx
.
room
.
on
(
"track_subscribed"
)
def
on_track_subscribed
(
track
:
rtc
.
Track
,
publication
:
rtc
.
TrackPublication
,
participant
:
rtc
.
RemoteParticipant
,
)
:
if
track
.
kind
==
rtc
.
TrackKind
.
KIND_VIDEO
:
asyncio
.
create_task
(
handle_video
(
track
)
)
Publishing
Similarly, publishing data to the agent’s track is done by transmitting a continuous live feed. Audio streams carry raw PCM data at a specified sample rate and channel count, while video streams can transmit data in any of 11 buffer encodings.
Publishing audio
Publishing audio involves splitting the stream into audio frames of a configurable length. An internal buffer holds 50ms of queued audio to be sent to the realtime stack. The
capture_frame
method, used to send new frames, is blocking and will not return control until the buffer has taken in the entire frame. This allows for easier interruption handling.
In order to publish an audio track, the sample rate and number of channels need to be determined beforehand, as well as the length (number of samples) of each frame. The following example transmits a constant 16-bit sine wave at 48kHz in 10ms long frames:
Python
Node.js
SAMPLE_RATE
=
48000
NUM_CHANNELS
=
1
# mono audio
AMPLITUDE
=
2
**
8
-
1
SAMPLES_PER_CHANNEL
=
480
# 10ms at 48kHz
async
def
entrypoint
(
ctx
:
JobContext
)
:
await
ctx
.
connect
(
)
source
=
rtc
.
AudioSource
(
SAMPLE_RATE
,
NUM_CHANNELS
)
track
=
rtc
.
LocalAudioTrack
.
create_audio_track
(
"example-track"
,
source
)
# since the agent is a participant, our audio I/O is its "microphone"
options
=
rtc
.
TrackPublishOptions
(
source
=
rtc
.
TrackSource
.
SOURCE_MICROPHONE
)
# ctx.agent is an alias for ctx.room.local_participant
publication
=
await
ctx
.
agent
.
publish_track
(
track
,
options
)
frequency
=
440
async
def
_sinewave
(
)
:
audio_frame
=
rtc
.
AudioFrame
.
create
(
SAMPLE_RATE
,
NUM_CHANNELS
,
SAMPLES_PER_CHANNEL
)
audio_data
=
np
.
frombuffer
(
audio_frame
.
data
,
dtype
=
np
.
int16
)
time
=
np
.
arange
(
SAMPLES_PER_CHANNEL
)
/
SAMPLE_RATE
total_samples
=
0
while
True
:
time
=
(
total_samples
+
np
.
arange
(
SAMPLES_PER_CHANNEL
)
)
/
SAMPLE_RATE
sinewave
=
(
AMPLITUDE
*
np
.
sin
(
2
*
np
.
pi
*
frequency
*
time
)
)
.
astype
(
np
.
int16
)
np
.
copyto
(
audio_data
,
sinewave
)
# send this frame to the track
await
source
.
capture_frame
(
frame
)
total_samples
+=
samples_per_channel
Warning
When streaming finite audio (e.g. from a file), make sure the frame length isn't longer than the amount of samples left to stream, otherwise the end of the buffer will consist of noise.
Publishing video
When publishing video tracks, the frame rate and buffer encoding of the video need to be established beforehand. In this example, the Agent connects to the room and starts publishing a solid color frame at 10 frames per second:
Python
Node.js
WIDTH
=
640
HEIGHT
=
480
async
def
entrypoint
(
ctx
:
JobContext
)
:
await
ctx
.
connect
(
)
source
=
rtc
.
VideoSource
(
WIDTH
,
HEIGHT
)
track
=
rtc
.
LocalVideoTrack
.
create_video_track
(
"example-track"
,
source
)
options
=
rtc
.
TrackPublishOptions
(
# since the agent is a participant, our video I/O is its "camera"
source
=
rtc
.
TrackSource
.
SOURCE_CAMERA
,
simulcast
=
True
,
# when modifying encoding options, max_framerate and max_bitrate must both be set
video_encoding
=
rtc
.
VideoEncoding
(
max_framerate
=
30
,
max_bitrate
=
3_000_000
,
)
,
audio_encoding
=
rtc
.
AudioEncoding
(
max_bitrate
=
48000
)
,
video_codec
=
rtc
.
VideoCodec
.
H264
,
)
publication
=
await
ctx
.
agent
.
publish_track
(
track
,
options
)
# this color is encoded as ARGB. when passed to VideoFrame it gets re-encoded.
COLOR
=
[
255
,
255
,
0
,
0
]
;
# FFFF0000 RED
async
def
_draw_color
(
)
:
argb_frame
=
bytearray
(
WIDTH
*
HEIGHT
*
4
)
while
True
:
await
asyncio
.
sleep
(
0.1
)
# 10 fps
argb_frame
[
:
]
=
COLOR
*
WIDTH
*
HEIGHT
frame
=
rtc
.
VideoFrame
(
WIDTH
,
HEIGHT
,
rtc
.
VideoBufferType
.
RGBA
,
argb_frame
)
# send this frame to the track
source
.
capture_frame
(
frame
)
asyncio
.
create_task
(
_draw_color
(
)
)
Note
Although the frame being published is static, it is still necessary to stream it continuously, for the benefit of participants joining the room after the initial frame had been sent.
Note
Unlike audio, video
capture_frame
does not keep an internal buffer.
On this page
Receiving
Working with video
Publishing
Publishing audio
Publishing video


Content from https://docs.livekit.io/agents/v0/build/metrics:

On this page
Overview
Logging events
Aggregating metrics
Metrics reference
Speech-to-text (STT)
LLM
Text-to-speech (TTS)
End-of-utterance (EOU)
Measuring conversation latency
Copy page
See more page options
Agents 1.0 available for Python
This documentation is for v0.x of the LiveKit Agents framework.
See updated documentation here:
Logging and metrics
.
v1.0 for Node.js is coming soon.
Overview
For increased observability into the performance and model usage by your agent, you can enable and log detailed metrics that are provided by LiveKit Agents. These metrics provide detailed insights into the duration, latency, and usage across the stages of a session and are provided for both
VoicePipelineAgent
and
MultimodalAgent
.
Logging events
Agent metrics events are fired by LiveKit Agents whenever there is a new metrics object available during an active session.
In order to capture newly available metrics objects in your agent, import the
metrics
module from LiveKit Agents and subscribe to the
metrics_collected
event. When your agent receives this event, log the available metrics in your agent.
The
metrics
module includes a simple helper function which formats logging output based upon the type of metrics received in the event. Call the
log_metrics
helper function when you receive a new
metrics_collected
event in order to utilize this formatting for your logs.
Python
Node.js
# The metrics module is required to capture agent metrics
from
livekit
.
agents
import
metrics
# Subscribe to metrics collection events and process accordingly
@agent
.
on
(
"metrics_collected"
)
def
_on_metrics_collected
(
mtrcs
:
metrics
.
AgentMetrics
)
:
# Use this helper to format and log based on metrics type
metrics
.
log_metrics
(
mtrcs
)
Aggregating metrics
The
metrics
module also includes a helper class that can be used to aggregate usage metrics over the course of a session and generate a summary once the session is complete. Use the
UsageCollector
class
Python
Node.js
# Use the usage collector to aggregate agent usage metrics
usage_collector
=
metrics
.
UsageCollector
(
)
# Add metrics to usage collector as they are received
@agent
.
on
(
"metrics_collected"
)
def
_on_metrics_collected
(
mtrcs
:
metrics
.
AgentMetrics
)
:
# Pass the latest usage metrics to the usage collector for aggregation
usage_collector
.
collect
(
mtrcs
)
# Log aggregated summary of usage metrics generated by usage collector
async
def
log_usage
(
)
:
summary
=
usage_collector
.
get_summary
(
)
logger
.
info
(
f"Usage: $
{
summary
}
"
)
# At shutdown, generate and log the summary from the usage collector
ctx
.
add_shutdown_callback
(
log_usage
)
Metrics reference
Note
The following metric types are available for
VoicePipelineAgent
.
Speech-to-text (STT)
STT metrics events are reported by the
metrics
module when the STT model being used by the agent has generated output.
Python
Node.js
Metric
Description
audio_duration
The duration (seconds) of the audio input received by the STT model.
duration
The total amount of time (seconds) that the connection has been open with the STT provider.
LLM
LLM metrics events are reported by the
metrics
module when the LLM being used by the agent has generated a completion.
Python
Node.js
Metric
Description
ttft
Time to first token. The amount of time (seconds) that it took for the LLM to generate the first token of the completion.
input_tokens
The number of tokens provided in the prompt sent to the LLM.
output_tokens
The number of tokens generated by the LLM in the completion.
tokens_per_second
The rate of token generation (tokens/second) by the LLM to generate the completion.
Text-to-speech (TTS)
TTS metrics events are reported by the
metrics
module when the TTS model being used by the agent has generated output.
Python
Node.js
Metric
Description
ttfb
Time to first byte. The amount of time (seconds) that it took for the TTS model to generate the first byte of its audio output.
audio_duration
The duration (seconds) of the audio output generated by the TTS model.
End-of-utterance (EOU)
EOU metrics events are reported by the
metrics
module when the agent is about to play speech back to the user.
Python
Node.js
Metric
Description
end_of_utterance_delay
Total amount of time (seconds) between when VAD detected end of speech and when LLM inference was performed.
transcription_delay
The amount of time (seconds) for the STT model to generate a final transcript.
Measuring conversation latency
Total conversation latency is defined as the time it takes for the agent to respond to a user's utterance. Given the metrics above, it can be computed as follows:
Python
Node.js
total_latency
=
eou
.
end_of_utterance_delay
+
llm
.
ttft
+
tts
.
ttfb
On this page
Overview
Logging events
Aggregating metrics
Metrics reference
Speech-to-text (STT)
LLM
Text-to-speech (TTS)
End-of-utterance (EOU)
Measuring conversation latency


Content from https://docs.livekit.io/agents/v0/build/record:

On this page
Video or audio recording
Example
Transcriptions
Copy page
See more page options
Agents 1.0 available for Python
This documentation is for v0.x of the LiveKit Agents framework.
See updated documentation here:
Session recording and transcripts
.
v1.0 for Node.js is coming soon.
There are multiple reasons you might want to record AI agent conversations with users. These reasons might include monitoring agent
performance, evaluating customer satisfaction, or for regulatory compliance. LiveKit allows you to record
video and audio of AI agent conversations or save conversations as text transcripts.
Video or audio recording
LiveKit's
Egress feature
provides flexible options for recording audio and/or video. Start a
room composite recorder
in your agent's entrypoint to record a session. Room recording starts when the agent enters a room with a user and captures all the participants and interactions in a LiveKit room. Recording ends when all participants leave and the room is closed.
LiveKit egress requires access to a cloud storage provider to upload the recording files. The following example uses
Google Cloud Storage, but you can also save files to any Amazon S3-compatible storage provider or Azure Blob Storage.
Example
This example uses the
VoicePipelineAgent template
as a starting point.
Clone the
repo
or run the following LiveKit CLI command:
lk app create
--template
=
voice-pipeline-agent-python my-recording-app
After you run the command, follow the instructions in the command output to finish setup.
Update the
agent.py
file to import
livekit.api
:
from
livekit
import
api
Update the
entrypoint
function to add room recording.
This example uses Google Cloud Storage. The
credentials.json
file includes authentication credentials for accessing
the bucket. For additional egress examples using Amazon S3 and Azure, see the
Egress examples
. To learn more about
credentials.json
, see
Cloud storage configurations
.
Replace
<my-bucket>
and update the entrypoint function with the following:
Note
To record only audio, update the
audio_only
parameter to
True
and remove the
preset
parameter.
async
def
entrypoint
(
ctx
:
JobContext
)
:
# Get GCP credentials from credentials.json file.
file_contents
=
""
with
open
(
"/path/to/credentials.json"
,
"r"
)
as
f
:
file_contents
=
f
.
read
(
)
# Set up recording
req
=
api
.
RoomCompositeEgressRequest
(
room_name
=
"my-room"
,
layout
=
"speaker"
,
preset
=
api
.
EncodingOptionsPreset
.
H264_720P_30
,
audio_only
=
False
,
segment_outputs
=
[
api
.
SegmentedFileOutput
(
filename_prefix
=
"my-output"
,
playlist_name
=
"my-playlist.m3u8"
,
live_playlist_name
=
"my-live-playlist.m3u8"
,
segment_duration
=
5
,
gcp
=
api
.
GCPUpload
(
credentials
=
file_contents
,
bucket
=
"<my-bucket>"
,
)
,
)
]
,
)
lkapi
=
api
.
LiveKitAPI
(
)
res
=
await
lkapi
.
egress
.
start_room_composite_egress
(
req
)
initial_ctx
=
llm
.
ChatContext
(
)
.
append
(
role
=
"system"
,
text
=
(
"You are a voice assistant created by LiveKit. Your interface with users will be voice. "
"You should use short and concise responses, and avoiding usage of unpronouncable punctuation. "
"You were created as a demo to showcase the capabilities of LiveKit's agents framework."
)
,
)
logger
.
info
(
f"connecting to room
{
ctx
.
room
.
name
}
"
)
await
ctx
.
connect
(
auto_subscribe
=
AutoSubscribe
.
AUDIO_ONLY
)
# Wait for the first participant to connect
participant
=
await
ctx
.
wait_for_participant
(
)
logger
.
info
(
f"starting voice assistant for participant
{
participant
.
identity
}
"
)
# This project is configured to use Deepgram STT, OpenAI LLM and TTS plugins
# Other great providers exist like Cartesia and ElevenLabs
# Learn more and pick the best one for your app:
# https://docs.livekit.io/agents/v0/integrations
agent
=
VoicePipelineAgent
(
vad
=
ctx
.
proc
.
userdata
[
"vad"
]
,
stt
=
deepgram
.
STT
(
)
,
llm
=
openai
.
LLM
(
model
=
"gpt-4o-mini"
)
,
tts
=
openai
.
TTS
(
)
,
chat_ctx
=
initial_ctx
,
)
agent
.
start
(
ctx
.
room
,
participant
)
# The agent should be polite and greet the user when it joins :)
await
agent
.
say
(
"Hey, how can I help you today?"
,
allow_interruptions
=
True
)
await
lkapi
.
aclose
(
)
Start the agent:
python3 agent
.
py dev
Recording starts when a participant joins a room and the agent is dispatched to that room. After the participant
leaves the room, the recording stops. Files are uploaded to storage as they're recorded.
Transcriptions
This section describes creating a text log of a conversation by the agent process (that is, server side). For transcriptions for your frontend applications, see
Transcriptions
.
You can save the text of a conversation with an AI voice agent by listening for agent events and logging user and agent speech to a text file. For example, log messages when user speech is committed (
user_speech_committed
) and when the agent stops speaking (
agent_stopped_speaking
).
For a list of events emitted by agents, see the following topics:
VoicePipelineAgent events
MultimodalAgent events
For example code in Python, see this example of a
MultimodalAgent that saves conversation to a text file
.
On this page
Video or audio recording
Example
Transcriptions


Content from https://docs.livekit.io/agents/v0/integrations/plugins:

On this page
STT
Voice activity detector (VAD) and StreamAdapter
TTS
Building your own
LiveKit plugins
Copy page
See more page options
Agents 1.0 available for Python
This documentation is for v0.x of the LiveKit Agents framework.
See updated documentation here:
Integration guides
.
v1.0 for Node.js is coming soon.
The Agents framework includes a set of prebuilt plugins that make it easier to build an AI agent. These plugins cover common tasks like speech-to-text (STT), text-to-speech (TTS), running inference on a generative AI model, and more.
The API for plugins is standardized to make it easy to switch between different providers. Having a consistent interface also makes it simpler for anyone to extend the framework and build new plugins for other providers.
STT
STT converts audio frames to a stream of text. The following example uses Deepgram STT to process an audio stream:
from
livekit
import
agents
,
rtc
from
livekit
.
plugins
import
deepgram
from
livekit
.
agents
.
stt
import
SpeechEventType
,
SpeechEvent
from
typing
import
AsyncIterable
async
def
process_track
(
ctx
:
agents
.
JobContext
,
track
:
rtc
.
Track
)
:
stt
=
deepgram
.
STT
(
)
stt_stream
=
stt
.
stream
(
)
audio_stream
=
rtc
.
AudioStream
(
track
)
ctx
.
create_task
(
process_text_from_speech
(
stt_stream
)
)
async
for
audio_event
in
audio_stream
:
stt_stream
.
push_frame
(
audio_event
.
frame
)
stt_stream
.
end_input
(
)
async
def
process_text_from_speech
(
self
,
stream
:
AsyncIterable
[
SpeechEvent
]
)
:
async
for
event
in
stream
:
if
event
.
type
==
SpeechEventType
.
FINAL_TRANSCRIPT
:
text
=
event
.
alternatives
[
0
]
.
text
# Do something with text
elif
event
.
type
==
SpeechEventType
.
INTERIM_TRANSCRIPT
:
pass
elif
event
.
type
==
SpeechEventType
.
START_OF_SPEECH
:
pass
elif
event
.
type
==
SpeechEventType
.
END_OF_SPEECH
:
pass
await
stream
.
aclose
(
)
Voice activity detector (VAD) and StreamAdapter
Some providers or models, such as Whisper, do not support streaming input. In these cases, the application must determine when a chunk of audio represents a
complete segment of speech. This can be accomplished using a VAD together with the
StreamAdapter
class.
The following example modifies the example above to use VAD and StreamAdapter:
from
livekit
import
agents
,
rtc
from
livekit
.
plugins
import
openai
,
silero
async
def
process_track
(
ctx
:
agents
.
JobContext
,
track
:
rtc
.
Track
)
:
whisper_stt
=
openai
.
STT
(
)
vad
=
silero
.
VAD
.
load
(
min_speech_duration
=
0.1
,
min_silence_duration
=
0.5
,
)
vad_stream
=
vad
.
stream
(
)
# StreamAdapter will buffer audio until VAD emits END_SPEAKING event
stt
=
agents
.
stt
.
StreamAdapter
(
whisper_stt
,
vad_stream
)
stt_stream
=
stt
.
stream
(
)
.
.
.
TTS
TTS synthesizes text into audio frames. The following example uses ElevenLabs TTS to convert text input into audio and
plays the audio stream:
from
livekit
import
agents
,
rtc
from
livekit
.
agents
.
tts
import
SynthesizedAudio
from
livekit
.
plugins
import
elevenlabs
from
typing
import
AsyncIterable
ctx
:
agents
.
JobContext
=
.
.
.
text_stream
:
AsyncIterable
[
str
]
=
.
.
.
audio_source
=
rtc
.
AudioSource
(
44100
,
1
)
track
=
rtc
.
LocalAudioTrack
.
create_audio_track
(
"agent-audio"
,
audio_source
)
await
ctx
.
room
.
local_participant
.
publish_track
(
track
)
tts
=
elevenlabs
.
TTS
(
model_id
=
"eleven_turbo_v2"
)
tts_stream
=
tts
.
stream
(
)
# create a task to consume and publish audio frames
ctx
.
create_task
(
send_audio
(
tts_stream
)
)
# push text into the stream, TTS stream will emit audio frames along with events
# indicating sentence (or segment) boundaries.
async
for
text
in
text_stream
:
tts_stream
.
push_text
(
text
)
tts_stream
.
end_input
(
)
async
def
send_audio
(
audio_stream
:
AsyncIterable
[
SynthesizedAudio
]
)
:
async
for
a
in
audio_stream
:
await
audio_source
.
capture_frame
(
e
.
audio
.
frame
)
Building your own
The plugin framework is designed to be extensible, allowing anyone to build their own plugin. Your plugin can integrate with various providers or directly load models for local inference.
By adopting the standard STT or TTS interfaces, you can abstract away implementation specifics and simplify switching between different providers in your agent code.
Code contributions to plugins are always welcome. To learn more, see the guidelines for contributions to
the
Python repository
or
the
Node.js repository
.
LiveKit plugins
The following plugins provide utilities for LiveKit agents. For a list of plugins for providers of LLM, STT, and TTS,
see
Integration guides for LiveKit Agents
.
Plugin
SDK
Feature
livekit-plugins-browser
Python
Chrome browser.
livekit-plugins-llama-index
Python
Support for LlamaIndex
query engine
and
chat engine
. Query engine is used primarily for RAG.
Chat engine can be used as an LLM in a pipeline agent.
livekit-plugins-nltk
Python
Utilities for working with text using
NLTK
.
livekit-plugins-rag
Python
Vector retrieval with
Annoy
.
livekit-plugins-silero
Python
,
Node.js
Silero VAD
.
livekit-plugins-turn-detector
Python
LiveKit
turn detector
.
On this page
STT
Voice activity detector (VAD) and StreamAdapter
TTS
Building your own
LiveKit plugins


Content from https://docs.livekit.io/agents/v0/integrations/openai-compatible-llms:

On this page
Customize the LLM for your voice agent
Example voice agent
Supported LLMs
Method name
Syntax
Parameters
Agents 1.0 available for Python
This documentation is for v0.x of the LiveKit Agents framework.
See updated documentation here:
LLM integration guides
.
v1.0 for Node.js is coming soon.
Customize the LLM for your voice agent
The OpenAI plugin provides methods that allow you to use OpenAI API compatible LLMs. In most cases, you can call the method with no parameter values by setting the required environment variables and accepting default values. The minimal syntax for each method assumes the environment variables for required values (for example, API keys) are set.
Support for additional LLMs is available through other
LiveKit plugins
.
Example voice agent
Use the
VoicePipelineAgent
class and the OpenAI plugin to specify the LLM. In this example, use Groq as the LLM:
Set the
GROQ_API_KEY
environment variable:
export
GROQ_API_KEY
=
<
your_groq_api_key
>
Create an agent:
Python
Node.js
agent
=
VoicePipelineAgent
(
vad
=
ctx
.
proc
.
userdata
[
"vad"
]
,
stt
=
deepgram
.
STT
(
)
,
llm
=
openai
.
LLM
.
with_groq
(
)
tts
=
cartesia
.
TTS
(
)
,
chat_ctx
=
initial_ctx
,
)
Supported LLMs
The OpenAI plugin offers support for the following LLMs:
Azure
Cerebras
Deepseek
Fireworks
Groq
Octo
Ollama
Perplexity
Telnyx
Together
xAI
Select an LLM in the dropdown menu to view parameters and syntax:
Azure
Cerebras
DeepSeek
Fireworks
Groq
Octo
Ollama
Perplexity
Telnyx
Together
xAI
Method name
with_azure
Syntax
The minimal syntax for this method assumes the environment variables for required values (for example, API keys) are set.
Python
Node.js
agent
=
VoicePipelineAgent
(
vad
=
ctx
.
proc
.
userdata
[
"vad"
]
,
stt
=
deepgram
.
STT
(
)
,
llm
=
openai
.
LLM
.
with_azure
(
)
tts
=
cartesia
.
TTS
(
)
,
chat_ctx
=
initial_ctx
,
)
Parameters
The
with_azure
method accepts the following parameters:
Python
Node.js
To learn more, see the
plugin documentation
.
Parameter
Data type
Default value /
Environment variable
model
String
gpt-4o
azure_endpoint
String
AZURE_OPENAI_ENDPOINT
azure_deployment
String
api_version
String
OPENAI_API_VERSION
api_key
String
AZURE_OPENAI_API_KEY
azure_ad_token
String
AZURE_OPENAI_AD_TOKEN
azure_ad_token_provider
AsyncAzureADTokenProvider
organization
String
OPENAI_ORG_ID
project
String
OPENAI_PROJECT_ID
base_url
String
user
String
temperature
Float
On this page
Customize the LLM for your voice agent
Example voice agent
Supported LLMs
Method name
Syntax
Parameters


Content from https://docs.livekit.io/agents/v0/integrations/azure:

On this page
Overview
Quick reference
Azure OpenAI LLM
Azure OpenAI TTS
Azure Speech STT
Azure Speech TTS
Azure Realtime API
Copy page
See more page options
Agents 1.0 available for Python
This documentation is for v0.x of the LiveKit Agents framework.
See updated documentation here:
Azure integration
.
v1.0 for Node.js is coming soon.
Overview
LiveKit's Azure integration provides support for multiple Azure AI Services. These include
Azure OpenAI
,
Speech service
for STT and TTS, and
Realtime API
. Azure OpenAI allows you to run OpenAI models using the security capabilities of Microsoft Azure. The Speech service's STT and TTS allow you to transcribe speech-to-text with high accuracy and produce natural-sounding text-to-speech voices. Azure's Realtime API processes user input and responds immediately, allowing you to create agents that sound naturally responsive.
LiveKit provides multiple integration paths for using Azure AI Services for building agents:
OpenAI plugin support for Azure OpenAI
LLM
and
TTS
.
Azure plugin for Speech service
STT
and
TTS
.
OpenAI plugin support for Azure AI Services
Realtime API
.
You can use Azure STT, TTS, and LLM to create agents using the
VoicePipelineAgent
class. To use the Realtime API, you can create an agent using the
MultimodalAgent
class.
Quick reference
The following sections provide a quick reference for integrating Azure AI Services with LiveKit. For the complete
reference, see the links provided in each section.
Azure OpenAI LLM
LiveKit's Azure integration provides an
OpenAI compatible
LLM interface.
This can be used as the LLM for an agent created using the
VoicePipelineAgent
class.
Azure's OpenAI compatible API needs to be configured to connect to OpenAI. You can set the environment variables listed
in the usage section or pass in these values when you create the LLM instance.
LLM.with_azure usage
Use the
with_azure
method to create an instance of an Azure OpenAI LLM:
main.py
.env.local
Python
Node.js
from
livekit
.
plugins
.
openai
import
LLM
azure_llm
=
LLM
.
with_azure
(
model
=
"gpt-4o"
,
temperature
=
0.8
,
)
LLM.with_azure parameters
This section describes some of the available parameters. For a complete reference of all available parameters, see the
plugin reference
.
model
string
Optional
Default:
gpt-4o
#
ID of the model to use for inference. To learn more, see
supported models
.
azure_endpoint
string
Optional
Env:
AZURE_OPENAI_ENDPOINT
#
Azure OpenAI endpoint in the following format:
https://{your-resource-name}.openai.azure.com
.
azure_deployment
string
Optional
#
Name of your model deployment.
api_version
string
Optional
Env:
OPENAI_API_VERSION
#
OpenAI REST API version used for the request.
api_key
string
Optional
Env:
AZURE_OPENAI_API_KEY
#
Azure OpenAI API key.
azure_ad_token
string
Optional
Env:
AZURE_OPENAI_AD_TOKEN
#
Azure Active Directory token.
azure_ad_token_provider
string
Optional
#
Function that returns an Azure Active Directory token.
organization
string
Optional
Env:
OPENAI_ORG_ID
#
OpenAI organization ID.
project
string
Optional
Env:
OPENAI_PROJECT_ID
#
OpenAI project ID.
temperature
float
Optional
Default:
1.0
#
A measure of randomness of completions. A lower temperature is more deterministic. To learn more,
see
chat completions
.
Azure OpenAI TTS
LiveKit's Azure integration provides an
OpenAI compatible
text-to-speech (TTS) interface.
This can be used for speech generation for an agent created with the
VoicePipelineAgent
class.
Azure's OpenAI compatible API needs to be configured to connect to OpenAI. You can set the environment variables listed
in the usage section or pass in these values when you create the TTS instance.
TTS.create_azure_client usage
Use the
TTS.create_azure_client
method to create an instance of an Azure OpenAI TTS:
main.py
.env.local
Python
Node.js
from
livekit
.
plugins
.
openai
import
tts
azure_tts
=
tts
.
TTS
.
create_azure_client
(
model
=
"tts-1"
,
voice
=
"alloy"
,
)
TTS.create_azure_client parameters
This section describes some of the available parameters. For a complete reference of all available parameters, see the
plugin reference
.
model
string
Optional
Default:
tts-1
#
ID of the model to use for TTS. To learn more, see
supported models
.
voice
string
Optional
#
OpenAI text-to-speech voice. To learn more, see
Voice options
.
azure_endpoint
string
Optional
Env:
AZURE_OPENAI_ENDPOINT
#
Azure OpenAI endpoint in the following format:
https://{your-resource-name}.openai.azure.com
.
azure_deployment
string
Optional
#
Name of your model deployment.
api_version
string
Optional
Env:
OPENAI_API_VERSION
#
OpenAI REST API version used for the request.
api_key
string
Optional
Env:
AZURE_OPENAI_API_KEY
#
Azure OpenAI API key.
azure_ad_token
string
Optional
Env:
AZURE_OPENAI_AD_TOKEN
#
Azure Active Directory token.
organization
string
Optional
Env:
OPENAI_ORG_ID
#
OpenAI organization ID.
project
string
Optional
Env:
OPENAI_PROJECT_ID
#
OpenAI project ID.
Azure Speech STT
LiveKit's Azure plugin provides support for Speech service STT. To connect to Azure's Speech service, set the environment
variables listed in the usage section, or pass these values in when you create an STT instance.
Note
The Azure plugin is currently only available for the Python Agents framework.
Azure Speech STT usage
.env.local
Python
AZURE_SPEECH_KEY
=
<
azure-speech-key
>
AZURE_SPEECH_REGION
=
<
azure-speech-region
>
AZURE_SPEECH_HOST
=
<
azure-speech-host
>
LIVEKIT_API_KEY
=
<
livekit-api-key
>
LIVEKIT_API_SECRET
=
<
livekit-api-secret
>
LIVEKIT_URL
=
<
livekit-url
>
Azure Speech STT parameters
This section describes some of the available parameters. For a complete reference of all available parameters, see the
plugin reference
.
Note
To create an instance of
azure.STT
, one of the following options must be met:
speech_host
must be set,
or
speech_key
and
speech_region
must both be set,
or
speech_auth_token
and
speech_region
must both be set
speech_key
string
Optional
Env:
AZURE_SPEECH_KEY
#
Azure Speech speech-to-text key. To learn more, see
Azure Speech prerequisites
.
speech_region
string
Optional
Env:
AZURE_SPEECH_REGION
#
Azure Speech speech-to-text region. To learn more, see
Azure Speech prerequisites
.
speech_host
string
Optional
Env:
AZURE_SPEECH_HOST
#
Azure Speech endpoint.
speech_auth_token
string
Optional
#
Azure Speech authentication token.
languages
list[string]
Optional
#
List of potential source languages. To learn more, see
Standard locale names
.
Azure Speech TTS
LiveKit's Azure plugin provides support for Speech service TTS. To connect to Azure's Speech service, set the environment
variables listed in the usage section, or pass these values in when you create a TTS instance.
Note
The Azure plugin is currently only available for the Python Agents framework.
Python
.env.local
from
livekit
.
plugins
import
azure
azure_stt
=
azure
.
TTS
(
speech_key
=
"<speech_service_key>"
,
speech_region
=
"<speech_service_region>"
,
)
Azure Speech TTS parameters
This section describes some of the available parameters. For a complete reference of all available parameters, see the
plugin reference
.
Note
To create an instance of
azure.TTS
, one of the following options must be met:
speech_host
must be set,
or
speech_key
and
speech_region
must both be set,
or
speech_auth_token
and
speech_region
must both be set
voice
string
Optional
#
Voice for text-to-speech. To learn more, see
Select synthesis language and voice
.
language
string
Optional
#
Language of the input text. To learn more, see
Select synthesis language and voice
.
prosody
ProsodyConfig
Optional
#
Specify changes to pitch, rate, and volume for the speech output. To learn more,
see
Adjust prosody
.
speech_key
string
Optional
Env:
AZURE_SPEECH_KEY
#
Azure Speech speech-to-text key. To learn more, see
Azure Speech prerequisites
.
speech_region
string
Optional
Env:
AZURE_SPEECH_REGION
#
Azure Speech speech-to-text region. To learn more, see
Azure Speech prerequisites
.
speech_host
string
Optional
Env:
AZURE_SPEECH_HOST
#
Azure Speech endpoint.
speech_auth_token
string
Optional
#
Azure Speech authentication token.
Azure Realtime API
LiveKit's OpenAI plugin provides support for Azure AI Services Realtime API when you create an agent
with the
MultimodalAgent
class.
To use the Realtime API, use the
RealtimeModel.with_azure
method.
RealtimeModel.with_azure usage
Create an instance of
MultimodalAgent
using Azure's Realtime API:
agent
=
multimodal
.
MultimodalAgent
(
model
=
openai
.
realtime
.
RealtimeModel
.
with_azure
(
azure_deployment
=
"<model-deployment>"
,
azure_endpoint
=
"wss://<endpoint>.openai.azure.com/"
,
# or AZURE_OPENAI_ENDPOINT
api_key
=
"<api-key>"
,
# or AZURE_OPENAI_API_KEY
api_version
=
"2024-10-01-preview"
,
# or OPENAI_API_VERSION
voice
=
"alloy"
,
temperature
=
0.8
,
instructions
=
"You are a helpful assistant"
,
turn_detection
=
openai
.
realtime
.
ServerVadOptions
(
threshold
=
0.6
,
prefix_padding_ms
=
200
,
silence_duration_ms
=
500
)
,
)
,
fnc_ctx
=
fnc_ctx
,
)
RealtimeModel.with_azure parameters
This section describes some of the parameters for the
RealtimeModel.with_azure
method.
For a full list of parameters, see the
plugin documentation
.
azure_deployment
string
Optional
#
Name of your model deployment.
azure_endpoint
string
Optional
Env:
AZURE_OPENAI_ENDPOINT
#
Azure OpenAI endpoint in the following format:
https://{your-resource-name}.openai.azure.com
.
api_version
string
Optional
Env:
OPENAI_API_VERSION
#
OpenAI REST API version used for the request.
api_key
string
Optional
Env:
AZURE_OPENAI_API_KEY
#
Azure OpenAI API key.
entra_token
string
Optional
#
Microsoft Entra authentication token. Required if not using API key authentication.
To learn more see Azure's
Authentication
documentation.
voice
string
Optional
Default:
alloy
#
Voice to use for speech. To learn more, see
Voice options
.
temperature
float
Optional
Default:
1.0
#
A measure of randomness of completions. A lower temperature is more deterministic. To learn more, see
chat completions
.
instructions
string
Optional
#
Initial system instructions.
modalities
list[api_proto.Modality]
Optional
Default:
["text", "audio"]
#
Modalities to use, such as ["text", "audio"].
turn_detection
ServerVadOptions
Optional
#
Server-side VAD settings.
To learn more, see
Turn detection
and
ServerVadOptions class
.
On this page
Overview
Quick reference
Azure OpenAI LLM
Azure OpenAI TTS
Azure Speech STT
Azure Speech TTS
Azure Realtime API


Content from https://docs.livekit.io/agents/v0/integrations/cartesia:

On this page
Overview
Quick reference
Environment variables
TTS
Copy page
See more page options
Agents 1.0 available for Python
This documentation is for v0.x of the LiveKit Agents framework.
See updated documentation here:
Cartesia integration
.
v1.0 for Node.js is coming soon.
Try the playground
Chat with a voice assistant built with LiveKit and Cartesia TTS
Overview
Cartesia
provides customizable speech synthesis (TTS) across a number of different languages and produces natural-sounding speech with low latency. With LiveKit's Cartesia integration and the Agents framework, you can build AI voice applications that sound realistic. For a demonstration of what you can build, try out the
LiveKit voice assistant with Cartesia
.
Note
If you're looking to build an AI voice assistant with Cartesia, check out our
Voice Agent Quickstart
guide and use the Cartesia TTS module as demonstrated below.
Quick reference
Environment variables
.env.local
CARTESIA_API_KEY
=
<
your-cartesia-api-key
>
TTS
LiveKit's Cartesia integration provides a text-to-speech (TTS) interface. This can be used in a
VoicePipelineAgent
or as a standalone speech generator. For a complete reference of all available parameters, see the
plugin reference
.
Usage
main.py
.env.local
from
livekit
.
plugins
.
cartesia
import
tts
cartesia_tts
=
tts
.
TTS
(
model
=
"sonic-english"
,
voice
=
"c2ac25f9-ecc4-4f56-9095-651354df60c0"
,
speed
=
0.8
,
emotion
=
[
"curiosity:highest"
,
"positivity:high"
]
)
Parameters
model
string
Optional
Default:
sonic
#
ID of the model to use for generation. See
supported models
.
voice
string | list[float]
Optional
Default:
c2ac25f9-ecc4-4f56-9095-651354df60c0
#
ID of the voice to use for generation, or an embedding array. See
official documentation
.
speed
string | float
Optional
Default:
1.0
#
Speed of generated speech. Either a float in range [-1.0, 1.0], or one of
"fastest"
,
"fast"
,
"normal"
,
"slow"
,
"slowest"
. See
speed options
.
emotion
list[string]
Optional
Default:
neutral
#
Emotion of generated speech. See
emotion options
.
language
string
Optional
Default:
en
#
Language of input text in
ISO-639-1
format. For a list of languages support by model, see
supported models
.
On this page
Overview
Quick reference
Environment variables
TTS


Content from https://docs.livekit.io/agents/v0/integrations/cerebras:

On this page
Overview
Quick reference
Environment variables
LLM
Copy page
See more page options
Agents 1.0 available for Python
This documentation is for v0.x of the LiveKit Agents framework.
See updated documentation here:
Cerebras integration
.
v1.0 for Node.js is coming soon.
Try the playground
Chat with a voice assistant built with LiveKit and Cerebras LLM
Overview
Cerebras
provides high-throughput, low-latency AI inference for Meta's Llama models. With LiveKit's Cerebras integration and the Agents framework, you can build responsive AI voice applications. For a demonstration of what you can build, check out the
voice assistant built with Cerebras and LiveKit
.
Note
If you're looking to build an AI voice assistant with Cerebras, check out our
Voice Agent Quickstart
guide and use the Cerebras LLM module as demonstrated below.
Quick reference
Environment variables
.env.local
CEREBRAS_API_KEY
=
<
your-cerebras-api-key
>
LLM
LiveKit's Cerebras integration provides an
OpenAI compatible
LLM interface. This can be used in a
VoicePipelineAgent
. For a complete reference of all available parameters, see the
plugin reference
.
Usage
main.py
.env.local
from
livekit
.
plugins
.
openai
import
llm
cerebras_llm
=
llm
.
LLM
.
with_cerebras
(
model
=
"llama3.1-8b"
,
temperature
=
0.8
,
)
Parameters
model
string
Optional
Default:
llama3.1-8b
#
ID of the model to use for inference. See
supported models
.
temperature
float
Optional
Default:
1.0
#
A measure of randomness of completions. A lower temperature is more deterministic. To learn more, see the
Cerebras documentation
.
On this page
Overview
Quick reference
Environment variables
LLM


Content from https://docs.livekit.io/agents/v0/integrations/deepgram:

On this page
Overview
Quick reference
Environment variables
STT
TTS
Copy page
See more page options
Agents 1.0 available for Python
This documentation is for v0.x of the LiveKit Agents framework.
See updated documentation here:
Deepgram integration
.
v1.0 for Node.js is coming soon.
Overview
Deepgram
provides advanced speech recognition technology and AI-driven audio processing solutions. Customizable speech models allow you to fine tune transcription performance for your specific use case. With LiveKit's Deepgram integration and the Agents framework, you can build AI agents that provide high-accuracy transcriptions.
Note
If you're looking to build an AI voice assistant with Deepgram, check out our
Voice Agent Quickstart
guide and use the Deepgram STT and/or TTS module as demonstrated below.
Quick reference
Environment variables
.env.local
DEEPGRAM_API_KEY
=
<
your-deepgram-api-key
>
STT
LiveKit's Deepgram integration provides a speech-to-text (STT) interface that can be used as the first stage in a
VoicePipelineAgent
or as a standalone transcription service. For a complete reference of all available parameters, see the plugin reference for
Python
or
Node
.
Usage
main.py
.env.local
Python
Node.js
from
livekit
.
plugins
.
deepgram
import
stt
deepgram_stt
=
deepgram
.
stt
.
STT
(
model
=
"nova-2-general"
,
interim_results
=
True
,
smart_format
=
True
,
punctuate
=
True
,
filler_words
=
True
,
profanity_filter
=
False
,
keywords
=
[
(
"LiveKit"
,
1.5
)
]
,
language
=
"en-US"
,
)
Parameters
model
string
Optional
Default:
nova-2-general
#
ID of the model to use for inference. To learn more, see
supported models
.
interim_results
bool
Optional
Default:
true
#
Enable preliminary results before the final transcription is available.
smart_format
bool
Optional
Default:
true
#
Enable smart formatting to improve the readability of transcriptions.
punctuate
bool
Optional
Default:
true
#
Enable punctuation in transcriptions.
filler_words
bool
Optional
Default:
true
#
Enable filler words to improve turn detection.
profanity_filter
bool
Optional
Default:
false
#
Replace recognized profanity with asterisks in transcriptions.
keywords
list[tuple[string, float]]
Optional
Default:
[]
#
A list of keywords and intensifiers to boost or suppress in transcriptions. Positive values boost; negative values suppress.
language
string
Optional
Default:
en
#
Language of input audio in
ISO-639-1
format.
TTS
LiveKit's Deepgram integration also provides a text-to-speech (TTS) interface. This can be used in a
VoicePipelineAgent
or as a standalone speech generator. For a complete reference of all available parameters, see the
plugin reference
.
Usage
main.py
.env.local
Python
Node.js
from
livekit
.
plugins
.
deepgram
import
tts
deepgram_tts
=
tts
.
TTS
(
model
=
"aura-asteria-en"
,
)
Parameters
model
string
Optional
Default:
aura-asteria-en
#
ID of the model to use for generation. To learn more, see
supported models
.
On this page
Overview
Quick reference
Environment variables
STT
TTS


Content from https://docs.livekit.io/agents/v0/integrations/elevenlabs:

On this page
Overview
Quick reference
Environment variables
TTS
Copy page
See more page options
Agents 1.0 available for Python
This documentation is for v0.x of the LiveKit Agents framework.
See updated documentation here:
ElevenLabs integration
.
v1.0 for Node.js is coming soon.
Overview
ElevenLabs
provides an AI text-to-speech (TTS) service with thousands of human-like voices
across a number of different languages. With LiveKit's ElevenLabs integration and the Agents framework, you can build
AI voice applications that sound realistic.
Quick reference
Environment variables
.env.local
ELEVEN_API_KEY
=
<
your-elevenlabs-api-key
>
TTS
LiveKit's ElevenLabs integration provides a text-to-speech (TTS) interface. This can be used in a
VoicePipelineAgent
or as a standalone speech generator. For a complete reference of all available parameters, see the
plugin reference
.
Usage
main.py
.env.local
Python
Node.js
from
livekit
.
plugins
.
elevenlabs
import
tts
eleven_tts
=
elevenlabs
.
tts
.
TTS
(
model
=
"eleven_turbo_v2_5"
,
voice
=
elevenlabs
.
tts
.
Voice
(
id
=
"EXAVITQu4vr4xnSDxMaL"
,
name
=
"Bella"
,
category
=
"premade"
,
settings
=
elevenlabs
.
tts
.
VoiceSettings
(
stability
=
0.71
,
similarity_boost
=
0.5
,
style
=
0.0
,
use_speaker_boost
=
True
)
,
)
,
language
=
"en"
,
streaming_latency
=
3
,
enable_ssml_parsing
=
False
,
chunk_length_schedule
=
[
80
,
120
,
200
,
260
]
,
)
Parameters
model
string
Optional
Default:
eleven_turbo_v2_5
#
ID of the model to use for generation. To learn more, see the
ElevenLabs documentation
.
voice
Voice
Optional
Default:
DEFAULT_VOICE
#
Voice configuration. To learn more, see the
ElevenLabs documentation
.
id
string
Required
#
ID of the voice to use for generation. To learn more, see the
ElevenLabs documentation
.
name
string
Required
#
category
string
Required
#
settings
VoiceSettings
Optional
#
See the
ElevenLabs documentation
.
stability
float
Required
#
similarity_boost
float
Required
#
style
float
Optional
#
use_speaker_boost
bool
Optional
#
language
string
Optional
Default:
en
#
Language of output audio in
ISO-639-1
format. To learn more,
see the
ElevenLabs documentation
.
streaming_latency
int
Optional
Default:
3
#
Latency in seconds for streaming.
enable_ssml_parsing
bool
Optional
Default:
false
#
Enable Speech Synthesis Markup Language (SSML) parsing for input text.
chunk_length_schedule
list[int]
Optional
Default:
[80, 120, 200, 260]
#
Schedule for chunk lengths. Valid values range from
50
to
500
.
On this page
Overview
Quick reference
Environment variables
TTS


Content from https://docs.livekit.io/agents/v0/integrations/google:

On this page
Overview
Gemini LLM
google.LLM usage
google.LLM parameters
Google Cloud STT and TTS
google.STT usage
google.STT parameters
google.TTS usage
google.TTS parameters
Gemini Live API
RealtimeModel usage
RealtimeModel parameters
Copy page
See more page options
Agents 1.0 available for Python
This documentation is for v0.x of the LiveKit Agents framework.
See updated documentation here:
Google integration
.
v1.0 for Node.js is coming soon.
Try the playground
Chat with a voice assistant built with LiveKit and Google's Gemini Live API
Overview
LiveKit's Google integration provides support for Google Gemini LLM, Google Cloud STT and TTS, and Gemini Live API:
Google plugin support for
Gemini LLM
, and Google Cloud
STT and TTS
.
Support for Google's
Gemini Live API
using the
RealtimeModel
class.
The following sections provide a quick reference for integrating Google AI services with LiveKit. For the complete
reference, see the links provided in each section.
Gemini LLM
LiveKit's Google plugin provides support for Gemini models across both Google AI and Vertex AI platforms. Use LiveKit's
Google integration with the LiveKit Agents framework and create AI agents with advanced reasoning and contextual
understanding.
google.LLM usage
Create a new instance of Gemini LLM to use in a
VoicePipelineAgent
:
agent.py
.env.local
from
livekit
.
plugins
import
google
google_llm
=
google
.
LLM
(
model
=
"gemini-2.0-flash-exp"
,
temperature
=
"0.8"
,
)
google.LLM parameters
This section describes some of the available parameters. For a complete reference of all available parameters, see the
plugin reference
.
Note
Google application credentials must be provided using one of the following options:
For Vertex AI, the
GOOGLE_APPLICATION_CREDENTIALS
environment variable must be set to the path of
the service account key file.
The Google Cloud project and location can be set via
project
and
location
arguments or the environment variables
GOOGLE_CLOUD_PROJECT
and
GOOGLE_CLOUD_LOCATION
. By default, the project is inferred from the service account
key file and the location defaults to "us-central1".
For Google AI, set the
api_key
argument or the
GOOGLE_API_KEY
environment variable.
model
ChatModels | str
Optional
Default:
gemini-2.0-flash-exp
#
ID of the model to use. For a full list, see
Gemini models
.
api_key
str
Optional
Env:
GOOGLE_API_KEY
#
API key for Google Gemini.
vertexai
bool
Optional
Default:
false
#
True to use
Vertex AI
; false to use
Google AI
.
project
str
Optional
#
Google Cloud project to use (only if using Vertex AI).
temperature
float
Optional
Default:
0.8
#
The temperature controls the degree of randomness in token selection. A lower temperature results in more deterministic
output.
To learn more, see
Model parameters
.
max_output_tokens
int
Optional
#
Maximum number of tokens that can be generated in the response.
To learn more, see
Model parameters
.
Google Cloud STT and TTS
LiveKit's Google integration includes a
Google plugin
with STT and TTS support.
Google Cloud STT
supports over 125 languages and can use
chirp
, a foundational model with improved recognition and transcription for spoken languages and accents.
Google Cloud TTS
provides a wide voice selection and generates speech with humanlike intonation.
Instances of Google STT and TTS can be used as part of the pipeline for an agent created using the
VoicePipelineAgent
class or as part of a standalone transcription service.
Note
LiveKit's Google plugin is currently only available in Python.
google.STT usage
Use the
google.STT
method to create an instance of an STT:
main.py
.env.local
from
livekit
.
plugins
import
google
google_stt
=
google
.
STT
(
model
=
"chirp"
,
spoken_punctuation
=
True
,
)
google.STT parameters
This section describes some of the available parameters. For a complete reference of all available parameters, see the
plugin reference
.
Note
Google Cloud credentials must be provided by one of the following methods:
Passed in the
credentials_info
dictionary.
Saved in the
credentials_file
JSON file (
GOOGLE_APPLICATION_CREDENTIALS
environment variable).
Application Default Credentials. To learn more,
see
How Application Default Credentials works
languages
LanguageCode
Optional
Default:
en-US
#
Specify input languages. For a full list of supported languages,
see
Speech-to-text supported languages
.
spoken_punctuation
boolean
Optional
Default:
True
#
Replace spoken punctuation with punctuation characters in text.
model
SpeechModels | string
Optional
Default:
long
#
Model to use for speech-to-text. To learn more, see
Select a transcription model
.
credentials_info
array
Optional
#
Key-value pairs of authentication credential information.
credentials_file
string
Optional
#
Name of the JSON file that contains authentication credentials for Google Cloud.
google.TTS usage
Use the
google.TTS
method to create an instance of a TTS:
main.py
.env.local
from
livekit
.
plugins
import
google
google_stt
=
google
.
TTS
(
gender
=
"female"
,
voice_name
=
"en-US-Standard-H"
,
)
google.TTS parameters
This section describes some of the available parameters. For a complete reference of all available parameters, see the
plugin reference
.
Note
Google Cloud credentials must be provided by one of the following methods:
Passed in the
credentials_info
dictionary.
Saved in the
credentials_file
JSON file (
GOOGLE_APPLICATION_CREDENTIALS
environment variable).
Application Default Credentials. To learn more,
see
How Application Default Credentials works
language
SpeechLanguages | string
Optional
Default:
en-US
#
Specify output language. For a full list of languages,
see
Supported voices and languages
.
gender
Gender | string
Optional
Default:
neutral
#
Voice gender. Valid values are
male
,
female
, and
neutral
.
voice_name
string
Optional
#
Name of the voice to use for speech. For a full list of voices,
see
Supported voices and languages
.
credentials_info
array
Optional
#
Key-value pairs of authentication credential information.
credentials_file
string
Optional
#
Name of the JSON file that contains authentication credentials for Google Cloud.
Gemini Live API
LiveKit's Google plugin includes a
RealtimeModel
class that allows you to use Google's
Gemini Live API
.
The Gemini Live API enables low-latency, two-way interactions that use text, audio, and video input,
with audio and text output. Use LiveKit's Google integration with the Agents framework to create agents with
natural, human-like voice conversations.
RealtimeModel usage
Create a model using the Gemini Live API for use in a
MultimodalAgent
:
from
livekit
.
plugins
import
google
model
=
google
.
beta
.
realtime
.
RealtimeModel
(
voice
=
"Puck"
,
temperature
=
0.8
,
instructions
=
"You are a helpful assistant"
,
)
,
For a full agent example, see the
Gemini example
in the LiveKit Agents GitHub repository.
RealtimeModel parameters
This section describes some of the available parameters. For a complete reference of all available parameters, see the
plugin reference
.
instructions
string
Optional
#
System instructions to better control the model's output and specify tone and sentiment of responses. To learn more,
see
System instructions
.
model
LiveAPIModels | string
Required
Default:
gemini-2.0-flash-exp
#
Live API model to use.
api_key
string
Required
Env:
GOOGLE_API_KEY
#
Google Gemini API key.
voice
Voice | string
Required
Default:
Puck
#
Name of the Gemini Live API voice. For a full list, see
Voices
.
modalities
list[Modality]
Optional
Default:
["AUDIO"]
#
List of modalities to use, such as ["TEXT", "AUDIO"].
vertexai
boolean
Required
Default:
False
#
If set to true, use Vertex AI.
project
string
Optional
Env:
GOOGLE_CLOUD_PROJECT
#
Google Cloud project ID to use for the API (if
vertextai=True
). By default, the project is inferred from the service
account key file (set using the
GOOGLE_APPLICATION_CREDENTIALS
environment variable).
location
string
Optional
Env:
GOOGLE_CLOUD_LOCATION
#
Google Cloud location to use for the API (if
vertextai=True
). By default, the project is inferred from the service
account key file and the location defaults to
us-central1
.
temperature
float
Optional
#
A measure of randomness of completions. A lower temperature is more deterministic. To learn more,
see
Temperature
.
On this page
Overview
Gemini LLM
google.LLM usage
google.LLM parameters
Google Cloud STT and TTS
google.STT usage
google.STT parameters
google.TTS usage
google.TTS parameters
Gemini Live API
RealtimeModel usage
RealtimeModel parameters


Content from https://docs.livekit.io/agents/v0/integrations/groq:

On this page
Overview
Prerequisites
Instructions
Setup a LiveKit account and install the CLI
Bootstrap an agent from template
Create a minimal frontend with Next.js
Launch your app and talk to your agent
Quick reference
Environment variables
STT
LLM
Copy page
See more page options
Agents 1.0 available for Python
This documentation is for v0.x of the LiveKit Agents framework.
See updated documentation here:
Groq integration
.
v1.0 for Node.js is coming soon.
Try out Groq STT live
See Groq's STT in action with our realtime transcription playground
Overview
Groq
provides low-latency AI inference with deterministic results and automatic speech recognition. LiveKit's Groq integration provides both STT and LLM functionality through an OpenAI-compatible interface. Use Groq and the Agents framework to build AI voice assistants that are realistic and predictable with accurate transcriptions.
This guide walks you through the steps to build a live transcription application that uses LiveKit's
Agents Framework
and the Groq STT service. For a demonstration of the following application, see the
LiveKit and Groq transcription app
.
Note
If you're looking to build an AI voice assistant with Groq, check out our
Voice Agent Quickstart
guide and use the Groq integration as your STT and/or LLM provider.
Prerequisites
Groq API Key
Python 3.9-3.12
or
Node 20.17.0
or higher
Instructions
Setup a LiveKit account and install the CLI
Create an account or sign in to your
LiveKit Cloud account
.
Install the LiveKit CLI
and authenticate using
lk cloud auth
— (
Optional
).
Tip
The LiveKit CLI utility
lk
is a convenient way to setup and configure applications and manage your LiveKit services,
but installing it isn't required.
Bootstrap an agent from template
Clone a starter template for your preferred language using the CLI:
Terminal
Python
Node.js
lk app create
\
--template-url https://github.com/livekit-examples/transcription-groq-python
If you aren't using the LiveKit CLI, clone the repository yourself:
Terminal
Python
Node.js
git
clone https://github.com/livekit-examples/transcription-groq-python
Enter your
Groq API Key
when prompted or manually add your environment variables:
GROQ_API_KEY
=
<
your-groq-api-key
>
LIVEKIT_API_KEY
=
<
your-livekit-api-key
>
LIVEKIT_API_SECRET
=
<
your-livekit-api-secret
>
LIVEKIT_URL
=
<
your-livekit-url
>
Install dependencies and start your agent:
Terminal
Python
Node.js
cd
<
agent_dir
>
python3
-m
venv venv
source
venv/bin/activate
python3
-m
pip
install
-r
requirements.txt
python3 main.py dev
Note
For more details on using the STT module to perform transcription outside of the context of a voice pipeline or multimodal agent, see the
transcriptions
documentation.
Create a minimal frontend with Next.js
Clone the
Transcription Frontend
Next.js app starter template using the CLI:
lk app create
--template
transcription-frontend
If you aren't using the LiveKit CLI, clone the repository yourself:
git
clone https://github.com/livekit-examples/transcription-frontend
Enter your environment variables:
LIVEKIT_API_KEY
=
<
your-livekit-api-key
>
LIVEKIT_API_SECRET
=
<
your-livekit-api-secret
>
NEXT_PUBLIC_LIVEKIT_URL
=
<
your-livekit-url
>
Install dependencies and start your frontend application:
cd
<
frontend_dir
>
pnpm
install
pnpm
dev
Launch your app and talk to your agent
Visit your locally-running application (by default,
http://localhost:3000
).
Select
Start voice transcription
and begin speaking.
Quick reference
Environment variables
.env.local
GROQ_API_KEY
=
<
your-groq-api-key
>
STT
LiveKit's Groq integration provides an
OpenAI compatible
speech-to-text (STT) interface. This can be used as the first stage in a
VoicePipelineAgent
or as a standalone transcription service as documented above. For a complete reference of all available parameters, see the
plugin reference
.
Usage
main.py
.env.local
Python
Node.js
from
livekit
.
plugins
.
openai
import
stt
groq_stt
=
stt
.
STT
.
with_groq
(
model
=
"whisper-large-v3-turbo"
,
language
=
"en"
,
)
Parameters
model
string
Optional
Default:
whisper-large-v3-turbo
#
ID of the model to use for inference. See
supported models
.
language
string
Optional
Default:
en
#
Language of input audio in
ISO-639-1
format.
detect_language
bool
Optional
Default:
false
#
Whether or not language should be detected from the audio stream. Not every
model
supports language detection. See
supported models
.
LLM
LiveKit's Groq integration also provides an
OpenAI compatible
LLM interface. This can be used in a
VoicePipelineAgent
. For a complete reference of all available parameters, see the
plugin reference
.
Usage
main.py
.env.local
Python
Node.js
from
livekit
.
plugins
.
openai
import
llm
groq_llm
=
llm
.
LLM
.
with_groq
(
model
=
"llama3-8b-8192"
,
temperature
=
0.8
,
)
Parameters
model
string
Optional
Default:
llama3-8b-8192
#
ID of the model to use for inference. For a complete list, see
supported models
.
temperature
float
Optional
Default:
1.0
#
A measure of randomness of completions. A lower temperature is more deterministic. To learn more, see
chat completions
.
On this page
Overview
Prerequisites
Instructions
Setup a LiveKit account and install the CLI
Bootstrap an agent from template
Create a minimal frontend with Next.js
Launch your app and talk to your agent
Quick reference
Environment variables
STT
LLM


Content from https://docs.livekit.io/agents/v0/integrations/playai:

On this page
Overview
Quick reference
TTS
Copy page
See more page options
Agents 1.0 available for Python
This documentation is for v0.x of the LiveKit Agents framework.
See updated documentation here:
PlayHT integration guide
.
v1.0 for Node.js is coming soon.
Overview
PlayHT
provides realistic TTS voice generation. With LiveKit's PlayHT integration and the Agents
framework, you can build AI voice applications with fluent and conversational voices.
Note
If you're looking to build an AI voice assistant with PlayHT, check out our
Voice Agent Quickstart
guide and use the PlayHT TTS module as demonstrated below.
Quick reference
The following sections provide a quick reference for integrating PlayHT TTS with LiveKit.
For a complete reference of all available parameters, see the
plugin reference
.
TTS
LiveKit's PlayHT integration provides a text-to-speech (TTS) interface. This can be used in a
VoicePipelineAgent
or as a standalone speech generator.
Usage
Use the
playai.tts.TTS
class to create an instance of a PlayHT TTS:
main.py
.env.local
from
livekit
.
plugins
.
playai
import
tts
playht_tts
=
tts
.
TTS
(
voice
=
"s3://voice-cloning-zero-shot/a59cb96d-bba8-4e24-81f2-e60b888a0275/charlottenarrativesaad/manifest.json"
,
language
=
"SPANISH"
,
)
Parameters
This section describes some of the available parameters. For a complete reference of all available parameters,
see the
plugin reference
.
api_key
string
Required
Env:
PLAYHT_API_KEY
#
PlayAI API key. Generate a user ID and API key in the
PlayHT dashboard
.
user_id
string
Required
Env:
PLAYHT_USER_ID
#
PlayAI user ID. Generate a user ID and API key in the
PlayHT dashboard
.
voice
string
Required
Default:
s3://voice-cloning-zero-shot/d9ff78ba-d016-47f6-b0ef-dd630f59414e/female-cs/manifest.json
#
URL of the voice manifest file. For a full list, see
List of pre-built voices
.
model
TTSModel | string
Required
Default:
Play3.0-mini
#
Name of the TTS model. For a full list, see
Models
.
language
string
Required
Default:
ENGLISH
#
Language of the text to be spoken. For language support by model, see
Models
.
On this page
Overview
Quick reference
TTS


Content from https://docs.livekit.io/agents/v0/integrations/rime:

On this page
Overview
Quick reference
TTS
Copy page
See more page options
Agents 1.0 available for Python
This documentation is for v0.x of the LiveKit Agents framework.
See updated documentation here:
Rime integration guide
.
v1.0 for Node.js is coming soon.
Overview
Rime
provides text-to-speech synthesis (TTS) optimized for speed and quality. With LiveKit's Rime integration and the Agents framework, you can build AI voice applications that are responsive and sound realistic.
Note
If you're looking to build an AI voice assistant with Rime, check out our
Voice Agent quickstart
guide and use the Rime TTS module as demonstrated below.
Quick reference
The following sections provide a quick reference for integrating Rime TTS with LiveKit.
For the complete reference, see the links provided in each section.
TTS
LiveKit's Rime integration provides a text-to-speech (TTS) interface. This can be used in a
VoicePipelineAgent
or as a standalone speech generator.
Usage
Create an instance of Rime TTS:
agent.py
.env.local
from
livekit
.
plugins
.
rime
import
TTS
rime_tts
=
TTS
(
model
=
"mist"
,
speaker
=
"rainforest"
,
speed_alpha
=
0.9
,
reduce_latency
=
True
,
)
Parameters
This section describes some of the available parameters. For a complete reference of all available parameters,
see the
plugin reference
.
model
string
Optional
Default:
mist
#
ID of the model to use. To learn more, see
Models
.
speaker
string
Optional
Default:
lagoon
#
ID of the voice to use for speech generation. To learn more, see
Voices
.
audio_format
TTSEncoding
Optional
Default:
pcm
#
Audio format to use. Valid values are:
pcm
and
mp3
.
sample_rate
integer
Optional
Default:
16000
#
Sample rate of the generated audio. Set this rate to best match your application needs.
To learn more, see
Recommendations for reducing response time
.
speed_alpha
float
Optional
Default:
1.0
#
Adjusts the speed of speech. Lower than
1.0
results in faster speech; higher than
1.0
results in slower speech.
reduce_latency
boolean
Optional
Default:
false
#
When set to
true
, turns off text normalization to reduce the amount of time spent preparing input text for TTS inference. This
might result in the mispronunciation of digits and abbreviations.
To learn more, see
Recommendations for reducing response time
.
phonemize_between_brackets
boolean
Optional
Default:
false
#
When set to
true
, allows the use of custom pronunciation strings in text. To learn more, see
Custom pronunciation
.
api_key
string
Optional
Env:
RIME_API_KEY
#
Rime API Key.
On this page
Overview
Quick reference
TTS


Content from https://docs.livekit.io/agents/v0/integrations/speechmatics:

On this page
Overview
Quick reference
Environment variables
STT
Copy page
See more page options
Agents 1.0 available for Python
This documentation is for v0.x of the LiveKit Agents framework.
See updated documentation here:
Speechmatics integration guide
.
v1.0 for Node.js is coming soon.
Overview
Speechmatics provides AI speech recognition technology. Their advanced speech models deliver highly accurate transcriptions across diverse languages, dialects, and accents. With LiveKit’s Speechmatics integration and the Agents framework, you can build voice AI agents that provide reliable, real-time transcriptions.
Note
If you're looking to build an AI voice assistant with Speechmatics, check out our
Voice Agent Quickstart
guide and use the Speechmatics STT module as demonstrated below.
Quick reference
Environment variables
.env.local
SPEECHMATICS_API_KEY
=
<
your-speechmatics-api-key
>
STT
LiveKit's Speechmatics integration provides a speech-to-text (STT) interface that can be used as the first stage in a
VoicePipelineAgent
or as a standalone transcription service. For a complete reference of all available parameters, see the plugin reference for
Python
.
Note
The Speechmatics STT plugin is currently only supported for Python.
Usage
main.py
.env.local
from
livekit
.
plugins
import
speechmatics
from
livekit
.
plugins
.
speechmatics
.
types
import
TranscriptionConfig
,
AudioSettings
speechmatics_stt
=
speechmatics
.
STT
(
transcription_config
=
TranscriptionConfig
(
operating_point
=
"enhanced"
,
enable_partials
=
True
,
language
=
"en"
,
output_locale
=
"en-US"
,
diarization
=
"speaker"
,
enable_entities
=
True
,
additional_vocab
=
[
{
"content"
:
"financial crisis"
}
,
{
"content"
:
"gnocchi"
,
"sounds_like"
:
[
"nyohki"
,
"nokey"
,
"nochi"
]
}
,
{
"content"
:
"CEO"
,
"sounds_like"
:
[
"C.E.O."
]
}
]
,
max_delay
=
0.7
,
max_delay_mode
=
"flexible"
)
,
audio_settings
=
AudioSettings
(
encoding
=
"pcm_s16le"
,
sample_rate
=
16000
,
)
,
)
Parameters
operating_point
string
Optional
Default:
enhanced
#
Operating point to use for the transcription per required accuracy & complexity. To learn more, see
Accuracy Reference
.
enable_partials
bool
Optional
Default:
True
#
Partial transcripts allow you to receive preliminary transcriptions and update as more context is available until the higher-accuracy
final transcript
is returned. Partials are returned faster but without any post-processing such as formatting.
language
string
Optional
Default:
en
#
ISO 639-1 language code. All languages are global and can understand different dialects/accents. To see the list of all supported languages, see
Supported Languages
.
output_locale
string
Optional
Default:
en-US
#
RFC-5646 language code for transcription output. For supported locales, see
Output Locale
.
diarization
string
Optional
Default:
NULL
#
Setting this to
speaker
enables accurate labeling of different speakers detected with the attributed transcribed output e.g. S1, S2. For more information, visit
Speaker Diarization
.
additional_vocab
list[dict{“content”:str, ”sounds_like”:str}]
Optional
Default:
NULL
#
Add custom words for each transcription job. To learn more, see
Custom Dictionary
.
enable_entities
bool
Optional
Default:
False
#
Allows the written form of various entities such as phone numbers, emails, currency, etc to be output in the transcript. To learn more about the supported entities, see
Entities
.
max_delay
number
Optional
Default:
0.7
#
The delay in seconds between the end of a spoken word and returning the final transcript results.
max_delay_mode
string
Optional
Default:
flexible
#
If set to
flexible
, the final transcript is delayed until proper numeral formatting is complete. To learn more, see
Numeral Formatting
.
On this page
Overview
Quick reference
Environment variables
STT


Content from https://docs.livekit.io/agents/build/turns/turn-detector:

On this page
Overview
Quick reference
Requirements
Installation
Download model weights
Usage
Parameters
Supported languages
Realtime model usage
Benchmarks
Runtime performance
Detection accuracy
Additional resources
Copy page
See more page options
Overview
The LiveKit turn detector plugin is a custom, open-weights language model that adds conversational context as an additional signal to voice activity detection (VAD) to improve end of turn detection in voice AI apps.
Traditional VAD models are effective at determining the presence or absence of speech, but without language understanding they can provide a poor user experience. For instance, a user might say "I need to think about that for a moment" and then take a long pause. The user has more to say but a VAD-only system interrupts them anyways. A context-aware model can predict that they have more to say and wait for them to finish before responding.
The LiveKit turn detector plugin is free to use with the Agents SDK and includes both English-only and multilingual models.
Turn detector demo
A video showcasing the improvements provided by the LiveKit turn detector.
Quick reference
The following sections provide a quick overview of the turn detector plugin. For more information, see
Additional resources
.
Requirements
The LiveKit turn detector is designed for use inside an
AgentSession
and also requires an
STT plugin
be provided. If you're using a realtime LLM you must include a separate STT plugin to use the LiveKit turn detector plugin.
LiveKit recommends also using the
Silero VAD plugin
for maximum performance, but you can rely on your STT plugin's endpointing instead if you prefer.
The model runs locally on the CPU and requires <500 MB of RAM even with multiple concurrent jobs with a shared inference server.
For production deployments, use compute-optimized instances (such as AWS c6i or c7i) rather than burstable instances (such as AWS t3). Burstable instances can cause inference timeouts under consistent load due to their CPU credit system.
Installation
Install the plugin from PyPI:
pip
install
"livekit-agents[turn-detector]~=1.0"
Download model weights
You must download the model weights before running your agent for the first time:
python agent.py download-files
Usage
Initialize your
AgentSession
with the turn detector and initialize your STT plugin with matching language settings. These examples use the Deepgram STT plugin, but more than 10 other STT plugins
are available
.
English-only model
Use the
EnglishModel
and ensure your STT plugin configuration matches:
from
livekit
.
plugins
.
turn_detector
.
english
import
EnglishModel
from
livekit
.
plugins
import
deepgram
session
=
AgentSession
(
turn_detection
=
EnglishModel
(
)
,
stt
=
deepgram
.
STT
(
model
=
"nova-3"
,
language
=
"en"
)
,
# ... vad, stt, tts, llm, etc.
)
Multilingual model
Use the
MultilingualModel
and ensure your STT plugin configuration matches. In this example, Deepgram performs automatic language detection and passes that value to the turn detector.
from
livekit
.
plugins
.
turn_detector
.
multilingual
import
MultilingualModel
from
livekit
.
plugins
import
deepgram
session
=
AgentSession
(
turn_detection
=
MultilingualModel
(
)
,
stt
=
deepgram
.
STT
(
model
=
"nova-3"
,
language
=
"multi"
)
,
# ... vad, stt, tts, llm, etc.
)
Parameters
The turn detector itself has no configuration, but the
AgentSession
that uses it supports the following related parameters:
min_endpointing_delay
float
Optional
Default:
0.5
#
The number of seconds to wait before considering the turn complete. The session uses this delay when no turn detector model is present, or when the model indicates a likely turn boundary.
max_endpointing_delay
float
Optional
Default:
6.0
#
The maximum time to wait for the user to speak after the turn detector model indicates the user is likely to continue speaking. This parameter has no effect without the turn detector model.
Supported languages
The
MultilingualModel
supports English and 13 other languages. The model relies on your
STT plugin
to report the language of the user's speech. To set the language to a fixed value, configure the STT plugin with a specific language. For example, to force the model to use Spanish:
session
=
AgentSession
(
turn_detection
=
MultilingualModel
(
)
,
stt
=
deepgram
.
STT
(
model
=
"nova-2"
,
language
=
"es"
)
,
# ... vad, stt, tts, llm, etc.
)
The model currently supports English, Spanish, French, German, Italian, Portuguese, Dutch, Chinese, Japanese, Korean, Indonesian, Turkish, Russian, and Hindi.
Realtime model usage
Realtime models like the OpenAI Realtime API produce user transcripts after the end of the turn, rather than incrementally while the user speaks. The turn detector model requires live STT results to operate, so you must provide an STT plugin to the
AgentSession
to use it with a realtime model. This incurs extra cost for the STT model.
Benchmarks
The following data shows the expected performance of the turn detector model.
Runtime performance
The size on disk and typical CPU inference time for the turn detector models is as follows:
Model
Base Model
Size on Disk
Per Turn Latency
English-only
SmolLM2-135M
66 MB
~15-45 ms
Multilingual
Qwen2.5-0.5B
281 MB
~50-160 ms
Detection accuracy
The following tables show accuracy metrics for the turn detector models in each supported language.
True positive
means the model correctly identifies the user has finished speaking.
True negative
means the model correctly identifies the user will continue speaking.
English-only model
Accuracy metrics for the English-only model:
Language
True Positive Rate
True Negative Rate
English
98.8%
87.5%
Multilingual model
Accuracy metrics for the multilingual model, when configured with the correct language:
Language
True Positive Rate
True Negative Rate
Hindi
99.4%
93.6%
Korean
99.3%
88.7%
French
99.3%
84.9%
Portuguese
99.4%
82.8%
Indonesian
99.3%
80.3%
Russian
99.3%
80.2%
English
99.3%
80.2%
Chinese
99.3%
80.0%
Japanese
99.3%
79.8%
Italian
99.3%
79.8%
Spanish
99.3%
79.7%
German
99.3%
77.6%
Turkish
99.3%
74.3%
Dutch
99.3%
73.4%
Additional resources
The following resources provide more information about using the LiveKit turn detector plugin.
Python package
The
livekit-plugins-turn-detector
package on PyPI.
Plugin reference
Reference for the LiveKit turn detector plugin.
GitHub repo
View the source or contribute to the LiveKit turn detector plugin.
LiveKit Model License
LiveKit Model License used for the turn detector model.
On this page
Overview
Quick reference
Requirements
Installation
Download model weights
Usage
Parameters
Supported languages
Realtime model usage
Benchmarks
Runtime performance
Detection accuracy
Additional resources


Content from https://docs.livekit.io/agents/build/turns/vad:

On this page
Overview
Quick reference
Requirements
Installation
Download model weights
Usage
Prewarm
Configuration
Additional resources
Copy page
See more page options
Overview
The Silero VAD plugin provides voice activity detection (VAD) that contributes to accurate
turn detection
in voice AI applications.
VAD is a crucial component for voice AI applications as it helps determine when a user is speaking versus when they are silent. This enables natural turn-taking in conversations and helps optimize resource usage by only performing speech-to-text while the user speaks.
LiveKit recommends using the Silero VAD plugin in combination with the custom
turn detector model
for the best performance.
Quick reference
The following sections provide a quick overview of the Silero VAD plugin. For more information, see
Additional resources
.
Requirements
The model runs locally on the CPU and requires minimal system resources.
Installation
Install the plugin from PyPI:
pip
install
"livekit-agents[silero]~=1.0"
Download model weights
You must download the model weights before running your agent for the first time:
python agent.py download-files
Usage
Initialize your
AgentSession
with the Silero VAD plugin:
from
livekit
.
plugins
import
silero
session
=
AgentSession
(
vad
=
silero
.
VAD
.
load
(
)
,
# ... stt, tts, llm, etc.
)
Prewarm
You can
prewarm
the plugin to improve load times for new jobs:
async
def
entrypoint
(
ctx
:
agents
.
JobContext
)
:
session
=
AgentSession
(
vad
=
ctx
.
proc
.
userdata
[
"vad"
]
,
# ... stt, tts, llm, etc.
)
# ... session.start etc ...
def
prewarm
(
proc
:
agents
.
JobProcess
)
:
proc
.
userdata
[
"vad"
]
=
silero
.
VAD
.
load
(
)
if
__name__
==
"__main__"
:
agents
.
cli
.
run_app
(
agents
.
WorkerOptions
(
entrypoint_fnc
=
entrypoint
,
prewarm_fnc
=
prewarm
)
)
Configuration
The following parameters are available on the
load
method:
min_speech_duration
float
Optional
Default:
0.05
#
Minimum duration of speech required to start a new speech chunk.
min_silence_duration
float
Optional
Default:
0.55
#
Duration of silence to wait after speech ends to determine if the user has finished speaking.
prefix_padding_duration
float
Optional
Default:
0.5
#
Duration of padding to add to the beginning of each speech chunk.
max_buffered_speech
float
Optional
Default:
60.0
#
Maximum duration of speech to keep in the buffer (in seconds).
activation_threshold
float
Optional
Default:
0.5
#
Threshold to consider a frame as speech. A higher threshold results in more conservative detection but might potentially miss soft speech. A lower threshold results in more sensitive detection, but might identify noise as speech.
sample_rate
Literal[8000, 16000]
Optional
Default:
16000
#
Sample rate for the inference (only 8KHz and 16KHz are supported).
force_cpu
bool
Optional
Default:
True
#
Force the use of CPU for inference.
Additional resources
The following resources provide more information about using the LiveKit Silero VAD plugin.
Python package
The
livekit-plugins-silero
package on PyPI.
Plugin reference
Reference for the LiveKit Silero VAD plugin.
GitHub repo
View the source or contribute to the LiveKit Silero VAD plugin.
Silero VAD project
The open source VAD model that powers the LiveKit Silero VAD plugin.
Transcriber
An example using standalone VAD and STT outside of an
AgentSession
.
On this page
Overview
Quick reference
Requirements
Installation
Download model weights
Usage
Prewarm
Configuration
Additional resources


Content from https://docs.livekit.io/agents/integrations/realtime/openai:

On this page
Overview
Quick reference
Installation
Authentication
Usage
Parameters
Turn detection
Server VAD
Semantic VAD
Usage with separate TTS
Loading conversation history
Additional resources
Copy page
See more page options
OpenAI Playground
Experiment with OpenAI's Realtime API in the playground with personalities like
the
Snarky Teenager
or
Opera Singer
.
Overview
OpenAI's Realtime API enables low-latency, multimodal interactions with realtime audio and text processing. Use
LiveKit's OpenAI plugin to create an agent that uses the Realtime API.
Note
Using Azure OpenAI? See our
Azure OpenAI Realtime API guide
.
Quick reference
This section includes a basic usage example and some reference material. For links to more detailed documentation, see
Additional resources
.
Installation
Install the OpenAI plugin from PyPI:
pip
install
"livekit-agents[openai]~=1.0"
Authentication
The OpenAI plugin requires an
OpenAI API key
.
Set
OPENAI_API_KEY
in your
.env
file.
Usage
Use the OpenAI Realtime API within an
AgentSession
. For example,
you can use it in the
Voice AI quickstart
.
from
livekit
.
plugins
import
openai
session
=
AgentSession
(
llm
=
openai
.
realtime
.
RealtimeModel
(
)
,
)
Parameters
This section describes some of the available parameters. For a complete reference of all available parameters, see the
plugin reference
.
model
str
Optional
Default:
'gpt-4o-realtime-preview'
#
ID of the Realtime model to use. For a list of available models, see the
Models
.
voice
str
Optional
Default:
'alloy'
#
Voice to use for speech generation. For a list of available voices, see
Voice options
.
temperature
float
Optional
Default:
0.8
#
Valid values are between
0.6
and
1.2
. To learn more, see
temperature
.
turn_detection
TurnDetection | None
Optional
#
Configuration for turn detection, see the section on
Turn detection
for more information.
modalities
list[str]
Optional
Default:
['text', 'audio']
#
List of response modalities to use for the session. Set to
['text']
to use the model in text-only mode with a
separate TTS plugin
.
Turn detection
OpenAI's Realtime API includes
voice activity detection (VAD)
to automatically detect when a user has started or stopped speaking. This feature is enabled by default.
There are two modes for VAD:
Server VAD
(default): Uses periods of silence to automatically chunk the audio.
Semantic VAD
: Uses a semantic classifier to detect when the user has finished speaking based on their words.
Server VAD
Server VAD is the default mode and can be configured with the following properties:
from
livekit
.
plugins
.
openai
import
realtime
from
openai
.
types
.
beta
.
realtime
.
session
import
TurnDetection
session
=
AgentSession
(
llm
=
realtime
.
RealtimeModel
(
turn_detection
=
TurnDetection
(
type
=
"server_vad"
,
threshold
=
0.5
,
prefix_padding_ms
=
300
,
silence_duration_ms
=
500
,
create_response
=
True
,
interrupt_response
=
True
,
)
)
,
)
threshold
: Higher values require louder audio to activate, better for noisy environments.
prefix_padding_ms
: Amount of audio to include before detected speech.
silence_duration_ms
: Duration of silence to detect speech stop (shorter = faster turn detection).
Semantic VAD
Semantic VAD uses a classifier to determine when the user is done speaking based on their words. This mode is less likely to interrupt users mid-sentence or chunk transcripts prematurely.
from
livekit
.
plugins
.
openai
import
realtime
from
openai
.
types
.
beta
.
realtime
.
session
import
TurnDetection
session
=
AgentSession
(
llm
=
realtime
.
RealtimeModel
(
turn_detection
=
TurnDetection
(
type
=
"semantic_vad"
,
eagerness
=
"auto"
,
create_response
=
True
,
interrupt_response
=
True
,
)
)
,
)
The
eagerness
property controls how quickly the model responds:
auto
(default) - Equivalent to
medium
.
low
- Lets users take their time speaking.
high
- Chunks audio as soon as possible.
medium
- Balanced approach.
For more information about turn detection in general, see the
Turn detection guide
.
Usage with separate TTS
To use the OpenAI Realtime API with a different
TTS provider
, configure it with a text-only response modality and include a TTS plugin in your
AgentSession
configuration. This configuration allows you to gain the benefits of realtime speech comprehension while maintaining complete control over the speech output.
session
=
AgentSession
(
llm
=
openai
.
realtime
.
RealtimeModel
(
modalities
=
[
"text"
]
)
,
tts
=
cartesia
.
TTS
(
)
# Or other TTS plugin of your choice
)
Loading conversation history
If you load conversation history into the model, it might respond with text output even if configured for audio response. To work around this issue, use the model
with a separate TTS plugin
and text-only response modality. You can use the
Azure OpenAI TTS plugin
to continue using the same voices supported by the Realtime API.
For additional workaround options, see the OpenAI
thread
on this topic.
Additional resources
The following resources provide more information about using OpenAI with LiveKit Agents.
Python package
The
livekit-plugins-openai
package on PyPI.
Plugin reference
Reference for the OpenAI Realtime API plugin.
GitHub repo
View the source or contribute to the LiveKit OpenAI LLM plugin.
Voice AI quickstart
Build a simple realtime model voice assistant using the OpenAI Realtime API in less than 10 minutes.
OpenAI docs
OpenAI Realtime API documentation.
OpenAI ecosystem overview
Overview of the entire OpenAI and LiveKit Agents integration.
On this page
Overview
Quick reference
Installation
Authentication
Usage
Parameters
Turn detection
Server VAD
Semantic VAD
Usage with separate TTS
Loading conversation history
Additional resources


Content from https://docs.livekit.io/agents/integrations/llm/openai:

On this page
Overview
Quick reference
Installation
Authentication
Usage
Parameters
Additional resources
Copy page
See more page options
Overview
OpenAI
provides powerful language models like
gpt-4o
and
o1
. With LiveKit's OpenAI integration and the Agents framework, you can build sophisticated voice AI applications using their industry-leading models.
Using Azure OpenAI?
See our
Azure OpenAI LLM guide
.
Quick reference
This section includes a basic usage example and some reference material. For links to more detailed documentation, see
Additional resources
.
Installation
Install the plugin from PyPI:
pip
install
"livekit-agents[openai]~=1.0"
Authentication
The OpenAI plugin requires an
OpenAI API key
.
Set
OPENAI_API_KEY
in your
.env
file.
Usage
Use OpenAI within an
AgentSession
or as a standalone LLM service. For example,
you can use this LLM in the
Voice AI quickstart
.
from
livekit
.
plugins
import
openai
session
=
AgentSession
(
llm
=
openai
.
LLM
(
model
=
"gpt-4o-mini"
)
,
# ... tts, stt, vad, turn_detection, etc.
)
Parameters
This section describes some of the available parameters. See the
plugin reference
for a complete list of all available parameters.
model
string
Optional
Default:
gpt-4o-mini
#
The model to use for the LLM. For more information, see the
OpenAI documentation
.
temperature
float
Optional
Default:
0.8
#
A measure of randomness in output. A lower value results in more predictable output, while a higher value results in
more creative output.
tool_choice
ToolChoice | Literal['auto', 'required', 'none']
Optional
Default:
auto
#
Specifies whether to use tools during response generation.
Additional resources
The following resources provide more information about using OpenAI with LiveKit Agents.
Python package
The
livekit-plugins-openai
package on PyPI.
Plugin reference
Reference for the OpenAI LLM plugin.
GitHub repo
View the source or contribute to the LiveKit OpenAI LLM plugin.
OpenAI docs
OpenAI platform documentation.
Voice AI quickstart
Get started with LiveKit Agents and OpenAI.
OpenAI ecosystem overview
Overview of the entire OpenAI and LiveKit Agents integration.
On this page
Overview
Quick reference
Installation
Authentication
Usage
Parameters
Additional resources


Content from https://docs.livekit.io/agents/integrations/tts/openai:

On this page
Overview
Quick reference
Installation
Authentication
Usage
Parameters
Additional resources
Copy page
See more page options
Try LiveKit.fm
Chat with OpenAI's latest `gpt-4o-mini-tts` model in a LiveKit demo inspired by OpenAI.fm
Overview
OpenAI TTS
provides lifelike spoken audio through their latest model
gpt-4o-mini-tts
model or their well-tested
tts-1
and
tts-1-hd
models. With LiveKit's OpenAI TTS integration and the Agents framework, you can build voice AI applications that sound realistic and natural.
To learn more about TTS and generating agent speech, see
Agent speech
.
Quick reference
This section includes a basic usage example and some reference material. For links to more detailed documentation, see
Additional resources
.
Installation
Install the plugin from PyPI:
pip
install
"livekit-agents[openai]~=1.0"
Authentication
The OpenAI plugin requires an
OpenAI API key
.
Set
OPENAI_API_KEY
in your
.env
file.
Usage
Use OpenAI TTS in an
AgentSession
or as a standalone speech generator. For example, you can use this TTS in the
Voice AI quickstart
.
from
livekit
.
plugins
import
openai
session
=
AgentSession
(
tts
=
openai
.
TTS
(
model
=
"gpt-4o-mini-tts"
,
voice
=
"ash"
,
instructions
=
"Speak in a friendly and conversational tone."
,
)
,
# ... llm, stt, etc.
)
Parameters
This section describes some of the available parameters. See the
plugin reference
for a complete list of all available parameters.
model
TTSModels | string
Optional
Default:
gpt-4o-mini-tts
#
ID of the model to use for speech generation. To learn more, see
TTS models
.
voice
TTSVoice | string
Optional
Default:
ash
#
ID of the voice used for speech generation. To learn more, see
TTS voice options
.
instructions
string
Optional
#
Instructions to control tone, style, and other characteristics of the speech. Does not work with
tts-1
or
tts-1-hd
models.
Additional resources
The following resources provide more information about using OpenAI with LiveKit Agents.
Python package
The
livekit-plugins-openai
package on PyPI.
Plugin reference
Reference for the OpenAI TTS plugin.
GitHub repo
View the source or contribute to the LiveKit OpenAI TTS plugin.
OpenAI docs
OpenAI TTS docs.
Voice AI quickstart
Get started with LiveKit Agents and OpenAI TTS.
OpenAI ecosystem guide
Overview of the entire OpenAI and LiveKit Agents integration.
On this page
Overview
Quick reference
Installation
Authentication
Usage
Parameters
Additional resources


Content from https://docs.livekit.io/agents/integrations/stt/openai:

On this page
Overview
Quick reference
Installation
Authentication
Usage
Parameters
Additional resources
Copy page
See more page options
Overview
OpenAI
provides STT support via the latest
gpt-4o-transcribe
model as well as
whisper-1
. You can use the open source OpenAI plugin for LiveKit agents to build voice AI applications with fast, accurate transcription.
Quick reference
This section includes a basic usage example and some reference material. For links to more detailed documentation, see
Additional resources
.
Installation
Install the plugin from PyPI:
pip
install
"livekit-agents[openai]~=1.0"
Authentication
The OpenAI plugin requires an
OpenAI API key
.
Set
OPENAI_API_KEY
in your
.env
file.
Usage
Use OpenAI STT in an
AgentSession
or as a standalone transcription service. For example, you can use this STT in the
Voice AI quickstart
.
from
livekit
.
plugins
import
openai
session
=
AgentSession
(
stt
=
openai
.
STT
(
model
=
"gpt-4o-transcribe"
,
)
,
# ... llm, tts, etc.
)
Parameters
This section describes some of the available parameters. See the
plugin reference
for a complete list of all available parameters.
model
WhisperModels | string
Optional
Default:
gpt-4o-transcribe
#
Model to use for transcription. See OpenAI's documentation for a list of
supported models
.
language
string
Optional
Default:
en
#
Language of input audio in
ISO-639-1
format. See OpenAI's documentation for a list of
supported languages
.
Additional resources
The following resources provide more information about using OpenAI with LiveKit Agents.
Python package
The
livekit-plugins-openai
package on PyPI.
Plugin reference
Reference for the OpenAI STT plugin.
GitHub repo
View the source or contribute to the LiveKit OpenAI STT plugin.
OpenAI docs
OpenAI STT docs.
Voice AI quickstart
Get started with LiveKit Agents and OpenAI STT.
OpenAI ecosystem guide
Overview of the entire OpenAI and LiveKit Agents integration.
On this page
Overview
Quick reference
Installation
Authentication
Usage
Parameters
Additional resources


Content from https://docs.livekit.io/agents/integrations/realtime/gemini:

On this page
Overview
Quick reference
Installation
Authentication
Usage
Parameters
Gemini tools
Turn detection
Usage with separate TTS
Additional resources
Copy page
See more page options
Try the playground
Chat with a voice assistant built with LiveKit and the Gemini Live API
Overview
Google's
Gemini Live API
enables low-latency, two-way interactions that use text, audio, and video input, with audio and text output. LiveKit's Google plugin includes a
RealtimeModel
class that allows you to use this API to create agents with natural, human-like voice conversations.
Quick reference
This section includes a basic usage example and some reference material. For links to more detailed documentation, see
Additional resources
.
Installation
Install the Google plugin from PyPI:
pip
install
"livekit-agents[google]~=1.0"
Authentication
The Google plugin requires authentication based on your chosen service:
For Vertex AI, you must set the
GOOGLE_APPLICATION_CREDENTIALS
environment variable to the path of the service account key file.
For Google Gemini API, set the
GOOGLE_API_KEY
environment variable.
Usage
Use the Gemini Live API within an
AgentSession
. For example,
you can use it in the
Voice AI quickstart
.
from
livekit
.
plugins
import
google
session
=
AgentSession
(
llm
=
google
.
beta
.
realtime
.
RealtimeModel
(
model
=
"gemini-2.0-flash-exp"
,
voice
=
"Puck"
,
temperature
=
0.8
,
instructions
=
"You are a helpful assistant"
,
)
,
)
Parameters
This section describes some of the available parameters. For a complete reference of all available parameters, see the
plugin reference
.
instructions
string
Optional
#
System instructions to better control the model's output and specify tone and sentiment of responses. To learn more,
see
System instructions
.
model
LiveAPIModels | string
Required
Default:
gemini-2.0-flash-exp
#
Live API model to use.
api_key
string
Required
Env:
GOOGLE_API_KEY
#
Google Gemini API key.
voice
Voice | string
Required
Default:
Puck
#
Name of the Gemini Live API voice. For a full list, see
Voices
.
modalities
list[Modality]
Optional
Default:
["AUDIO"]
#
List of response modalities to use, such as
["TEXT", "AUDIO"]
. Set to
["TEXT"]
to use the model in text-only mode with a
separate TTS plugin
.
vertexai
boolean
Required
Default:
false
#
If set to true, use Vertex AI.
project
string
Optional
Env:
GOOGLE_CLOUD_PROJECT
#
Google Cloud project ID to use for the API (if
vertextai=True
). By default, it uses the project in the service
account key file (set using the
GOOGLE_APPLICATION_CREDENTIALS
environment variable).
location
string
Optional
Env:
GOOGLE_CLOUD_LOCATION
#
Google Cloud location to use for the API (if
vertextai=True
). By default, it uses the location from the service
account key file or
us-central1
.
_gemini_tools
list[GeminiTool]
Optional
#
List of built-in Google tools, such as Google Search. For more information, see
Gemini tools
.
Gemini tools
Experimental feature
This integration is experimental and may change in a future SDK release.
The
_gemini_tools
parameter allows you to use built-in Google tools with the Gemini model. For example, you can use this feature to implement
Grounding with Google Search
:
from
google
.
genai
import
types
session
=
AgentSession
(
llm
=
google
.
beta
.
realtime
.
RealtimeModel
(
model
=
"gemini-2.0-flash-exp"
,
_gemini_tools
=
[
types
.
GoogleSearch
(
)
]
,
)
)
Turn detection
The Gemini Live API includes built-in VAD-based turn detection, which is currently the only supported turn detection method.
Usage with separate TTS
To use the Gemini Live API with a different
TTS provider
, configure it with a text-only response modality and include a TTS plugin in your
AgentSession
configuration. This configuration allows you to gain the benefits of realtime speech comprehension while maintaining complete control over the speech output.
from
google
.
genai
.
types
import
Modality
session
=
AgentSession
(
llm
=
google
.
beta
.
realtime
.
RealtimeModel
(
modalities
=
[
Modality
.
TEXT
]
)
,
tts
=
cartesia
.
TTS
(
)
,
)
Additional resources
The following resources provide more information about using Gemini with LiveKit Agents.
Python package
The
livekit-plugins-google
package on PyPI.
Plugin reference
Reference for the Gemini Live API plugin.
GitHub repo
View the source or contribute to the LiveKit Google plugin.
Gemini docs
Gemini Live API documentation.
Voice AI quickstart
Get started with LiveKit Agents and Gemini Live API.
Google AI ecosystem guide
Overview of the entire Google AI and LiveKit Agents integration.
On this page
Overview
Quick reference
Installation
Authentication
Usage
Parameters
Gemini tools
Turn detection
Usage with separate TTS
Additional resources


Content from https://docs.livekit.io/agents/integrations/llm/gemini:

On this page
Overview
Quick reference
Installation
Authentication
Usage
Parameters
Gemini tools
Additional resources
Copy page
See more page options
Overview
Google Gemini
provides powerful language models with advanced reasoning and multimodal capabilities. With LiveKit's Google plugin and the Agents framework, you can build sophisticated voice AI applications using Vertex AI or Google Gemini API.
Quick reference
This section includes a basic usage example and some reference material. For links to more detailed documentation, see
Additional resources
.
Installation
Install the plugin from PyPI:
pip
install
"livekit-agents[google]~=1.0"
Authentication
The Google plugin requires authentication based on your chosen service:
For Vertex AI, you must set the
GOOGLE_APPLICATION_CREDENTIALS
environment variable to the path of the service account key file.
For Google Gemini API, set the
GOOGLE_API_KEY
environment variable.
Usage
Use Gemini within an
AgentSession
or as a standalone LLM service. For example,
you can use this LLM in the
Voice AI quickstart
.
from
livekit
.
plugins
import
google
session
=
AgentSession
(
llm
=
google
.
LLM
(
model
=
"gemini-2.0-flash-exp"
,
temperature
=
0.8
,
)
,
# ... tts, stt, vad, turn_detection, etc.
)
Parameters
This section describes some of the available parameters. For a complete reference of all available parameters, see the
plugin reference
.
model
ChatModels | str
Optional
Default:
gemini-2.0-flash-001
#
ID of the model to use. For a full list, see
Gemini models
.
api_key
str
Optional
Env:
GOOGLE_API_KEY
#
API key for Google Gemini API.
vertexai
bool
Optional
Default:
false
#
True to use
Vertex AI
; false to use
Google AI
.
project
str
Optional
Env:
GOOGLE_CLOUD_PROJECT
#
Google Cloud project to use (only if using Vertex AI). Required if using Vertex AI and the environment variable isn't set.
location
str
Optional
Env:
GOOGLE_CLOUD_LOCATION
#
Google Cloud location to use (only if using Vertex AI). Required if using Vertex AI and the environment variable isn't set.
gemini_tools
List[GeminiTool]
Optional
#
List of built-in Google tools, such as Google Search. For more information, see
Gemini tools
.
Gemini tools
The
gemini_tools
parameter allows you to use built-in Google tools with the Gemini model. For example, you can use this feature to implement
Grounding with Google Search
:
from
livekit
.
plugins
import
google
from
google
.
genai
import
types
session
=
AgentSession
(
llm
=
google
.
LLM
(
model
=
"gemini-2.0-flash-exp"
,
gemini_tools
=
[
types
.
GoogleSearch
(
)
]
,
)
,
# ... tts, stt, vad, turn_detection, etc.
)
The full list of supported tools, depending on the model, is:
google.genai.types.GoogleSearchRetrieval()
google.genai.types.ToolCodeExecution()
google.genai.types.GoogleSearch()
google.genai.types.UrlContext()
google.genai.types.GoogleMaps()
Additional resources
The following resources provide more information about using Google Gemini with LiveKit Agents.
Python package
The
livekit-plugins-google
package on PyPI.
Plugin reference
Reference for the Google Gemini LLM plugin.
GitHub repo
View the source or contribute to the LiveKit Google Gemini LLM plugin.
Gemini docs
Google Gemini documentation.
Voice AI quickstart
Get started with LiveKit Agents and Google Gemini.
Google AI ecosystem guide
Overview of the entire Google AI and LiveKit Agents integration.
On this page
Overview
Quick reference
Installation
Authentication
Usage
Parameters
Gemini tools
Additional resources


Content from https://docs.livekit.io/agents/integrations/tts/google:

On this page
Overview
Quick reference
Installation
Authentication
Usage
Parameters
Customizing speech
Additional resources
Copy page
See more page options
Overview
Google Cloud TTS
provides a wide voice selection and generates speech with humanlike intonation. With LiveKit's Google Cloud TTS integration and the Agents framework, you can build voice AI applications that sound realistic.
Quick reference
This section includes a basic usage example and some reference material. For links to more detailed documentation, see
Additional resources
.
Installation
Install the plugin from PyPI:
pip
install
"livekit-agents[google]~=1.0"
Authentication
Google Cloud credentials must be provided by one of the following methods:
Passed in the
credentials_info
dictionary.
Saved in the
credentials_file
JSON file (
GOOGLE_APPLICATION_CREDENTIALS
environment variable).
Application Default Credentials. To learn more,
see
How Application Default Credentials works
Usage
Use a Google Cloud TTS in an
AgentSession
or as a standalone speech generator. For example, you can use this TTS in the
Voice AI quickstart
.
from
livekit
.
plugins
import
google
session
=
AgentSession
(
tts
=
google
.
TTS
(
gender
=
"female"
,
voice_name
=
"en-US-Standard-H"
,
)
,
# ... llm, stt, etc.
)
Parameters
This section describes some of the available parameters. See the
plugin reference
for a complete list of all available parameters.
language
SpeechLanguages | string
Optional
Default:
en-US
#
Specify output language. For a full list of languages,
see
Supported voices and languages
.
gender
Gender | string
Optional
Default:
neutral
#
Voice gender. Valid values are
male
,
female
, and
neutral
.
voice_name
string
Optional
#
Name of the voice to use for speech. For a full list of voices,
see
Supported voices and languages
.
credentials_info
array
Optional
#
Key-value pairs of authentication credential information.
credentials_file
string
Optional
#
Name of the JSON file that contains authentication credentials for Google Cloud.
Customizing speech
Google Cloud TTS supports Speech Synthesis Markup Language (SSML) to customize pronunciation and speech.
To learn more, see the
SSML reference
.
Additional resources
The following resources provide more information about using Google Cloud with LiveKit Agents.
Python package
The
livekit-plugins-google
package on PyPI.
Plugin reference
Reference for the Google Cloud TTS plugin.
GitHub repo
View the source or contribute to the LiveKit Google Cloud TTS plugin.
Google Cloud docs
Google Cloud TTS docs.
Voice AI quickstart
Get started with LiveKit Agents and Google Cloud TTS.
Google ecosystem guide
Overview of the entire Google AI and LiveKit Agents integration.
On this page
Overview
Quick reference
Installation
Authentication
Usage
Parameters
Customizing speech
Additional resources


Content from https://docs.livekit.io/agents/integrations/stt/google:

On this page
Overview
Quick reference
Installation
Authentication
Usage
Parameters
Additional resources
Copy page
See more page options
Overview
Google Cloud
provides a streaming
STT service
with support for over 125 languages and access to the foundational model
chirp
, which provides improved recognition and transcription for spoken languages and accents. You can use the open source Google Cloud plugin for LiveKit Agents to build voice AI with fast, accurate transcription.
Quick reference
This section includes a basic usage example and some reference material. For links to more detailed documentation, see
Additional resources
.
Installation
Install the plugin from PyPI:
pip
install
"livekit-agents[google]~=1.0"
Authentication
Google Cloud credentials must be provided by one of the following methods:
Passed in the
credentials_info
dictionary.
Saved in the
credentials_file
JSON file (
GOOGLE_APPLICATION_CREDENTIALS
environment variable).
Application Default Credentials. To learn more,
see
How Application Default Credentials works
Usage
Use a Google Cloud STT in an
AgentSession
or as a standalone transcription service. For example, you can use this STT in the
Voice AI quickstart
.
from
livekit
.
plugins
import
google
session
=
AgentSession
(
stt
=
google
.
STT
(
model
=
"chirp"
,
spoken_punctuation
=
False
,
)
,
# ... llm, tts, etc.
)
Parameters
This section describes some of the available parameters. See the
plugin reference
for a complete list of all available parameters.
languages
LanguageCode
Optional
Default:
en-US
#
Specify input languages. For a full list of supported languages,
see
Speech-to-text supported languages
.
spoken_punctuation
boolean
Optional
Default:
True
#
Replace spoken punctuation with punctuation characters in text.
model
SpeechModels | string
Optional
Default:
long
#
Model to use for speech-to-text. To learn more, see
Select a transcription model
.
credentials_info
array
Optional
#
Key-value pairs of authentication credential information.
credentials_file
string
Optional
#
Name of the JSON file that contains authentication credentials for Google Cloud.
Additional resources
The following resources provide more information about using Google Cloud with LiveKit Agents.
Python package
The
livekit-plugins-google
package on PyPI.
Plugin reference
Reference for the Google Cloud STT plugin.
GitHub repo
View the source or contribute to the LiveKit Google Cloud STT plugin.
Google Cloud docs
Google Cloud STT docs.
Voice AI quickstart
Get started with LiveKit Agents and Google Cloud STT.
Google ecosystem guide
Overview of the entire Google AI and LiveKit Agents integration.
On this page
Overview
Quick reference
Installation
Authentication
Usage
Parameters
Additional resources


Content from https://docs.livekit.io/agents/integrations/tts/azure:

On this page
Overview
Quick reference
Installation
Authentication
Usage
Parameters
Controlling speech and pronunciation
Additional resources
Copy page
See more page options
Overview
Azure Speech
provides a
streaming TTS service
with high accuracy, realtime transcription. You can use the open source Azure Speech plugin for LiveKit Agents to build voice AI with fast, accurate transcription.
Quick reference
This section provides a brief overview of the Azure Speech TTS plugin. For more information, see
Additional resources
.
Installation
Install the plugin from PyPI:
pip
install
"livekit-agents[azure]~=1.0"
Authentication
The Azure Speech plugin requires an
Azure Speech key
.
Set the following environment variables in your
.env
file:
AZURE_SPEECH_KEY
=
<
azure-speech-key
>
AZURE_SPEECH_REGION
=
<
azure-speech-region
>
AZURE_SPEECH_HOST
=
<
azure-speech-host
>
Usage
Use an Azure Speech TTS within an
AgentSession
or as a standalone speech generator. For example, you can use this TTS in the
Voice AI quickstart
.
from
livekit
.
plugins
import
azure
session
=
AgentSession
(
tts
=
azure
.
TTS
(
speech_key
=
"<speech_service_key>"
,
speech_region
=
"<speech_service_region>"
,
)
,
# ... llm, stt, etc.
)
Note
To create an instance of
azure.TTS
, one of the following options must be met:
speech_host
must be set,
or
speech_key
and
speech_region
must both be set,
or
speech_auth_token
and
speech_region
must both be set.
Parameters
This section describes some of the available parameters. See the
plugin reference
for a complete list of all available parameters.
voice
string
Optional
#
Voice for text-to-speech. To learn more, see
Select synthesis language and voice
.
language
string
Optional
#
Language of the input text. To learn more, see
Select synthesis language and voice
.
prosody
ProsodyConfig
Optional
#
Specify changes to pitch, rate, and volume for the speech output. To learn more,
see
Adjust prosody
.
speech_key
string
Optional
Env:
AZURE_SPEECH_KEY
#
Azure Speech speech-to-text key. To learn more, see
Azure Speech prerequisites
.
speech_region
string
Optional
Env:
AZURE_SPEECH_REGION
#
Azure Speech speech-to-text region. To learn more, see
Azure Speech prerequisites
.
speech_host
string
Optional
Env:
AZURE_SPEECH_HOST
#
Azure Speech endpoint.
speech_auth_token
string
Optional
#
Azure Speech authentication token.
Controlling speech and pronunciation
Azure Speech TTS supports Speech Synthesis Markup Language (SSML) for customizing generated speech. To learn
more, see
SSML overview
.
Additional resources
The following resources provide more information about using Azure Speech with LiveKit Agents.
Python package
The
livekit-plugins-azure
package on PyPI.
Plugin reference
Reference for the Azure Speech TTS plugin.
GitHub repo
View the source or contribute to the LiveKit Azure Speech TTS plugin.
Azure Speech docs
Azure Speech's full docs site.
Voice AI quickstart
Get started with LiveKit Agents and Azure Speech.
Azure ecosystem guide
Overview of the entire Azure AI and LiveKit Agents integration.
On this page
Overview
Quick reference
Installation
Authentication
Usage
Parameters
Controlling speech and pronunciation
Additional resources


Content from https://docs.livekit.io/agents/integrations/stt/azure:

On this page
Overview
Quick reference
Installation
Authentication
Usage
Parameters
Additional resources
Copy page
See more page options
Overview
Azure Speech
provides a streaming
STT service
with high accuracy, realtime transcription. You can use the open source Azure Speech plugin for LiveKit Agents to build voice AI with fast, accurate transcription.
Quick reference
This section provides a brief overview of the Azure Speech STT plugin. For more information, see
Additional resources
.
Installation
Install the plugin from PyPI:
pip
install
"livekit-agents[azure]~=1.0"
Authentication
The Azure Speech plugin requires an
Azure Speech key
.
Set the following environment variables in your
.env
file:
AZURE_SPEECH_KEY
=
<
azure-speech-key
>
AZURE_SPEECH_REGION
=
<
azure-speech-region
>
AZURE_SPEECH_HOST
=
<
azure-speech-host
>
Usage
Use Azure Speech STT in an
AgentSession
or as a standalone transcription service. For example, you can use this STT in the
Voice AI quickstart
.
from
livekit
.
plugins
import
azure
azure_stt
=
stt
.
STT
(
speech_key
=
"<speech_service_key>"
,
speech_region
=
"<speech_service_region>"
,
)
Note
To create an instance of
azure.STT
, one of the following options must be met:
speech_host
must be set,
or
speech_key
and
speech_region
must both be set,
or
speech_auth_token
and
speech_region
must both be set
Parameters
This section describes some of the available parameters. See the
plugin reference
for a complete list of all available parameters.
speech_key
string
Optional
Env:
AZURE_SPEECH_KEY
#
Azure Speech speech-to-text key. To learn more, see
Azure Speech prerequisites
.
speech_region
string
Optional
Env:
AZURE_SPEECH_REGION
#
Azure Speech speech-to-text region. To learn more, see
Azure Speech prerequisites
.
speech_host
string
Optional
Env:
AZURE_SPEECH_HOST
#
Azure Speech endpoint.
speech_auth_token
string
Optional
#
Azure Speech authentication token.
languages
list[string]
Optional
#
List of potential source languages. To learn more, see
Standard locale names
.
Additional resources
The following resources provide more information about using Azure Speech with LiveKit Agents.
Python package
The
livekit-plugins-azure
package on PyPI.
Plugin reference
Reference for the Azure Speech STT plugin.
GitHub repo
View the source or contribute to the LiveKit Azure Speech STT plugin.
Azure Speech docs
Azure Speech's full docs site.
Voice AI quickstart
Get started with LiveKit Agents and Azure Speech.
Azure ecosystem guide
Overview of the entire Azure AI and LiveKit Agents integration.
On this page
Overview
Quick reference
Installation
Authentication
Usage
Parameters
Additional resources


Content from https://docs.livekit.io/agents/integrations/realtime/azure-openai:

On this page
Overview
Quick reference
Installation
Authentication
Usage
Parameters
Turn detection
Server VAD
Usage with separate TTS
Loading conversation history
Additional resources
Copy page
See more page options
Overview
Azure OpenAI
provides an implementation of OpenAI's Realtime API that enables low-latency, multimodal interactions with realtime audio and text processing through Azure's managed service. Use LiveKit's Azure OpenAI plugin to create an agent that uses the Realtime API.
Note
Using the OpenAI platform instead of Azure? See our
OpenAI Realtime API guide
.
Quick reference
This section includes a basic usage example and some reference material. For links to more detailed documentation, see
Additional resources
.
Installation
Install the OpenAI plugin from PyPI:
pip
install
"livekit-agents[openai]~=1.0"
Authentication
The Azure OpenAI plugin requires an
Azure OpenAI API key
and your Azure OpenAI endpoint.
Set the following environment variables in your
.env
file:
AZURE_OPENAI_API_KEY
=
<
your-azure-openai-api-key
>
AZURE_OPENAI_ENDPOINT
=
<
your-azure-openai-endpoint
>
OPENAI_API_VERSION
=
2024
-10-01-preview
Usage
Use the Azure OpenAI Realtime API within an
AgentSession
:
from
livekit
.
plugins
import
openai
session
=
AgentSession
(
llm
=
openai
.
realtime
.
RealtimeModel
.
with_azure
(
azure_deployment
=
"<model-deployment>"
,
azure_endpoint
=
"wss://<endpoint>.openai.azure.com/"
,
api_key
=
"<api-key>"
,
api_version
=
"2024-10-01-preview"
,
)
,
)
For a more comprehensive agent example, see the
Voice AI quickstart
.
Parameters
This section describes the Azure-specific parameters. For a complete list of all available parameters, see the
plugin documentation
.
azure_deployment
string
Required
#
Name of your model deployment.
entra_token
string
Optional
#
Microsoft Entra ID authentication token. Required if not using API key authentication.
To learn more see Azure's
Authentication
documentation.
voice
string
Optional
Default:
alloy
#
Voice to use for speech. To learn more, see
Voice options
.
temperature
float
Optional
Default:
1.0
#
A measure of randomness of completions. A lower temperature is more deterministic. To learn more, see
chat completions
.
instructions
string
Optional
#
Initial system instructions.
modalities
list[api_proto.Modality]
Optional
Default:
["text", "audio"]
#
Modalities to use, such as ["text", "audio"]. Set to
["text"]
to use the model in text-only mode with a
separate TTS plugin
.
turn_detection
TurnDetection | None
Optional
#
Configuration for turn detection, see the section on
Turn detection
for more information.
Turn detection
The Azure OpenAI Realtime API includes
voice activity detection (VAD)
to automatically detect when a user has started or stopped speaking. This feature is enabled by default
There is one supported mode for VAD:
Server VAD
(default) - Uses periods of silence to automatically chunk the audio
Server VAD
Server VAD is the default mode and can be configured with the following properties:
from
livekit
.
plugins
.
openai
import
realtime
from
openai
.
types
.
beta
.
realtime
.
session
import
TurnDetection
session
=
AgentSession
(
llm
=
realtime
.
RealtimeModel
(
turn_detection
=
TurnDetection
(
type
=
"server_vad"
,
threshold
=
0.5
,
prefix_padding_ms
=
300
,
silence_duration_ms
=
500
,
create_response
=
True
,
interrupt_response
=
True
,
)
)
,
)
threshold
: Higher values require louder audio to activate, better for noisy environments.
prefix_padding_ms
: Amount of audio to include before detected speech.
silence_duration_ms
: Duration of silence to detect speech stop (shorter = faster turn detection).
Usage with separate TTS
To use the Azure OpenAI Realtime API with a different
TTS provider
, configure it with a text-only response modality and include a TTS plugin in your
AgentSession
configuration. This configuration allows you to gain the benefits of direct speech understanding while maintaining complete control over the speech output.
session
=
AgentSession
(
llm
=
openai
.
realtime
.
RealtimeModel
.
with_azure
(
# ... endpoint and auth params ...,
modalities
=
[
"text"
]
)
,
tts
=
cartesia
.
TTS
(
)
# Or other TTS plugin of your choice
)
Loading conversation history
If you load conversation history into the model, it might respond with text output even if configured for audio response. To work around this issue, use the model
with a separate TTS plugin
and text-only response modality. You can use the
Azure OpenAI TTS plugin
to continue using the same voices supported by the Realtime API.
For additional workaround options, see the OpenAI
thread
on this topic.
Additional resources
The following resources provide more information about using Azure OpenAI with LiveKit Agents.
Python package
The
livekit-plugins-openai
package on PyPI.
Plugin reference
Reference for the Azure OpenAI Realtime plugin.
GitHub repo
View the source or contribute to the LiveKit OpenAI Realtime plugin.
Azure OpenAI docs
Azure OpenAI service documentation.
Voice AI quickstart
Get started with LiveKit Agents and Azure OpenAI.
Azure ecosystem overview
Overview of the entire Azure AI ecosystem and LiveKit Agents integration.
On this page
Overview
Quick reference
Installation
Authentication
Usage
Parameters
Turn detection
Server VAD
Usage with separate TTS
Loading conversation history
Additional resources


Content from https://docs.livekit.io/agents/integrations/llm/azure-openai:

On this page
Overview
Quick reference
Installation
Authentication
Usage
Parameters
Additional resources
Copy page
See more page options
Overview
Azure OpenAI
provides access to OpenAI's powerful language models like
gpt-4o
and
o1
through Azure's managed service. With LiveKit's Azure OpenAI integration and the Agents framework, you can build sophisticated voice AI applications using their industry-leading models.
Note
Using the OpenAI platform instead of Azure? See our
OpenAI LLM integration guide
.
Quick reference
This section includes a basic usage example and some reference material. For links to more detailed documentation, see
Additional resources
.
Installation
Install the plugin from PyPI:
pip
install
"livekit-agents[openai]~=1.0"
Authentication
The Azure OpenAI plugin requires either an
Azure OpenAI API key
or a Microsoft Entra ID token.
Set the following environment variables in your
.env
file:
AZURE_OPENAI_API_KEY
or
AZURE_OPENAI_ENTRA_TOKEN
AZURE_OPENAI_ENDPOINT
OPENAI_API_VERSION
Usage
Use Azure OpenAI within an
AgentSession
or as a standalone LLM service. For example,
you can use this LLM in the
Voice AI quickstart
.
from
livekit
.
plugins
import
openai
session
=
AgentSession
(
llm
=
openai
.
LLM
.
with_azure
(
azure_deployment
=
"<model-deployment>"
,
azure_endpoint
=
"https://<endpoint>.openai.azure.com/"
,
# or AZURE_OPENAI_ENDPOINT
api_key
=
"<api-key>"
,
# or AZURE_OPENAI_API_KEY
api_version
=
"2024-10-01-preview"
,
# or OPENAI_API_VERSION
)
,
# ... tts, stt, vad, turn_detection, etc.
)
Parameters
This section describes the Azure-specific parameters. For a complete list of all available parameters, see the
plugin documentation
.
azure_deployment
string
Required
#
Name of your model deployment.
entra_token
string
Optional
#
Microsoft Entra ID authentication token. Required if not using API key authentication.
To learn more see Azure's
Authentication
documentation.
Additional resources
The following links provide more information about the Azure OpenAI LLM plugin.
Python package
The
livekit-plugins-openai
package on PyPI.
Plugin reference
Reference for the Azure OpenAI LLM plugin.
GitHub repo
View the source or contribute to the LiveKit OpenAI LLM plugin.
Azure OpenAI docs
Azure OpenAI service documentation.
Voice AI quickstart
Get started with LiveKit Agents and Azure OpenAI.
Azure ecosystem overview
Overview of the entire Azure AI ecosystem and LiveKit Agents integration.
On this page
Overview
Quick reference
Installation
Authentication
Usage
Parameters
Additional resources


Content from https://docs.livekit.io/agents/integrations/tts/azure-openai:

On this page
Overview
Quick reference
Installation
Authentication
Usage
Parameters
Additional resources
Copy page
See more page options
Overview
Azure OpenAI
provides OpenAI services hosted on Azure. With LiveKit's Azure OpenAI TTS integration and the Agents framework, you can build voice AI applications that sound realistic and natural.
To learn more about TTS and generating agent speech, see
Agent speech
.
Quick reference
This section includes a basic usage example and some reference material. For links to more detailed documentation, see
Additional resources
.
Installation
Support for Azure OpenAI TTS is available in the
openai
plugin.
Install the plugin from PyPI:
pip
install
"livekit-agents[openai]~=1.0"
Authentication
The Azure OpenAI TTS requires
authentication
using an API key or a managed identity.
Set the following environment variables in your
.env
file:
AZURE_OPENAI_API_KEY
=
<
azure-openai-api-key
>
AZURE_OPENAI_AD_TOKEN
=
<
azure-openai-ad-token
>
AZURE_OPENAI_ENDPOINT
=
<
azure-openai-endpoint
>
Usage
Use Azure OpenAI TTS within an
AgentSession
or as a standalone speech generator. For example,
you can use this TTS in the
Voice AI quickstart
.
from
livekit
.
plugins
import
openai
session
=
AgentSession
(
tts
=
openai
.
TTS
.
with_azure
(
model
=
"gpt-4o-mini-tts"
,
voice
=
"coral"
,
)
# ... llm, stt, etc.
)
Parameters
This section describes some of the available parameters. For a complete reference of all available parameters, see the
plugin reference
.
model
string
Optional
Default:
gpt-4o-mini-tts
#
ID of the model to use for TTS. To learn more, see
Text to speech models
.
voice
string
Optional
Default:
ash
#
OpenAI text-to-speech voice. To learn more, see the list of supported voices for
voice
in the
Azure documentation
.
instructions
string
Optional
#
Instructions to control tone, style, and other characteristics of the speech.
azure_endpoint
string
Optional
Env:
AZURE_OPENAI_ENDPOINT
#
Azure OpenAI endpoint in the following format:
https://{your-resource-name}.openai.azure.com
.
azure_deployment
string
Optional
#
Name of your model deployment.
api_version
string
Optional
Env:
OPENAI_API_VERSION
#
OpenAI REST API version used for the request.
api_key
string
Optional
Env:
AZURE_OPENAI_API_KEY
#
Azure OpenAI API key.
azure_ad_token
string
Optional
Env:
AZURE_OPENAI_AD_TOKEN
#
Azure Active Directory token.
organization
string
Optional
Env:
OPENAI_ORG_ID
#
OpenAI organization ID.
project
string
Optional
Env:
OPENAI_PROJECT_ID
#
OpenAI project ID.
Additional resources
The following resources provide more information about using Azure OpenAI with LiveKit Agents.
Python package
The
livekit-plugins-openai
package on PyPI.
Plugin reference
Reference for the Azure OpenAI TTS plugin.
GitHub repo
View the source or contribute to the LiveKit Azure OpenAI plugin.
Azure OpenAI
Azure OpenAI documentation.
Voice AI quickstart
Get started with LiveKit Agents and Azure OpenAI.
Azure ecosystem guide
Overview of the entire Azure AI and LiveKit Agents integration.
On this page
Overview
Quick reference
Installation
Authentication
Usage
Parameters
Additional resources


Content from https://docs.livekit.io/agents/integrations/stt/azure-openai:

On this page
Overview
Quick reference
Installation
Authentication
Usage
Parameters
Additional resources
Copy page
See more page options
Overview
Azure OpenAI
provides OpenAI services hosted on Azure. With LiveKit's Azure OpenAI STT integration and the Agents framework, you can build voice AI applications with fast, accurate transcription.
Quick reference
This section includes a basic usage example and some reference material. For links to more detailed documentation, see
Additional resources
.
Installation
Support for Azure OpenAI STT is available in the
openai
plugin.
Install the plugin from PyPI:
pip
install
"livekit-agents[openai]~=1.0"
Authentication
The Azure OpenAI TTS requires
authentication
using an API key or a managed identity.
Set the following environment variables in your
.env
file:
AZURE_OPENAI_API_KEY
=
<
azure-openai-api-key
>
AZURE_OPENAI_AD_TOKEN
=
<
azure-openai-ad-token
>
AZURE_OPENAI_ENDPOINT
=
<
azure-openai-endpoint
>
Usage
Use Azure OpenAI STT within an
AgentSession
or as a standalone transcription service. For example,
you can use this STT in the
Voice AI quickstart
.
from
livekit
.
plugins
import
openai
session
=
AgentSession
(
stt
=
openai
.
STT
.
with_azure
(
model
=
"gpt-4o-transcribe"
,
)
,
# ... llm, tts, etc.
)
Parameters
This section describes some of the available parameters. For a complete reference of all available parameters, see the
plugin reference
.
language
string
Optional
Default:
en
#
Language code for the transcription.
model
STTModels | string
Optional
Default:
gpt-4o-mini-transcribe
#
ID of the model to use for speech-to-text.
prompt
string
Optional
#
Initial prompt to guide the transcription.
azure_endpoint
string
Optional
Env:
AZURE_OPENAI_ENDPOINT
#
Azure OpenAI endpoint in the following format:
https://{your-resource-name}.openai.azure.com
.
azure_deployment
string
Optional
#
Name of your model deployment.
api_version
string
Optional
Env:
OPENAI_API_VERSION
#
OpenAI REST API version used for the request.
api_key
string
Optional
Env:
AZURE_OPENAI_API_KEY
#
Azure OpenAI API key.
azure_ad_token
string
Optional
Env:
AZURE_OPENAI_AD_TOKEN
#
Azure Active Directory token.
organization
string
Optional
Env:
OPENAI_ORG_ID
#
OpenAI organization ID.
project
string
Optional
Env:
OPENAI_PROJECT_ID
#
OpenAI project ID.
Additional resources
The following resources provide more information about using Azure OpenAI with LiveKit Agents.
Python package
The
livekit-plugins-openai
package on PyPI.
Plugin reference
Reference for the Azure OpenAI STT plugin.
GitHub repo
View the source or contribute to the LiveKit Azure OpenAI plugin.
Azure OpenAI
Azure OpenAI documentation.
Voice AI quickstart
Get started with LiveKit Agents and Azure OpenAI.
Azure ecosystem guide
Overview of the entire Azure AI and LiveKit Agents integration.
On this page
Overview
Quick reference
Installation
Authentication
Usage
Parameters
Additional resources


Content from https://docs.livekit.io/agents/integrations/llm/aws:

On this page
Overview
Quick reference
Installation
Authentication
Usage
Parameters
Amazon Nova Sonic
Additional resources
Copy page
See more page options
Overview
Amazon Bedrock
is a fully managed service that provides a wide range of pre-trained models. With LiveKit's open source Bedrock integration and the Agents framework, you can build sophisticated voice AI applications using models from a wide variety of providers.
Quick reference
This section includes a basic usage example and some reference material. For links to more detailed documentation, see
Additional resources
.
Installation
Install the plugin from PyPI:
pip
install
"livekit-agents[aws]~=1.0"
Authentication
The AWS plugin requires AWS credentials. Set the following environment variables in your
.env
file:
AWS_ACCESS_KEY_ID
=
<
your-aws-access-key-id
>
AWS_SECRET_ACCESS_KEY
=
<
your-aws-secret-access-key
>
Usage
Use Bedrock within an
AgentSession
or as a standalone LLM service. For example,
you can use this LLM in the
Voice AI quickstart
.
from
livekit
.
plugins
import
aws
session
=
AgentSession
(
llm
=
aws
.
LLM
(
model
=
"anthropic.claude-3-5-sonnet-20240620-v1:0"
,
temperature
=
0.8
,
)
,
# ... tts, stt, vad, turn_detection, etc.
)
Parameters
This section describes some of the available parameters. For a complete reference of all available parameters, see the
plugin reference
.
model
string | TEXT_MODEL
Optional
Default:
anthropic.claude-3-5-sonnet-20240620-v1:0
#
The model to use for the LLM. For more information, see the documentation for the
modelId
parameter in the
Amazon Bedrock API reference
.
region
string
Optional
Default:
us-east-1
#
The region to use for AWS API requests.
temperature
float
Optional
#
A measure of randomness in output. A lower value results in more predictable output, while a higher value results in
more creative output.
Default values vary depending on the model you select. To learn more, see
Inference request parameters and response fields for foundation models
.
tool_choice
[ToolChoice | Literal['auto', 'required', 'none']]
Optional
Default:
auto
#
Specifies whether to use tools during response generation.
Amazon Nova Sonic
To use Amazon Nova Sonic on AWS Bedrock, refer to the following integration guide:
Amazon Nova Sonic
Integration guide for the Amazon Nova Sonic speech-to-speech model on AWS Bedrock.
Additional resources
The following links provide more information about the Amazon Bedrock LLM plugin.
Python package
The
livekit-plugins-aws
package on PyPI.
Plugin reference
Reference for the Amazon Bedrock LLM plugin.
GitHub repo
View the source or contribute to the LiveKit Amazon Bedrock LLM plugin.
Bedrock docs
Amazon Bedrock docs.
Voice AI quickstart
Get started with LiveKit Agents and Amazon Bedrock.
AWS ecosystem guide
Overview of the entire AWS and LiveKit Agents integration.
On this page
Overview
Quick reference
Installation
Authentication
Usage
Parameters
Amazon Nova Sonic
Additional resources


Content from https://docs.livekit.io/agents/integrations/tts/aws:

On this page
Overview
Quick reference
Installation
Authentication
Usage
Parameters
Controlling speech and pronunciation
Additional resources
Copy page
See more page options
Overview
Amazon Polly
is an AI voice generator that provides high-quality, natural-sounding
human voices in multiple languages. With LiveKit's Amazon Polly integration and the Agents framework, you can build
voice AI applications that sound realistic.
Quick reference
This section includes a basic usage example and some reference material. For links to more detailed documentation, see
Additional resources
.
Installation
Install the plugin from PyPI:
pip
install
"livekit-agents[aws]~=1.0"
Authentication
The Amazon Polly plugin requires an
AWS API key
.
Set the following environment variables in your
.env
file:
AWS_ACCESS_KEY_ID
=
<
aws-access-key-id
>
AWS_SECRET_ACCESS_KEY
=
<
aws-secret-access-key
>
AWS_DEFAULT_REGION
=
<
aws-deployment-region
>
Usage
Use an Amazon Polly TTS within an
AgentSession
or as a standalone speech generator. For example, you can use this TTS in the
Voice AI quickstart
.
from
livekit
.
plugins
import
aws
session
=
AgentSession
(
tts
=
aws
.
TTS
(
voice
=
"Ruth"
,
speech_engine
=
"generative"
,
language
=
"en-US"
,
)
,
# ... llm, stt, etc.
)
Parameters
This section describes some of the parameters. See the
plugin reference
for a complete list of all available parameters.
voice
TTSModels
Optional
Default:
Ruth
#
Voice to use for the synthesis. For a full list, see
Available voices
.
language
TTS_LANGUAGE | string
Optional
#
Language code for the Synthesize Speech request. This is only necessary if using a bilingual voice, such as Aditi,
which can be used for either Indian English (en-IN) or Hindi (hi-IN). To learn more,
see
Languages in Amazon Polly
.
speech_engine
TTS_SPEECH_ENGINE
Optional
Default:
generative
#
The voice engine to use for the synthesis. Valid values are
standard
,
neural
,
long-form
, and
generative
.
To learn more, see
Amazon Polly voice engines
.
Controlling speech and pronunciation
Amazon Polly supports Speech Synthesis Markup Language (SSML) for customizing generated speech. To learn more, see
Generating speech from SSML docs
and
Supported SSML tags
.
Additional resources
The following resources provide more information about using Amazon Polly with LiveKit Agents.
Python package
The
livekit-plugins-aws
package on PyPI.
Plugin reference
Reference for the Amazon Polly TTS plugin.
GitHub repo
View the source or contribute to the LiveKit Amazon Polly TTS plugin.
AWS docs
Amazon Polly's full docs site.
Voice AI quickstart
Get started with LiveKit Agents and Amazon Polly.
On this page
Overview
Quick reference
Installation
Authentication
Usage
Parameters
Controlling speech and pronunciation
Additional resources


Content from https://docs.livekit.io/agents/integrations/stt/aws:

On this page
Quick reference
Installation
Authentication
Usage
Parameters
Additional resources
Copy page
See more page options
Amazon Transcribe
provides a streaming STT service with high accuracy, realtime transcription. You can use the open source Amazon Transcribe plugin for LiveKit Agents to build voice AI with fast, accurate transcription.
Quick reference
This section provides a brief overview of the Amazon Transcribe STT plugin. For more information, see
Additional resources
.
Installation
Install the plugin from PyPI:
pip
install
"livekit-agents[aws]~=1.0"
Authentication
The Amazon Transcribe plugin requires an
AWS API key
.
Set the following environment variables in your
.env
file:
AWS_ACCESS_KEY_ID
=
<
aws-access-key-id
>
AWS_SECRET_ACCESS_KEY
=
<
aws-secret-access-key
>
AWS_DEFAULT_REGION
=
<
aws-deployment-region
>
Usage
Use Amazon Transcribe STT in an
AgentSession
or as a standalone transcription service. For example, you can use this STT in the
Voice AI quickstart
.
from
livekit
.
plugins
import
aws
session
=
AgentSession
(
stt
=
aws
.
STT
(
session_id
=
"my-session-id"
,
language
=
"en-US"
,
vocabulary_name
=
"my-vocabulary"
,
vocab_filter_name
=
"my-vocab-filter"
,
vocab_filter_method
=
"mask"
,
)
,
# ... llm, tts, etc.
)
Parameters
This section describes some of the available parameters. See the
plugin reference
for a complete list of all available parameters.
speech_region
string
Optional
Default:
us-east-1
Env:
AWS_DEFAULT_REGION
#
The region of the AWS deployment. Required if the environment variable isn't set.
language
string
Optional
Default:
en-US
#
The language of the audio. For a full list of supported languages, see the
Supported languages
page.
vocabulary_name
string
Optional
Default:
None
#
Name of the custom vocabulary you want to use when processing your transcription. To learn more, see
Custom vocabularies
.
session_id
string
Optional
#
Name for your transcription session. If left empty, Amazon Transcribe generates an ID and returns it in the response.
vocab_filter_name
string
Optional
Default:
None
#
Name of the custom vocabulary filter that you want to use when processing your transcription. To learn more, see
Using custom vocabulary filters to delete, mask, or flag words
.
vocab_filter_method
string
Optional
Default:
None
#
Display method for the vocabulary filter. To learn more, see
Using custom vocabulary filters to delete, mask, or flag words
.
Additional resources
The following resources provide more information about using Amazon Transcribe with LiveKit Agents.
Python package
The
livekit-plugins-aws
package on PyPI.
Plugin reference
Reference for the Amazon Transcribe STT plugin.
GitHub repo
View the source or contribute to the LiveKit Amazon Transcribe STT plugin.
AWS docs
Amazon Transcribe's full docs site.
Voice AI quickstart
Get started with LiveKit Agents and Amazon Transcribe.
On this page
Quick reference
Installation
Authentication
Usage
Parameters
Additional resources


Content from https://docs.livekit.io/agents/integrations/realtime/nova-sonic:

On this page
Overview
Quick reference
Installation
Authentication
Usage
Parameters
Turn detection
Additional resources
Copy page
See more page options
Overview
Amazon
Nova Sonic
is a state of the art speech-to-speech model with a bidirectional audio streaming API. Nova Sonic processes and responds to realtime speech as it occurs, enabling natural, human-like conversational experiences. LiveKit's AWS plugin includes support for Nova Sonic on AWS Bedrock, allowing you to use this model to create true realtime conversational agents.
Quick reference
This section includes a basic usage example and some reference material. For links to more detailed documentation, see
Additional resources
.
Installation
Install the AWS plugin from PyPI with the
realtime
extra:
pip
install
"livekit-plugins-aws[realtime]"
Authentication
The AWS plugin requires AWS credentials. Set the following environment variables in your
.env
file:
AWS_ACCESS_KEY_ID
=
<
your-aws-access-key-id
>
AWS_SECRET_ACCESS_KEY
=
<
your-aws-secret-access-key
>
Usage
Use the Nova Sonic API within an
AgentSession
. For example, you can use it in the
Voice AI quickstart
.
from
livekit
.
plugins
import
aws
session
=
AgentSession
(
llm
=
aws
.
realtime
.
RealtimeModel
(
)
,
)
Parameters
This section describes some of the available parameters. For a complete reference of all available parameters, see the
plugin reference
.
voice
string
Optional
#
Name of the Nova Sonic API voice. For a full list, see
Voices
.
region
string
Optional
#
AWS region of the Bedrock runtime endpoint.
Turn detection
The Nova Sonic API includes built-in VAD-based turn detection, which is currently the only supported turn detection method.
Additional resources
The following resources provide more information about using Nova Sonic with LiveKit Agents.
Python package
The
livekit-plugins-aws
package on PyPI.
Plugin reference
Reference for the Nova Sonic integration.
GitHub repo
View the source or contribute to the LiveKit AWS plugin.
Nova Sonic docs
Nova Sonic API documentation.
Voice AI quickstart
Get started with LiveKit Agents and Amazon Nova Sonic.
AWS AI ecosystem guide
Overview of the entire AWS AI and LiveKit Agents integration.
On this page
Overview
Quick reference
Installation
Authentication
Usage
Parameters
Turn detection
Additional resources


Content from https://docs.livekit.io/agents/integrations/llm/groq:

On this page
Overview
Quick reference
Installation
Authentication
Usage
Parameters
Additional resources
Copy page
See more page options
Try out Groq Talk
See our voice assistant playground using Groq STT, LLM, and TTS
Overview
Groq
provides fast LLM inference using open models from Llama, DeepSeek, and more. With LiveKit's Groq integration and the Agents framework, you can build low-latency voice AI applications.
Quick reference
This section includes a basic usage example and some reference material. For links to more detailed documentation, see
Additional resources
.
Installation
Install the plugin from PyPI:
pip
install
"livekit-agents[groq]~=1.0"
Authentication
The Groq plugin requires a
Groq API key
.
Set
GROQ_API_KEY
in your
.env
file.
Usage
Use a Groq LLM in your
AgentSession
or as a standalone LLM service. For example,
you can use this LLM in the
Voice AI quickstart
.
from
livekit
.
plugins
import
groq
session
=
AgentSession
(
llm
=
groq
.
LLM
(
model
=
"llama3-8b-8192"
)
,
# ... tts, stt, vad, turn_detection, etc.
)
Parameters
This section describes some of the available parameters. For a complete reference of all available parameters,
see the
plugin reference
.
model
string
Optional
Default:
llama-3.3-70b-versatile
#
Name of the LLM model to use. For all options, see the
Groq model list
.
temperature
float
Optional
Default:
1.0
#
A measure of randomness in output. A lower value results in more predictable output, while a higher value results in
more creative output.
parallel_tool_calls
bool
Optional
#
Set to true to parallelize tool calls.
tool_choice
ToolChoice | Literal['auto', 'required', 'none']
Optional
Default:
auto
#
Specifies whether to use tools during response generation.
Additional resources
The following resources provide more information about using Groq with LiveKit Agents.
Python package
The
livekit-plugins-groq
package on PyPI.
Plugin reference
Reference for the Groq LLM plugin.
GitHub repo
View the source or contribute to the LiveKit Groq LLM plugin.
Groq docs
Groq docs.
Voice AI quickstart
Get started with LiveKit Agents and Groq.
Groq ecosystem overview
Overview of the entire Groq and LiveKit Agents integration.
On this page
Overview
Quick reference
Installation
Authentication
Usage
Parameters
Additional resources


Content from https://docs.livekit.io/agents/integrations/tts/groq:

On this page
Overview
Quick reference
Installation
Authentication
Usage
Parameters
Additional resources
Copy page
See more page options
Try out Groq Talk
See our voice assistant playground using Groq STT, LLM, and TTS
Overview
Groq
provides fast TTS using models from PlayAI. With LiveKit's Groq integration and the Agents
framework, you can build voice AI applications with fluent and conversational voices. Groq TTS supports English and Arabic speech.
Quick reference
This section includes a basic usage example and some reference material. For links to more detailed documentation, see
Additional resources
.
Installation
Install the plugin from PyPI:
pip
install
"livekit-agents[groq]~=1.0"
Authentication
The Groq plugin requires a
Groq API key
.
Set
GROQ_API_KEY
in your
.env
file.
Usage
Use Groq TTS in your
AgentSession
or as a standalone speech generator. For example,
you can use this TTS in the
Voice AI quickstart
.
from
livekit
.
plugins
import
groq
session
=
AgentSession
(
tts
=
groq
.
TTS
(
model
=
"playai-tts"
,
voice
=
"Arista-PlayAI"
,
)
,
# ... stt, llm, vad, turn_detection, etc.
)
Parameters
This section describes some of the available parameters. See the
plugin reference
for a complete list of all available parameters.
model
TTSModel | string
Optional
Default:
playai-tts
#
Name of the TTS model. For a full list, see
Models
.
voice
string
Optional
Default:
Arista-PlayAI
#
Name of the voice. For a full list, see
English
and
Arabic
voices.
Additional resources
The following resources provide more information about using Groq with LiveKit Agents.
Python package
The
livekit-plugins-groq
package on PyPI.
Plugin reference
Reference for the Groq TTS plugin.
GitHub repo
View the source or contribute to the LiveKit Groq TTS plugin.
Groq docs
Groq TTS docs.
Voice AI quickstart
Get started with LiveKit Agents and Groq TTS.
Groq ecosystem guide
Overview of the entire Groq and LiveKit Agents integration.
On this page
Overview
Quick reference
Installation
Authentication
Usage
Parameters
Additional resources


Content from https://docs.livekit.io/agents/integrations/stt/groq:

On this page
Overview
Quick reference
Installation
Authentication
Usage
Parameters
Additional resources
Copy page
See more page options
Try Groq transcription
Experience Groq's fast STT in a LiveKit-powered playground
Overview
Groq
provides fast STT using fine-tuned and distilled models based on Whisper V3 Large. With LiveKit's Groq integration and the Agents framework, you can build AI voice applications with fluent and conversational voices.
Quick reference
This section includes a basic usage example and some reference material. For links to more detailed documentation, see
Additional resources
.
Installation
Install the plugin from PyPI:
pip
install
"livekit-agents[groq]~=1.0"
Authentication
The Groq plugin requires a
Groq API key
.
Set
GROQ_API_KEY
in your
.env
file.
Usage
Use Groq STT in your
AgentSession
or as a standalone transcription service. For example,
you can use this STT in the
Voice AI quickstart
.
from
livekit
.
plugins
import
groq
session
=
AgentSession
(
stt
=
groq
.
STT
(
model
=
"whisper-large-v3-turbo"
,
language
=
"en"
,
)
,
# ... tts, llm, vad, turn_detection, etc.
)
Parameters
This section describes some of the available parameters. See the
plugin reference
for a complete list of all available parameters.
model
string
Optional
Default:
whisper-large-v3-turbo
#
Name of the STT model to use. For help with model selection, see the
Groq STT documentation
.
language
string
Optional
Default:
en
#
Language of the input audio in
ISO-639-1
format.
prompt
string
Optional
#
Prompt to guide the model's style or specify how to spell unfamiliar words. 224 tokens max.
Additional resources
The following resources provide more information about using Groq with LiveKit Agents.
Python package
The
livekit-plugins-groq
package on PyPI.
Plugin reference
Reference for the Groq STT plugin.
GitHub repo
View the source or contribute to the LiveKit Groq STT plugin.
Groq docs
Groq STT docs.
Voice AI quickstart
Get started with LiveKit Agents and Groq STT.
Groq ecosystem guide
Overview of the entire Groq and LiveKit Agents integration.
On this page
Overview
Quick reference
Installation
Authentication
Usage
Parameters
Additional resources


Content from https://docs.livekit.io/agents/integrations/llm/anthropic:

On this page
Overview
Quick reference
Installation
Authentication
Usage
Parameters
Additional resources
Copy page
See more page options
Overview
Anthropic
provides Claude, an advanced AI assistant with capabilities including advanced reasoning, vision analysis, code generation, and multilingual processing. With LiveKit's Anthropic integration and the Agents framework, you can build sophisticated voice AI applications.
You can also use Claude with
Amazon Bedrock
.
Quick reference
This section includes a basic usage example and some reference material. For links to more detailed documentation, see
Additional resources
.
Installation
Install the plugin from PyPI:
pip
install
"livekit-agents[anthropic]~=1.0"
Authentication
The Anthropic plugin requires an
Anthropic API key
.
Set
ANTHROPIC_API_KEY
in your
.env
file.
Usage
Use Claude within an
AgentSession
or as a standalone LLM service. For example,
you can use this LLM in the
Voice AI quickstart
.
from
livekit
.
plugins
import
anthropic
session
=
AgentSession
(
llm
=
anthropic
.
LLM
(
model
=
"claude-3-5-sonnet-20241022"
,
temperature
=
0.8
,
)
,
# ... tts, stt, vad, turn_detection, etc.
)
Parameters
This section describes some of the available parameters. See the
plugin reference
for a complete list of all available parameters.
model
str | ChatModels
Optional
Default:
claude-3-5-sonnet-20241022
#
Model to use. For a full list of available models, see the
Model options
.
max_tokens
int
Optional
#
The maximum number of tokens to generate before stopping. To learn more, see the
Anthropic API reference
.
temperature
float
Optional
Default:
1
#
A measure of randomness in output. A lower value results in more predictable output, while a higher value results in
more creative output.
Valid values are between
0
and
1
. To learn more, see the
Anthropic API reference
.
parallel_tool_calls
bool
Optional
#
Set to true to parallelize tool calls.
tool_choice
ToolChoice | Literal['auto', 'required', 'none']
Optional
Default:
auto
#
Specifies whether to use tools during response generation.
Additional resources
The following links provide more information about the Anthropic LLM plugin.
Python package
The
livekit-plugins-anthropic
package on PyPI.
Plugin reference
Reference for the Anthropic LLM plugin.
GitHub repo
View the source or contribute to the LiveKit Anthropic LLM plugin.
Anthropic docs
Anthropic Claude docs.
Voice AI quickstart
Get started with LiveKit Agents and Anthropic.
On this page
Overview
Quick reference
Installation
Authentication
Usage
Parameters
Additional resources


Content from https://docs.livekit.io/agents/integrations/llm/baseten:

On this page
Overview
Quick reference
Installation
Authentication
Model selection
Usage
Parameters
Additional resources
Copy page
See more page options
Overview
Baseten
is a hosted inference platform that includes a Model API for a variety of popular open source LLMs from Llama, DeepSeek, and more. With LiveKit's Baseten integration and the Agents framework, you can build AI agents on top of these models.
Quick reference
This section includes a basic usage example and some reference material. For links to more detailed documentation, see
Additional resources
.
Installation
Install the plugin from PyPI:
pip
install
"livekit-agents[baseten]~=1.0"
Authentication
The Baseten plugin requires a
Baseten API key
.
Set the following in your
.env
file:
BASETEN_API_KEY
=
<
your-baseten-api-key
>
Model selection
LiveKit Agents integrates with Baseten's Model API, which supports the most popular open source LLMs with per-token billing. To use the Model API, you only need to activate the model and then copy its name.
Activate your desired model in the
Model API
Copy its name from your model API endpoint dialog in your
model library
Use the model name in the plugin (e.g.
"meta-llama/Llama-4-Maverick-17B-128E-Instruct"
)
Usage
Use a Baseten LLM in your
AgentSession
or as a standalone LLM service. For example,
you can use this LLM in the
Voice AI quickstart
.
from
livekit
.
plugins
import
baseten
session
=
AgentSession
(
llm
=
baseten
.
LLM
(
model
=
"meta-llama/Llama-4-Maverick-17B-128E-Instruct"
)
,
# ... tts, stt, vad, turn_detection, etc.
)
Parameters
This section describes some of the available parameters. For a complete reference of all available parameters,
see the
plugin reference
.
model
string
Optional
Default:
meta-llama/Llama-4-Maverick-17B-128E-Instruct
#
Name of the LLM model to use from the
Model API
. See
Model selection
for more information.
Additional resources
The following resources provide more information about using Baseten with LiveKit Agents.
Python package
The
livekit-plugins-baseten
package on PyPI.
Plugin reference
Reference for the Baseten LLM plugin.
GitHub repo
View the source or contribute to the LiveKit Baseten LLM plugin.
Baseten docs
Baseten docs.
Voice AI quickstart
Get started with LiveKit Agents and Baseten.
Baseten TTS
Baseten TTS integration guide.
Baseten STT
Baseten STT integration guide.
On this page
Overview
Quick reference
Installation
Authentication
Model selection
Usage
Parameters
Additional resources


Content from https://docs.livekit.io/agents/integrations/llm/cerebras:

On this page
Overview
Usage
Parameters
Links
Copy page
See more page options
Overview
Cerebras
provides access to Llama 3.1 and 3.3 models through their inference API. These models are multilingual and text-only, making them suitable for a variety of agent applications.
Usage
Install the OpenAI plugin to add Cerebras support:
pip
install
"livekit-agents[openai]~=1.0"
Set the following environment variable in your
.env
file:
CEREBRAS_API_KEY
=
<
your-cerebras-api-key
>
Create a Cerebras LLM using the
with_cerebras
method:
from
livekit
.
plugins
import
openai
session
=
AgentSession
(
llm
=
openai
.
LLM
.
with_cerebras
(
model
=
"llama3.1-8b"
,
temperature
=
0.7
)
,
# ... tts, stt, vad, turn_detection, etc.
)
Parameters
This section describes some of the available parameters. See the
plugin reference
for a complete list of all available parameters.
model
str | CerebrasChatModels
Optional
Default:
llama3.1-8b
#
Model to use for inference. To learn more, see
supported models
.
temperature
float
Optional
Default:
1.0
#
A measure of randomness in output. A lower value results in more predictable output, while a higher value results in
more creative output.
Valid values are between
0
and
1.5
. To learn more, see the
Cerebras documentation
.
parallel_tool_calls
bool
Optional
#
Set to true to parallelize tool calls.
tool_choice
ToolChoice | Literal['auto', 'required', 'none']
Optional
Default:
auto
#
Specifies whether to use tools during response generation.
Links
The following links provide more information about the Cerebras LLM integration.
Python package
The
livekit-plugins-openai
package on PyPI.
Plugin reference
Reference for the
with_cerebras
method of the OpenAI LLM plugin.
GitHub repo
View the source or contribute to the LiveKit OpenAI LLM plugin.
Cerebras docs
Cerebras inference docs.
Voice AI quickstart
Get started with LiveKit Agents and Cerebras.
On this page
Overview
Usage
Parameters
Links


Content from https://docs.livekit.io/agents/integrations/llm/deepseek:

On this page
Overview
Usage
Parameters
Links
Copy page
See more page options
Overview
DeepSeek
provides access to their latest models through their OpenAI-compatible API. These models are multilingual and text-only, making them suitable for a variety of agent applications.
Additional providers
DeepSeek models are also available through a number of other providers, such as
Cerebras
and
Groq
.
Usage
Use the OpenAI plugin's
with_deepseek
method to set the default agent session LLM to DeepSeek:
pip
install
"livekit-agents[openai]~=1.0"
Set the following environment variable in your
.env
file:
DEEPSEEK_API_KEY
=
<
your-deepseek-api-key
>
from
livekit
.
plugins
import
openai
deepseek_llm
=
openai
.
LLM
.
with_deepseek
(
model
=
"deepseek-chat"
,
# this is DeepSeek-V3
temperature
=
0.7
)
Parameters
This section describes some of the available parameters. For a complete reference of all available parameters, see the
method reference
.
model
str | DeepSeekChatModels
Optional
Default:
deepseek-chat
#
DeepSeek model to use. See
models and pricing
for a complete list.
temperature
float
Optional
Default:
1.0
#
Controls the randomness of the model's output. Higher values (e.g., 0.8) make the output more random, while lower values (e.g., 0.2) make it more focused and deterministic.
Valid values are between
0
and
2
.
parallel_tool_calls
bool
Optional
#
Controls whether the model can make multiple tool calls in parallel. When enabled, the model can make multiple tool calls simultaneously, which can improve performance for complex tasks.
tool_choice
ToolChoice | Literal['auto', 'required', 'none']
Optional
Default:
auto
#
Controls how the model uses tools. Set to 'auto' to let the model decide, 'required' to force tool usage, or 'none' to disable tool usage.
Links
The following links provide more information about the DeepSeek LLM integration.
Python package
The
livekit-plugins-openai
package on PyPI.
Plugin reference
Reference for the
with_deepseek
method of the OpenAI LLM plugin.
GitHub repo
View the source or contribute to the LiveKit OpenAI LLM plugin.
DeepSeek docs
DeepSeek API documentation.
Voice AI quickstart
Get started with LiveKit Agents and DeepSeek.
On this page
Overview
Usage
Parameters
Links


Content from https://docs.livekit.io/agents/integrations/llm/fireworks:

On this page
Overview
Usage
Parameters
Links
Copy page
See more page options
Overview
Fireworks AI
provides access to Llama 3.1 instruction-tuned models through their inference API. These models are multilingual and text-only, making them suitable for a variety of agent applications.
Usage
Install the OpenAI plugin to add Fireworks AI support:
pip
install
"livekit-agents[openai]~=1.0"
Set the following environment variable in your
.env
file:
FIREWORKS_API_KEY
=
<
your-fireworks-api-key
>
Create a Fireworks AI LLM using the
with_fireworks
method:
from
livekit
.
plugins
import
openai
session
=
AgentSession
(
llm
=
openai
.
LLM
.
with_fireworks
(
model
=
"accounts/fireworks/models/llama-v3p3-70b-instruct"
,
temperature
=
0.7
)
,
# ... tts, stt, vad, turn_detection, etc.
)
Parameters
This section describes some of the available parameters. For a complete reference of all available parameters, see the
method reference
.
model
str
Optional
Default:
accounts/fireworks/models/llama-v3p3-70b-instruct
#
Model to use for inference. To learn more, see
supported models
.
temperature
float
Optional
Default:
1.0
#
Controls the randomness of the model's output. Higher values (e.g., 0.8) make the output more random, while lower values (e.g., 0.2) make it more focused and deterministic.
Valid values are between
0
and
1.5
.
parallel_tool_calls
bool
Optional
#
Controls whether the model can make multiple tool calls in parallel. When enabled, the model can make multiple tool calls simultaneously, which can improve performance for complex tasks.
tool_choice
ToolChoice | Literal['auto', 'required', 'none']
Optional
Default:
auto
#
Controls how the model uses tools. Set to 'auto' to let the model decide, 'required' to force tool usage, or 'none' to disable tool usage.
Links
The following links provide more information about the Fireworks AI LLM integration.
Python package
The
livekit-plugins-openai
package on PyPI.
Plugin reference
Reference for the
with_fireworks
method of the OpenAI LLM plugin.
GitHub repo
View the source or contribute to the LiveKit OpenAI LLM plugin.
Fireworks AI docs
Fireworks AI API documentation.
Voice AI quickstart
Get started with LiveKit Agents and Fireworks AI.
On this page
Overview
Usage
Parameters
Links


Content from https://docs.livekit.io/agents/integrations/llm/langchain:

On this page
Overview
Quick reference
Installation
Usage
Parameters
Additional resources
Copy page
See more page options
Overview
LangChain
is a framework for developing applications powered by large language models. The LiveKit LangChain plugin allows you to integrate existing LangGraph workflows as LLMs in your voice AI applications.
Quick reference
This section includes a basic usage example and some reference material. For links to more detailed documentation, see
Additional resources
.
Installation
Install the LiveKit LangChain plugin from PyPI:
pip
install
"livekit-plugins-langchain~=1.1"
Usage
Use LangGraph workflows within an
AgentSession
by wrapping them with the
LLMAdapter
. For example,
you can use this LLM in the
Voice AI quickstart
.
from
langgraph
.
graph
import
StateGraph
from
livekit
.
agents
import
AgentSession
,
Agent
from
livekit
.
plugins
import
langchain
# Define your LangGraph workflow
def
create_workflow
(
)
:
workflow
=
StateGraph
(
.
.
.
)
# Add your nodes and edges
return
workflow
.
compile
(
)
# Use the workflow as an LLM
session
=
AgentSession
(
llm
=
langchain
.
LLMAdapter
(
graph
=
create_workflow
(
)
)
,
# ... stt, tts, vad, turn_detection, etc.
)
The
LLMAdapter
automatically converts the LiveKit chat context to
LangChain messages
. The mapping is as follows:
system
and
developer
messages to
SystemMessage
user
messages to
HumanMessage
assistant
messages to
AIMessage
Parameters
This section describes the available parameters for the
LLMAdapter
. See the
plugin reference
for a complete list of all available parameters.
graph
PregelProtocol
Required
#
The LangGraph workflow to use as an LLM. Must be a locally compiled graph. To learn more, see
Graph Definitions
.
config
RunnableConfig | None
Optional
Default:
None
#
Configuration options for the LangGraph workflow execution. This can include runtime configuration, callbacks, and other LangGraph-specific options. To learn more, see
RunnableConfig
.
Additional resources
The following resources provide more information about using LangChain with LiveKit Agents.
Python package
The
livekit-plugins-langchain
package on PyPI.
Plugin reference
Reference for the LangChain LLM adapter.
GitHub repo
View the source or contribute to the LiveKit LangChain plugin.
LangChain docs
LangChain documentation and tutorials.
LangGraph docs
LangGraph documentation for building stateful workflows.
Voice AI quickstart
Get started with LiveKit Agents and LangChain.
On this page
Overview
Quick reference
Installation
Usage
Parameters
Additional resources


Content from https://docs.livekit.io/agents/integrations/llm/letta:

On this page
Overview
Quick reference
Installation
Authentication
Usage
Parameters
Additional resources
Copy page
See more page options
Overview
Letta
enables you to build and deploy stateful AI agents that maintain memory and context across long-running conversations. You can build sophisticated voice AI applications using a Letta agent as the LLM in your STT-LLM-TTS pipeline.
Quick reference
This section includes a basic usage example and some reference material. For links to more detailed documentation, see
Additional resources
.
Installation
Install the OpenAI plugin to add Letta support:
pip
install
"livekit-agents[openai]~=1.0"
Authentication
If your Letta server requires authentication, you need to provide an API key. Set the following environment variable in your
.env
file:
LETTA_API_KEY
Usage
Use Letta LLM within an
AgentSession
or as a standalone LLM service. For example,
you can use this LLM in the
Voice AI quickstart
.
from
livekit
.
plugins
import
openai
session
=
AgentSession
(
llm
=
openai
.
LLM
.
with_letta
(
agent_id
=
"<agent-id>"
,
)
,
# ... tts, stt, vad, turn_detection, etc.
)
Parameters
This section describes some of the parameters for the
with_letta
method. For a complete list of all available parameters,
see the
plugin documentation
.
agent_id
string
Required
#
Letta
agent ID
. Must begin with
agent-
.
base_url
string
Optional
Default:
https://api.letta.com/v1/voice-beta
#
URL of the Letta server. For example, your
self-hosted server
or
Letta Cloud
.
Additional resources
The following links provide more information about the Letta LLM plugin.
Python package
The
livekit-plugins-openai
package on PyPI.
Plugin reference
Reference for the Letta LLM plugin.
GitHub repo
View the source or contribute to the LiveKit OpenAI LLM plugin.
Letta docs
Letta documentation.
Voice AI quickstart
Get started with LiveKit Agents and Letta.
On this page
Overview
Quick reference
Installation
Authentication
Usage
Parameters
Additional resources


Content from https://docs.livekit.io/agents/integrations/llm/ollama:

On this page
Overview
Usage
Parameters
Links
Copy page
See more page options
Overview
Ollama
is an open source LLM engine that you can use to run models locally with an OpenAI-compatible API. Ollama support is available using the OpenAI plugin for LiveKit Agents.
Usage
Install the OpenAI plugin to add Ollama support:
pip
install
"livekit-agents[openai]~=1.0"
Create an Ollama LLM using the
with_ollama
method:
from
livekit
.
plugins
import
openai
session
=
AgentSession
(
llm
=
openai
.
LLM
.
with_ollama
(
model
=
"llama3.1"
,
base_url
=
"http://localhost:11434/v1"
,
)
,
# ... tts, stt, vad, turn_detection, etc.
)
Parameters
This section describes some of the available parameters. See the
plugin reference
for a complete list of all available parameters.
model
string
Optional
Default:
llama3.1
#
Ollama model to use. For a list of available models, see
Ollama models
.
base_url
string
Optional
Default:
http://localhost:11434/v1
#
Base URL for the Ollama API.
temperature
float
Optional
#
Controls the randomness of the model's output. Higher values (e.g., 0.8) make the output more random, while lower values (e.g., 0.2) make it more focused and deterministic.
Links
The following links provide more information about the Ollama integration.
Python package
The
livekit-plugins-openai
package on PyPI.
Plugin reference
Reference for the
with_ollama
method of the OpenAI LLM plugin.
GitHub repo
View the source or contribute to the LiveKit OpenAI LLM plugin.
Ollama docs
Ollama site and documentation.
Voice AI quickstart
Get started with LiveKit Agents and Ollama.
On this page
Overview
Usage
Parameters
Links


Content from https://docs.livekit.io/agents/integrations/llm/perplexity:

On this page
Overview
Usage
Parameters
Links
Copy page
See more page options
Overview
Perplexity
provides access to their Sonar models, which are based on Llama 3.1 but fine-tuned for search, through their inference API. These models are multilingual and text-only, making them suitable for a variety of agent applications.
Usage
Install the OpenAI plugin to add Perplexity support:
pip
install
"livekit-agents[openai]~=1.0"
Set the following environment variable in your
.env
file:
PERPLEXITY_API_KEY
=
<
your-perplexity-api-key
>
Create a Perplexity LLM using the
with_perplexity
method:
from
livekit
.
plugins
import
openai
session
=
AgentSession
(
llm
=
openai
.
LLM
.
with_perplexity
(
model
=
"llama-3.1-sonar-small-128k-chat"
,
temperature
=
0.7
)
,
# ... tts, stt, vad, turn_detection, etc.
)
Parameters
This section describes some of the available parameters. For a complete reference of all available parameters, see the
method reference
.
model
str | PerplexityChatModels
Optional
Default:
llama-3.1-sonar-small-128k-chat
#
Model to use for inference. To learn more, see
supported models
.
temperature
float
Optional
Default:
1.0
#
Controls the randomness of the model's output. Higher values (e.g., 0.8) make the output more random, while lower values (e.g., 0.2) make it more focused and deterministic.
Valid values are between
0
and
2
.
parallel_tool_calls
bool
Optional
#
Controls whether the model can make multiple tool calls in parallel. When enabled, the model can make multiple tool calls simultaneously, which can improve performance for complex tasks.
tool_choice
ToolChoice | Literal['auto', 'required', 'none']
Optional
Default:
auto
#
Controls how the model uses tools. Set to 'auto' to let the model decide, 'required' to force tool usage, or 'none' to disable tool usage.
Links
The following links provide more information about the Perplexity LLM integration.
Python package
The
livekit-plugins-openai
package on PyPI.
Plugin reference
Reference for the
with_perplexity
method of the OpenAI LLM plugin.
GitHub repo
View the source or contribute to the LiveKit OpenAI LLM plugin.
Perplexity docs
Perplexity API documentation.
Voice AI quickstart
Get started with LiveKit Agents and Perplexity.
On this page
Overview
Usage
Parameters
Links


Content from https://docs.livekit.io/agents/integrations/llm/telnyx:

On this page
Overview
Usage
Parameters
Links
Copy page
See more page options
Overview
Telnyx
provides access to Llama 3.1 and other models through their inference API. These models are multilingual and text-only, making them suitable for a variety of agent applications.
Usage
Install the OpenAI plugin to add Telnyx support:
pip
install
"livekit-agents[openai]~=1.0"
Set the following environment variable in your
.env
file:
TELNYX_API_KEY
=
<
your-telnyx-api-key
>
Create a Telnyx LLM using the
with_telnyx
method:
from
livekit
.
plugins
import
openai
session
=
AgentSession
(
llm
=
openai
.
LLM
.
with_telnyx
(
model
=
"meta-llama/Meta-Llama-3.1-70B-Instruct"
,
temperature
=
0.7
)
,
# ... tts, stt, vad, turn_detection, etc.
)
Parameters
This section describes some of the available parameters. For a complete reference of all available parameters, see the
method reference
.
model
str | TelnyxChatModels
Optional
Default:
meta-llama/Meta-Llama-3.1-70B-Instruct
#
Model to use for inference. To learn more, see
supported models
.
temperature
float
Optional
Default:
0.1
#
Controls the randomness of the model's output. Higher values (e.g., 0.8) make the output more random, while lower values (e.g., 0.2) make it more focused and deterministic.
Valid values are between
0
and
2
.
parallel_tool_calls
bool
Optional
#
Controls whether the model can make multiple tool calls in parallel. When enabled, the model can make multiple tool calls simultaneously, which can improve performance for complex tasks.
tool_choice
ToolChoice | Literal['auto', 'required', 'none']
Optional
Default:
auto
#
Controls how the model uses tools. Set to 'auto' to let the model decide, 'required' to force tool usage, or 'none' to disable tool usage.
Links
The following links provide more information about the Telnyx LLM integration.
Python package
The
livekit-plugins-openai
package on PyPI.
Plugin reference
Reference for the
with_telnyx
method of the OpenAI LLM plugin.
GitHub repo
View the source or contribute to the LiveKit OpenAI LLM plugin.
Telnyx docs
Telnyx API documentation.
Voice AI quickstart
Get started with LiveKit Agents and Telnyx.
On this page
Overview
Usage
Parameters
Links


Content from https://docs.livekit.io/agents/integrations/llm/together:

On this page
Overview
Usage
Parameters
Links
Copy page
See more page options
Overview
Together AI
provides access to Llama 2 and Llama 3 models including instruction-tuned models through their inference API. These models are multilingual and text-only, making them suitable for a variety of agent applications.
Usage
Install the OpenAI plugin to add Together AI support:
pip
install
"livekit-agents[openai]~=1.0"
Set the following environment variable in your
.env
file:
TOGETHER_API_KEY
=
<
your-together-api-key
>
Create a Together AI LLM using the
with_together
method:
from
livekit
.
plugins
import
openai
session
=
AgentSession
(
llm
=
openai
.
LLM
.
with_together
(
model
=
"meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo"
,
temperature
=
0.7
)
,
# ... tts, stt, vad, turn_detection, etc.
)
Parameters
This section describes some of the available parameters. For a complete reference of all available parameters, see the
method reference
.
model
str | TogetherChatModels
Optional
Default:
meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo
#
Model to use for inference. To learn more, see
supported models
.
temperature
float
Optional
Default:
1.0
#
Controls the randomness of the model's output. Higher values (e.g., 0.8) make the output more random, while lower values (e.g., 0.2) make it more focused and deterministic.
Valid values are between
0
and
1
.
parallel_tool_calls
bool
Optional
#
Controls whether the model can make multiple tool calls in parallel. When enabled, the model can make multiple tool calls simultaneously, which can improve performance for complex tasks.
tool_choice
ToolChoice | Literal['auto', 'required', 'none']
Optional
Default:
auto
#
Controls how the model uses tools. Set to 'auto' to let the model decide, 'required' to force tool usage, or 'none' to disable tool usage.
Links
The following links provide more information about the Together AI LLM integration.
Python package
The
livekit-plugins-openai
package on PyPI.
Plugin reference
Reference for the
with_together
method of the OpenAI LLM plugin.
GitHub repo
View the source or contribute to the LiveKit OpenAI LLM plugin.
Together AI docs
Together AI API documentation.
Voice AI quickstart
Get started with LiveKit Agents and Together AI.
On this page
Overview
Usage
Parameters
Links


Content from https://docs.livekit.io/agents/integrations/llm/xai:

On this page
Overview
Usage
Parameters
Links
Copy page
See more page options
Overview
xAI
provides access to Grok models through their OpenAI-compatible API. These models are multilingual and support multimodal capabilities, making them suitable for a variety of agent applications.
Usage
Install the OpenAI plugin to add xAI support:
pip
install
"livekit-agents[openai]~=1.0"
Set the following environment variable in your
.env
file:
XAI_API_KEY
=
<
your-xai-api-key
>
Create a Grok LLM using the
with_x_ai
method:
from
livekit
.
plugins
import
openai
session
=
AgentSession
(
llm
=
openai
.
LLM
.
with_x_ai
(
model
=
"grok-2-public"
,
temperature
=
1.0
,
)
,
# ... tts, stt, vad, turn_detection, etc.
)
Parameters
This section describes some of the available parameters. For a complete reference of all available parameters, see the
method reference
.
model
str | XAIChatModels
Optional
Default:
grok-2-public
#
Grok model to use. To learn more, see the
xAI Grok models
page.
temperature
float
Optional
Default:
1.0
#
Controls the randomness of the model's output. Higher values (e.g., 0.8) make the output more random, while lower values (e.g., 0.2) make it more focused and deterministic.
Valid values are between
0
and
2
. To learn more, see the optional parameters for
Chat completions
parallel_tool_calls
bool
Optional
#
Controls whether the model can make multiple tool calls in parallel. When enabled, the model can make multiple tool calls simultaneously, which can improve performance for complex tasks.
tool_choice
ToolChoice | Literal['auto', 'required', 'none']
Optional
Default:
auto
#
Controls how the model uses tools. Set to 'auto' to let the model decide, 'required' to force tool usage, or 'none' to disable tool usage.
Links
The following links provide more information about the xAI Grok LLM integration.
Python package
The
livekit-plugins-openai
package on PyPI.
Plugin reference
Reference for the
with_x_ai
method of the OpenAI LLM plugin.
GitHub repo
View the source or contribute to the LiveKit OpenAI LLM plugin.
xAI docs
xAI Grok documentation.
Voice AI quickstart
Get started with LiveKit Agents and xAI Grok.
On this page
Overview
Usage
Parameters
Links


Content from https://docs.livekit.io/agents/integrations/stt/assemblyai:

On this page
Overview
Quick reference
Installation
Authentication
Usage
Parameters
Turn detection
Additional resources
Copy page
See more page options
Overview
AssemblyAI
provides a streaming STT service with high accuracy, realtime transcription. You can use the open source AssemblyAI plugin for LiveKit Agents to build voice AI with fast, accurate transcription.
Quick reference
This section provides a brief overview of the AssemblyAI STT plugin. For more information, see
Additional resources
.
Installation
Install the plugin from PyPI:
pip
install
"livekit-agents[assemblyai]~=1.0"
Authentication
The AssemblyAI plugin requires an
AssemblyAI API key
.
Set
ASSEMBLYAI_API_KEY
in your
.env
file.
Usage
Use AssemblyAI STT in an
AgentSession
or as a standalone transcription service. For example, you can use this STT in the
Voice AI quickstart
.
from
livekit
.
plugins
import
assemblyai
session
=
AgentSession
(
stt
=
assemblyai
.
STT
(
)
,
# ... vad, llm, tts, etc.
)
Parameters
This section describes some of the available parameters. See the
plugin reference
for a complete list of all available parameters.
format_turns
bool
Optional
Default:
True
#
Whether to return formatted final transcripts. If enabled, formatted final transcripts are emitted shortly following an end-of-turn detection.
end_of_turn_confidence_threshold
float
Optional
Default:
0.7
#
The confidence threshold to use when determining if the end of a turn has been reached.
min_end_of_turn_silence_when_confident
int
Optional
Default:
160
#
The minimum duration of silence required to detect end of turn when confident.
max_turn_silence
int
Optional
Default:
2400
#
The maximum duration of silence allowed in a turn before end of turn is triggered.
Turn detection
AssemblyAI includes a custom phrase endpointing model that uses both audio and linguistic information to detect turn boundaries. To use this model for
turn detection
, set
turn_detection="stt"
in the
AgentSession
constructor. You should also provide a VAD plugin for responsive interruption handling.
session
=
AgentSession
(
turn_detection
=
"stt"
,
stt
=
assemblyai
.
STT
(
end_of_turn_confidence_threshold
=
0.7
,
min_end_of_turn_silence_when_confident
=
160
,
max_turn_silence
=
2400
,
)
,
vad
=
silero
.
VAD
.
load
(
)
,
# Recommended for responsive interruption handling
# ... llm, tts, etc.
)
Additional resources
The following resources provide more information about using AssemblyAI with LiveKit Agents.
Python package
The
livekit-plugins-assemblyai
package on PyPI.
Plugin reference
Reference for the AssemblyAI STT plugin.
GitHub repo
View the source or contribute to the LiveKit AssemblyAI STT plugin.
AssemblyAI docs
AssemblyAI's full docs for the Universal Streaming API.
Voice AI quickstart
Get started with LiveKit Agents and AssemblyAI.
AssemblyAI LiveKit guide
Guide to using AssemblyAI Universal Streaming STT with LiveKit.
On this page
Overview
Quick reference
Installation
Authentication
Usage
Parameters
Turn detection
Additional resources


Content from https://docs.livekit.io/agents/integrations/stt/baseten:

On this page
Overview
Quick reference
Installation
Authentication
Model deployment
Usage
Parameters
Additional resources
Copy page
See more page options
Overview
Baseten
is a hosted inference platform that allows you to deploy and serve any machine learning model. With LiveKit's Baseten integration and the Agents framework, you can build AI agents that provide high-accuracy transcriptions using models like Whisper.
Quick reference
This section provides a quick reference for the Baseten STT plugin. For more information, see
Additional resources
.
Installation
Install the plugin from PyPI:
pip
install
"livekit-agents[baseten]~=1.0"
Authentication
The Baseten plugin requires a
Baseten API key
.
Set the following in your
.env
file:
BASETEN_API_KEY
=
<
your-baseten-api-key
>
Model deployment
You must deploy a websocket-based STT model to Baseten to use it with LiveKit Agents. The standard Whisper deployments available in the Baseten library are not suitable for realtime use. Contact Baseten support for help deploying a websocket-compatible Whisper model.
Your model endpoint may show as an HTTP URL such as
https://model-<id>.api.baseten.co/environments/production/predict
. The domain is correct but you must change the protocol to
wss
and the path to
/v1/websocket
to use it as the
model_endpoint
parameter for the Baseten STT plugin.
The correct websocket URL format is:
wss
:
/
/
<
your-model-id
>
.api.baseten.co/v1/websocket
Usage
Use Baseten STT within an
AgentSession
or as a standalone transcription service. For example,
you can use this STT in the
Voice AI quickstart
.
from
livekit
.
plugins
import
baseten
session
=
AgentSession
(
stt
=
baseten
.
STT
(
model_endpoint
=
"wss://<your-model-id>.api.baseten.co/v1/websocket"
,
)
# ... llm, tts, etc.
)
Parameters
This section describes some of the available parameters. See the
plugin reference
for a complete list of all available parameters.
model_endpoint
string
Optional
Env:
BASETEN_MODEL_ENDPOINT
#
The endpoint URL for your deployed model. You can find this in your Baseten dashboard. Note that this must be a websocket URL (starts with
wss://
). See
Model deployment
for more details.
language
string
Optional
Default:
en
#
Language of input audio in
ISO-639-1
format.
vad_threshold
float
Optional
Default:
0.5
#
Threshold for voice activity detection.
vad_min_silence_duration_ms
int
Optional
Default:
300
#
Minimum duration of silence in milliseconds to consider speech ended.
vad_speech_pad_ms
int
Optional
Default:
30
#
Duration in milliseconds to pad speech segments.
Additional resources
The following resources provide more information about using Baseten with LiveKit Agents.
Python package
The
livekit-plugins-baseten
package on PyPI.
Plugin reference
Reference for the Baseten STT plugin.
GitHub repo
View the source or contribute to the LiveKit Baseten STT plugin.
Baseten docs
Baseten's full docs site.
Voice AI quickstart
Get started with LiveKit Agents and Baseten.
Baseten TTS
Guide to the Baseten TTS integration with LiveKit Agents.
On this page
Overview
Quick reference
Installation
Authentication
Model deployment
Usage
Parameters
Additional resources


Content from https://docs.livekit.io/agents/integrations/stt/cartesia:

On this page
Overview
Quick reference
Installation
Authentication
Usage
Parameters
Additional resources
Copy page
See more page options
Overview
Cartesia
provides advanced speech recognition technology with their Ink-Whisper model, optimized for real-time transcription in conversational settings. With LiveKit's Cartesia integration and the Agents framework, you can build AI agents that provide high-accuracy transcriptions with ultra-low latency.
Quick reference
This section includes a basic usage example and some reference material. For links to more detailed documentation, see
Additional resources
.
Installation
Install the plugin from PyPI:
pip
install
"livekit-agents[cartesia]~=1.0"
Authentication
The Cartesia plugin requires a
Cartesia API key
.
Set
CARTESIA_API_KEY
in your
.env
file.
Usage
Use Cartesia STT in an
AgentSession
or as a standalone transcription service. For example, you can use this STT in the
Voice AI quickstart
.
from
livekit
.
plugins
import
cartesia
session
=
AgentSession
(
stt
=
cartesia
.
STT
(
model
=
"ink-whisper"
)
,
# ... llm, tts, etc.
)
Parameters
This section describes some of the available parameters. See the
plugin reference
for a complete list of all available parameters.
model
string
Optional
Default:
ink-whisper
#
Selected model to use for STT. See
Cartesia STT models
for supported values.
language
string
Optional
Default:
en
#
Language of input audio in
ISO-639-1
format. See
Cartesia STT models
for supported values.
Additional resources
The following resources provide more information about using Cartesia with LiveKit Agents.
Python package
The
livekit-plugins-cartesia
package on PyPI.
Plugin reference
Reference for the Cartesia STT plugin.
GitHub repo
View the source or contribute to the LiveKit Cartesia STT plugin.
Cartesia docs
Cartesia STT docs.
Voice AI quickstart
Get started with LiveKit Agents and Cartesia STT.
Cartesia TTS
Guide to the Cartesia TTS integration with LiveKit Agents.
On this page
Overview
Quick reference
Installation
Authentication
Usage
Parameters
Additional resources


Content from https://docs.livekit.io/agents/integrations/stt/clova:

On this page
Overview
Quick reference
Installation
Authentication
Usage
Parameters
Additional resources
Copy page
See more page options
Overview
CLOVA Speech Recognition
is the NAVER Cloud Platform's service to convert human voice into text. You can use the open source CLOVA plugin for LiveKit Agents to build voice AI with fast, accurate transcription.
Quick reference
This section provides a brief overview of the CLOVA STT plugin. For more information, see
Additional resources
.
Installation
Install the plugin from PyPI:
pip
install
"livekit-agents[clova]~=1.0"
Authentication
The CLOVA plugin requires the following keys, which may set as environment variables or passed to the constructor.
CLOVA_STT_SECRET_KEY
=
<
your-api-key
>
CLOVA_STT_INVOKE_URL
=
<
your-invoke-url
>
Usage
Create a CLOVA STT to use within an
AgentSession
or as a standalone transcription service.
For example, you can use this STT in the
Voice AI quickstart
.
from
livekit
.
plugins
import
clova
session
=
AgentSession
(
stt
=
clova
.
STT
(
word_boost
=
[
"LiveKit"
]
,
)
,
# ... llm, tts, etc.
)
Parameters
This section describes some of the available parameters. See the
plugin reference
for a complete list of all available parameters.
language
ClovaSttLanguages
Optional
Default:
en-US
#
Speech recognition language. Clova supports English, Korean, Japanese, and Chinese. Valid values are
ko-KR
,
en-US
,
enko
,
ja
,
zh-cn
,
zh-tw
.
Additional resources
The following resources provide more information about using CLOVA with LiveKit Agents.
Python package
The
livekit-plugins-clova
package on PyPI.
Plugin reference
Reference for the CLOVA STT plugin.
GitHub repo
View the source or contribute to the LiveKit CLOVA STT plugin.
CLOVA docs
CLOVA's full docs site.
Voice AI quickstart
Get started with LiveKit Agents and CLOVA.
On this page
Overview
Quick reference
Installation
Authentication
Usage
Parameters
Additional resources


Content from https://docs.livekit.io/agents/integrations/stt/deepgram:

On this page
Overview
Quick reference
Installation
Authentication
Usage
Parameters
Additional resources
Copy page
See more page options
Overview
Deepgram
provides advanced speech recognition technology and AI-driven audio processing solutions. Customizable speech models allow you to fine tune transcription performance for your specific use case. With LiveKit's Deepgram integration and the Agents framework, you can build AI agents that provide high-accuracy transcriptions.
Quick reference
This section includes a basic usage example and some reference material. For links to more detailed documentation, see
Additional resources
.
Installation
Install the plugin from PyPI:
pip
install
"livekit-agents[deepgram]~=1.0"
Authentication
The Deepgram plugin requires a
Deepgram API key
.
Set
DEEPGRAM_API_KEY
in your
.env
file.
Usage
Use Deepgram STT in an
AgentSession
or as a standalone transcription service. For example, you can use this STT in the
Voice AI quickstart
.
from
livekit
.
plugins
import
deepgram
session
=
AgentSession
(
stt
=
deepgram
.
STT
(
model
=
"nova-3"
,
)
,
# ... llm, tts, etc.
)
Parameters
This section describes the available parameters. See the
plugin reference
for more details.
model
string
Optional
Default:
nova-3
#
The Deepgram model to use for speech recognition.
language
string
Optional
Default:
en-US
#
The language code for recognition.
endpointing_ms
int
Optional
Default:
25
#
Time in milliseconds of silence to consider end of speech. Set to 0 to disable.
keyterms
list[string]
Optional
Default:
[]
#
List of key terms to improve recognition accuracy. Supported by Nova-3 models.
Additional resources
The following resources provide more information about using Deepgram with LiveKit Agents.
Python package
The
livekit-plugins-deepgram
package on PyPI.
Plugin reference
Reference for the Deepgram STT plugin.
GitHub repo
View the source or contribute to the LiveKit Deepgram STT plugin.
Deepgram docs
Deepgram's full docs site.
Voice AI quickstart
Get started with LiveKit Agents and Deepgram.
Deepgram TTS
Guide to the Deepgram TTS integration with LiveKit Agents.
On this page
Overview
Quick reference
Installation
Authentication
Usage
Parameters
Additional resources


Content from https://docs.livekit.io/agents/integrations/stt/fal:

On this page
Overview
Quick reference
Installation
Authentication
Usage
Parameters
Additional resources
Copy page
See more page options
Overview
fal
provides a hosted inference platform for a wide variety of model types, including
Wizper
, a speech-to-text model based on Whisper v3 Large. With LiveKit's fal integration and the Agents framework, you can build AI agents that integrate Wizper for fast and accurate transcription.
Quick reference
This section includes a basic usage example and some reference material. For links to more detailed documentation, see
Additional resources
.
Installation
Install the plugin from PyPI:
pip
install
"livekit-agents[fal]~=1.0"
Authentication
The fal plugin requires a
fal API key
.
Set
FAL_KEY
in your
.env
file.
Usage
Use fal STT in an
AgentSession
or as a standalone transcription service. For example, you can use this STT in the
Voice AI quickstart
.
from
livekit
.
plugins
import
fal
session
=
AgentSession
(
stt
=
fal
.
STT
(
language
=
"de"
,
)
,
# ... llm, tts, etc.
)
Parameters
This section describes some of the available parameters. See the
plugin reference
for a complete list of all available parameters.
language
str
Optional
Default:
en
#
Speech recognition language.
Additional resources
The following resources provide more information about using fal with LiveKit Agents.
Python package
The
livekit-plugins-fal
package on PyPI.
Plugin reference
Reference for the fal STT plugin.
GitHub repo
View the source or contribute to the LiveKit fal STT plugin.
fal docs
fal's full docs site.
Voice AI quickstart
Get started with LiveKit Agents and fal.
On this page
Overview
Quick reference
Installation
Authentication
Usage
Parameters
Additional resources


Content from https://docs.livekit.io/agents/integrations/stt/gladia:

On this page
Overview
Quick reference
Installation
Authentication
Initialization
Realtime translation
Updating options
Parameters
Additional resources
Copy page
See more page options
Overview
Gladia
provides accurate speech recognition optimized for enterprise use cases. You can use the open source Gladia integration for LiveKit Agents to build voice AI with fast, accurate transcription and optional translation features.
Quick reference
This section provides a brief overview of the Gladia STT plugin. For more information, see
Additional resources
.
Installation
Install the plugin from PyPI:
pip
install
"livekit-agents[gladia]~=1.0"
Authentication
The Gladia plugin requires a
Gladia API key
.
Set
GLADIA_API_KEY
in your
.env
file.
Initialization
Use Gladia STT in an
AgentSession
or as a standalone transcription service. For example, you can use this STT in the
Voice AI quickstart
.
from
livekit
.
plugins
import
gladia
session
=
AgentSession
(
stt
=
gladia
.
STT
(
)
,
# ... llm, tts, etc.
)
Realtime translation
To use realtime translation, set
translation_enabled
to
True
and specify the expected audio languages in
languages
and the desired target language in
translation_target_languages
.
For example, to transcribe and translate a mixed English and French audio stream into English, set the following options:
gladia
.
STT
(
translation_enabled
=
True
,
languages
=
[
"en"
,
"fr"
]
,
translation_target_languages
=
[
"en"
]
)
Note that if you specify more than one target language, the plugin emits a separate transcription event for each. When used in an
AgentSession
, this adds each transcription to the conversation history, in order, which might confuse the LLM.
Updating options
Use the
update_options
method to configure the STT on the fly:
gladia_stt
=
gladia
.
STT
(
)
gladia_stt
.
update_options
(
languages
=
[
"ja"
,
"en"
]
,
translation_enabled
=
True
,
translation_target_languages
=
[
"fr"
]
)
Parameters
This section describes some of the available parameters. See the
plugin reference
for a complete list of all available parameters.
languages
list[string]
Optional
Default:
[]
#
List of languages to use for transcription. If empty, Gladia will auto-detect the language.
code_switching
bool
Optional
Default:
false
#
Enable switching between languages during recognition.
translation_enabled
bool
Optional
Default:
false
#
Enable real-time translation.
translation_target_languages
list[string]
Optional
Default:
[]
#
List of target languages for translation.
Additional resources
The following resources provide more information about using Gladia with LiveKit Agents.
Python package
The
livekit-plugins-gladia
package on PyPI.
Plugin reference
Reference for the Gladia STT plugin.
GitHub repo
View the source or contribute to the LiveKit Gladia STT plugin.
Gladia documentation
Gladia's full docs site.
Voice AI quickstart
Get started with LiveKit Agents and Gladia.
On this page
Overview
Quick reference
Installation
Authentication
Initialization
Realtime translation
Updating options
Parameters
Additional resources


Content from https://docs.livekit.io/agents/integrations/stt/sarvam:

On this page
Overview
Quick reference
Installation
Authentication
Usage
Parameters
Additional resources
Copy page
See more page options
Overview
Sarvam
provides advanced speech recognition technology optimized for Indian languages. With LiveKit's Sarvam integration and the Agents framework, you can build AI agents that provide high-accuracy transcriptions for Indian languages and accents.
Quick reference
This section provides a quick reference for the Sarvam STT plugin. For more information, see
Additional resources
.
Installation
Install the plugin from PyPI:
pip
install
"livekit-agents[sarvam]~=1.0"
Authentication
The Sarvam plugin requires a
Sarvam API key
.
Set
SARVAM_API_KEY
in your
.env
file.
Usage
Use Sarvam STT in an
AgentSession
or as a standalone transcription service. For example, you can use this STT in the
Voice AI quickstart
.
from
livekit
.
plugins
import
sarvam
session
=
AgentSession
(
stt
=
sarvam
.
STT
(
language
=
"hi-IN"
,
model
=
"saarika:v2.5"
,
)
,
# ... llm, tts, etc.
)
Parameters
This section describes some of the available parameters. See the
plugin reference
for a complete list of all available parameters.
language
string
Optional
#
BCP-47 language code for supported Indian languages. See
documentation
for a complete list of supported languages.
model
string
Optional
Default:
saarika:v2.5
#
The Sarvam STT model to use. See
documentation
for a complete list of supported models.
Additional resources
The following resources provide more information about using Sarvam with LiveKit Agents.
Python package
The
livekit-plugins-sarvam
package on PyPI.
Plugin reference
Reference for the Sarvam STT plugin.
GitHub repo
View the source or contribute to the LiveKit Sarvam STT plugin.
Sarvam docs
Sarvam's full docs site.
Voice AI quickstart
Get started with LiveKit Agents and Sarvam.
Sarvam TTS
Guide to the Sarvam TTS integration with LiveKit Agents.
On this page
Overview
Quick reference
Installation
Authentication
Usage
Parameters
Additional resources


Content from https://docs.livekit.io/agents/integrations/stt/speechmatics:

On this page
Overview
Quick reference
Installation
Authentication
Usage
Speaker diarization
Parameters
Additional resources
Copy page
See more page options
Overview
Speechmatics
provides enterprise-grade speech-to-text APIs. Their advanced speech models deliver highly accurate transcriptions across diverse languages, dialects, and accents. You can use the LiveKit Speechmatics plugin with the Agents framework to build voice AI agents that provide reliable, real-time transcriptions.
Quick reference
This section includes a basic usage example and some reference material. For links to more detailed documentation, see
Additional resources
.
Installation
Install the plugin from PyPI:
pip
install
"livekit-agents[speechmatics]~=1.0"
Authentication
The Speechmatics plugin requires an
API key
.
Set
SPEECHMATICS_API_KEY
in your
.env
file.
Usage
Use Speechmatics STT in an
AgentSession
or as a standalone transcription service. For example, you can use this STT in the
Voice AI quickstart
.
from
livekit
.
plugins
import
speechmatics
session
=
AgentSession
(
stt
=
speechmatics
.
STT
(
)
,
# ... llm, tts, etc.
)
Speaker diarization
Enable
speaker diarization
by initializing the STT with
diarization="speaker"
and a
speaker_diarization_config
. You need to override the entire
transcription_config
so set the other values as needed.
stt
=
speechmatics
.
STT
(
transcription_config
=
speechmatics
.
types
.
TranscriptionConfig
(
language
=
"en"
,
operating_point
=
"enhanced"
,
enable_partials
=
True
,
max_delay
=
0.7
,
diarization
=
"speaker"
,
speaker_diarization_config
=
{
"max_speakers"
:
2
}
,
# Adjust as needed
)
)
Results are available as the
speaker_id
property on the events emitted by
user_input_transcribed
:
from
livekit
.
agents
import
UserInputTranscribedEvent
@session
.
on
(
"user_input_transcribed"
)
def
on_user_input_transcribed
(
event
:
UserInputTranscribedEvent
)
:
print
(
f"user_input_transcribed: \"[
{
event
.
speaker_id
}
]:
{
event
.
transcript
}
\""
)
Parameters
This section describes the key parameters for the Speechmatics STT plugin. See the
plugin reference
for a complete list of all available parameters.
transcription_config
TranscriptionConfig
Optional
#
Configuration for the transcription service. If you override this parameter, you must provide all configuration values. The following parameters are available:
language
string
Optional
Default:
en
#
ISO 639-1 language code. All languages are global and can understand different dialects/accents. To see the list of all supported languages, see
Supported Languages
.
operating_point
string
Optional
Default:
enhanced
#
Operating point to use for the transcription per required accuracy & complexity. To learn more, see
Accuracy Reference
.
enable_partials
bool
Optional
Default:
true
#
Partial transcripts allow you to receive preliminary transcriptions and update as more context is available until the higher-accuracy
final transcript
is returned. Partials are returned faster but without any post-processing such as formatting.
max_delay
number
Optional
Default:
0.7
#
The delay in seconds between the end of a spoken word and returning the final transcript results.
speaker_diarization_config
dict
Optional
#
Configuration for speaker diarization. The following parameters are available:
max_speakers
int
Optional
Default:
2
#
Maximum number of speakers to detect in the audio. Valid values range from 2 to 100.
speaker_sensitivity
float
Optional
Default:
0.5
#
Sensitivity of speaker detection between 0 and 1. Higher values increase likelihood of detecting more unique speakers.
prefer_current_speaker
bool
Optional
Default:
false
#
When true, reduces likelihood of switching between similar sounding speakers by preferring the current speaker.
Additional resources
The following resources provide more information about using Speechmatics with LiveKit Agents.
Python package
The
livekit-plugins-speechmatics
package on PyPI.
Plugin reference
Reference for the Speechmatics STT plugin.
GitHub repo
View the source or contribute to the LiveKit Speechmatics STT plugin.
Speechmatics docs
Speechmatics STT docs.
Voice AI quickstart
Get started with LiveKit Agents and Speechmatics STT.
On this page
Overview
Quick reference
Installation
Authentication
Usage
Speaker diarization
Parameters
Additional resources


Content from https://docs.livekit.io/agents/integrations/stt/spitch:

On this page
Overview
Quick reference
Installation
Authentication
Usage
Parameters
Additional resources
Copy page
See more page options
Overview
Spitch
provides AI-powered speech and language solutions optimized for African languages. With LiveKit's Spitch STT integration and the Agents framework, you can build voice AI agents that understand speech in a variety of African languages.
Quick reference
This section provides a quick reference for the Spitch STT plugin. For more information, see
Additional resources
.
Installation
Install the plugin from PyPI:
pip
install
"livekit-agents[spitch]~=1.0"
Authentication
The Spitch plugin requires a
Spitch API key
.
Set
SPITCH_API_KEY
in your
.env
file.
Usage
Use Spitch STT in an
AgentSession
or as a standalone transcription service. For example, you can use this STT in the
Voice AI quickstart
.
from
livekit
.
plugins
import
spitch
session
=
AgentSession
(
stt
=
spitch
.
STT
(
language
=
"en"
,
)
,
# ... llm, tts, etc.
)
Parameters
This section describes some of the available parameters. See the
plugin reference
for a complete list of all available parameters.
language
string
Optional
Default:
en
#
Language short code of the input speech. For supported values, see
Spitch languages
.
Additional resources
The following resources provide more information about using Spitch with LiveKit Agents.
Python package
The
livekit-plugins-spitch
package on PyPI.
Plugin reference
Reference for the Spitch STT plugin.
GitHub repo
View the source or contribute to the LiveKit Spitch STT plugin.
Spitch docs
Spitch's official documentation.
Voice AI quickstart
Get started with LiveKit Agents and Spitch.
Spitch TTS
Guide to the Spitch TTS integration with LiveKit Agents.
On this page
Overview
Quick reference
Installation
Authentication
Usage
Parameters
Additional resources


Content from https://docs.livekit.io/agents/integrations/tts/baseten:

On this page
Overview
Quick reference
Installation
Authentication
Model deployment
Usage
Parameters
Additional resources
Copy page
See more page options
Overview
Baseten
is a hosted inference platform that allows you to deploy and serve any machine learning model. With LiveKit's Baseten integration and the Agents framework, you can build AI agents that provide high-quality speech synthesis using models like Orpheus.
Quick reference
This section provides a quick reference for the Baseten TTS plugin. For more information, see
Additional resources
.
Installation
Install the plugin from PyPI:
pip
install
"livekit-agents[baseten]~=1.0"
Authentication
The Baseten plugin requires a
Baseten API key
.
Set the following in your
.env
file:
BASETEN_API_KEY
=
<
your-baseten-api-key
>
Model deployment
You must deploy a TTS model such as
Orpheus
to Baseten to use it with LiveKit Agents. Your deployment includes a private model endpoint URL to provide to the LiveKit Agents integration.
Usage
Use Baseten TTS within an
AgentSession
or as a standalone speech generator. For example,
you can use this TTS in the
Voice AI quickstart
.
from
livekit
.
plugins
import
baseten
session
=
AgentSession
(
tts
=
baseten
.
TTS
(
model_endpoint
=
"<your-model-endpoint>"
,
voice
=
"tara"
,
)
# ... llm, stt, etc.
)
Parameters
This section describes some of the available parameters. See the
plugin reference
for a complete list of all available parameters.
model_endpoint
string
Optional
Env:
BASETEN_MODEL_ENDPOINT
#
The endpoint URL for your deployed model. You can find this in your Baseten dashboard.
voice
string
Optional
Default:
tara
#
The voice to use for speech synthesis.
language
string
Optional
Default:
en
#
Language of output audio in
ISO-639-1
format.
temperature
float
Optional
Default:
0.6
#
Controls the randomness of the generated speech. Higher values make the output more random.
Additional resources
The following resources provide more information about using Baseten with LiveKit Agents.
Python package
The
livekit-plugins-baseten
package on PyPI.
Plugin reference
Reference for the Baseten TTS plugin.
GitHub repo
View the source or contribute to the LiveKit Baseten TTS plugin.
Baseten docs
Baseten's full docs site.
Voice AI quickstart
Get started with LiveKit Agents and Baseten.
Baseten STT
Guide to the Baseten STT integration with LiveKit Agents.
On this page
Overview
Quick reference
Installation
Authentication
Model deployment
Usage
Parameters
Additional resources


Content from https://docs.livekit.io/agents/integrations/tts/cartesia:

On this page
Overview
Quick reference
Installation
Authentication
Usage
Parameters
Customizing pronunciation
Transcription timing
Additional resources
Copy page
See more page options
Try the playground
Chat with a voice assistant built with LiveKit and Cartesia TTS
Overview
Cartesia
provides customizable speech synthesis across a number of different languages and
produces natural-sounding speech with low latency. You can use the Cartesia TTS plugin for LiveKit Agents to build voice AI applications that sound realistic.
Quick reference
This section includes a brief overview of the Cartesia TTS plugin. For more information, see
Additional resources
.
Installation
Install the plugin from PyPI:
pip
install
"livekit-agents[cartesia]~=1.0"
Authentication
The Cartesia plugin requires a
Cartesia API key
.
Set
CARTESIA_API_KEY
in your
.env
file.
Usage
Use Cartesia TTS within an
AgentSession
or as a standalone speech generator. For example,
you can use this TTS in the
Voice AI quickstart
.
from
livekit
.
plugins
import
cartesia
session
=
AgentSession
(
tts
=
cartesia
.
TTS
(
model
=
"sonic-2"
,
voice
=
"f786b574-daa5-4673-aa0c-cbe3e8534c02"
,
)
# ... llm, stt, etc.
)
Parameters
This section describes some of the available parameters. See the
plugin reference
for a complete list of all available parameters.
model
string
Optional
Default:
sonic-2
#
ID of the model to use for generation. See
supported models
.
voice
string | list[float]
Optional
Default:
794f9389-aac1-45b6-b726-9d9369183238
#
ID of the voice to use for generation, or an embedding array. See
official documentation
.
language
string
Optional
Default:
en
#
Language of input text in
ISO-639-1
format. For a list of languages support by model, see
supported models
.
Customizing pronunciation
Cartesia TTS allows you to customize pronunciation using Speech Synthesis Markup Language (SSML). To learn more,
see
Specify Custom Pronunciations
.
Transcription timing
Cartesia TTS supports aligned transcription forwarding, which improves transcription synchronization in your frontend. Set
use_tts_aligned_transcript=True
in your
AgentSession
configuration to enable this feature. To learn more, see
the docs
.
Additional resources
The following resources provide more information about using Cartesia with LiveKit Agents.
Python package
The
livekit-plugins-cartesia
package on PyPI.
Plugin reference
Reference for the Cartesia TTS plugin.
GitHub repo
View the source or contribute to the LiveKit Cartesia TTS plugin.
Cartesia docs
Cartesia TTS docs.
Voice AI quickstart
Get started with LiveKit Agents and Cartesia TTS.
Cartesia STT
Guide to the Cartesia STT integration with LiveKit Agents.
On this page
Overview
Quick reference
Installation
Authentication
Usage
Parameters
Customizing pronunciation
Transcription timing
Additional resources


Content from https://docs.livekit.io/agents/integrations/tts/deepgram:

On this page
Overview
Quick reference
Installation
Authentication
Usage
Parameters
Prompting
Additional resources
Copy page
See more page options
Overview
Deepgram
provides responsive, human-like text-to-speech technology for voice AI. With LiveKit's Deepgram integration and the Agents framework, you can build voice AI agents that sound realistic.
Quick reference
This section provides a quick reference for the Deepgram TTS plugin. For more information, see
Additional resources
.
Installation
Install the plugin from PyPI:
pip
install
"livekit-agents[deepgram]~=1.0"
Authentication
The Deepgram plugin requires a
Deepgram API key
.
Set
DEEPGRAM_API_KEY
in your
.env
file.
Usage
Use Deepgram TTS within an
AgentSession
or as a standalone speech generator. For example,
you can use this TTS in the
Voice AI quickstart
.
from
livekit
.
plugins
import
deepgram
session
=
AgentSession
(
tts
=
deepgram
.
TTS
(
model
=
"aura-asteria-en"
,
)
# ... llm, stt, etc.
)
Parameters
This section describes some of the available parameters. See the
plugin reference
for a complete list of all available parameters.
model
string
Optional
Default:
aura-asteria-en
#
ID of the model to use for generation. To learn more, see
supported models
.
Prompting
Deepgram supports filler words and natural pauses through prompting. To learn more, see
Text to Speech Prompting
.
Additional resources
The following resources provide more information about using Deepgram with LiveKit Agents.
Python package
The
livekit-plugins-deepgram
package on PyPI.
Plugin reference
Reference for the Deepgram TTS plugin.
GitHub repo
View the source or contribute to the LiveKit Deepgram TTS plugin.
Deepgram docs
Deepgram's full docs site.
Voice AI quickstart
Get started with LiveKit Agents and Deepgram.
Deepgram STT
Guide to the Deepgram STT integration with LiveKit Agents.
On this page
Overview
Quick reference
Installation
Authentication
Usage
Parameters
Prompting
Additional resources


Content from https://docs.livekit.io/agents/integrations/tts/elevenlabs:

On this page
Overview
Quick reference
Installation
Authentication
Usage
Parameters
Customizing pronunciation
Transcription timing
Additional resources
Copy page
See more page options
Overview
ElevenLabs
provides an AI text-to-speech (TTS) service with thousands of human-like voices
across a number of different languages. With LiveKit's ElevenLabs integration and the Agents framework, you can build
voice AI applications that sound realistic.
Quick reference
This section provides a quick reference for the ElevenLabs TTS plugin. For more information, see
Additional resources
.
Installation
Install the plugin from PyPI:
pip
install
"livekit-agents[elevenlabs]~=1.0"
Authentication
The ElevenLabs plugin requires an
ElevenLabs API key
.
Set
ELEVEN_API_KEY
in your
.env
file.
Usage
Use ElevenLabs TTS within an
AgentSession
or as a standalone speech generator. For example,
you can use this TTS in the
Voice AI quickstart
.
from
livekit
.
plugins
import
elevenlabs
session
=
AgentSession
(
tts
=
elevenlabs
.
TTS
(
voice_id
=
"ODq5zmih8GrVes37Dizd"
,
model
=
"eleven_multilingual_v2"
)
# ... llm, stt, etc.
)
Parameters
This section describes some of the parameters you can set when you create an ElevenLabs TTS. See the
plugin reference
for a complete list of all available parameters.
model
string
Optional
Default:
eleven_flash_v2_5
#
ID of the model to use for generation. To learn more, see the
ElevenLabs documentation
.
voice_id
string
Optional
Default:
EXAVITQu4vr4xnSDxMaL
#
ID of the voice to use for generation. To learn more, see the
ElevenLabs documentation
.
voice_settings
VoiceSettings
Optional
#
Voice configuration. To learn more, see the
ElevenLabs documentation
.
stability
float
Optional
#
similarity_boost
float
Optional
#
style
float
Optional
#
use_speaker_boost
bool
Optional
#
speed
float
Optional
#
language
string
Optional
Default:
en
#
Language of output audio in
ISO-639-1
format. To learn more,
see the
ElevenLabs documentation
.
streaming_latency
int
Optional
Default:
3
#
Latency in seconds for streaming.
enable_ssml_parsing
bool
Optional
Default:
false
#
Enable Speech Synthesis Markup Language (SSML) parsing for input text. Set to
true
to
customize pronunciation
using SSML.
chunk_length_schedule
list[int]
Optional
Default:
[80, 120, 200, 260]
#
Schedule for chunk lengths. Valid values range from
50
to
500
.
Customizing pronunciation
ElevenLabs supports custom pronunciation for specific words or phrases with SSML
phoneme
tags. This is useful to ensure correct pronunciation of certain words, even when missing from the voice's lexicon. To learn more, see
Pronunciation
.
Transcription timing
ElevenLabs TTS supports aligned transcription forwarding, which improves transcription synchronization in your frontend. Set
use_tts_aligned_transcript=True
in your
AgentSession
configuration to enable this feature. To learn more, see
the docs
.
Additional resources
The following resources provide more information about using ElevenLabs with LiveKit Agents.
Python package
The
livekit-plugins-elevenlabs
package on PyPI.
Plugin reference
Reference for the ElevenLabs TTS plugin.
GitHub repo
View the source or contribute to the LiveKit ElevenLabs TTS plugin.
ElevenLabs docs
ElevenLabs TTS docs.
Voice AI quickstart
Get started with LiveKit Agents and ElevenLabs TTS.
On this page
Overview
Quick reference
Installation
Authentication
Usage
Parameters
Customizing pronunciation
Transcription timing
Additional resources


Content from https://docs.livekit.io/agents/integrations/tts/hume:

On this page
Overview
Quick reference
Installation
Authentication
Usage
Parameters
Updating utterance options
Additional resources
Copy page
See more page options
Overview
Hume
provides a text-to-speech service that understands emotional expressions. You can use the Hume TTS plugin for LiveKit Agents to create lifelike and emotional voice AI apps.
Quick reference
This section includes a brief overview of the Hume TTS plugin. For more information, see
Additional resources
.
Installation
Install the plugin from PyPI:
pip
install
"livekit-agents[hume]~=1.0"
Authentication
The Hume plugin requires a
Hume API key
.
Set
HUME_API_KEY
in your
.env
file.
Usage
Use Hume TTS within an
AgentSession
or as a standalone speech generator. For example, you can use this TTS in the
Voice AI quickstart
.
from
livekit
.
plugins
import
hume
session
=
AgentSession
(
tts
=
hume
.
TTS
(
voice
=
hume
.
VoiceByName
(
name
=
"Colton Rivers"
,
provider
=
hume
.
VoiceProvider
.
hume
)
,
description
=
"The voice exudes calm, serene, and peaceful qualities, like a gentle stream flowing through a quiet forest."
,
)
# ... llm, stt, etc.
)
Parameters
This section describes some of the available parameters. See the
plugin reference
for a complete list of all available parameters.
voice
VoiceByName | VoiceById
Optional
#
The voice, specified by name or id, to be used. When no voice is specified, a novel voice will be
generated based on the text and optionally provided description
.
description
string
Optional
#
Natural language instructions describing how the synthesized speech should sound, including but not limited to tone, intonation, pacing, and accent. If a Voice is specified in the request, this description serves as
acting
instructions. If no Voice is specified, a new voice is generated
based on this description
.
speed
float
Optional
Default:
1.0
#
Adjusts the relative speaking rate on a non-linear scale from 0.25 (much slower) to 3.0 (much faster), where 1.0 represents normal speaking pace.
instant_mode
bool
Optional
Default:
true
#
Enables ultra-low latency streaming, reducing time to first chunk. Recommended for real-time applications. Only for streaming endpoints. With this enabled, requests incur 10% higher cost.
Instant mode is automatically disabled when a voice is specified in the request.
Updating utterance options
To change the values during the session, use the
update_options
method. It accepts the same parameters as the TTS constructor. The new values take effect on the next utterance:
session
.
tts
.
update_options
(
voice
=
hume
.
VoiceByName
(
name
=
"Colton Rivers"
,
provider
=
hume
.
VoiceProvider
.
hume
)
,
description
=
"The voice exudes calm, serene, and peaceful qualities, like a gentle stream flowing through a quiet forest."
,
speed
=
2
,
)
Additional resources
The following resources provide more information about using Hume with LiveKit Agents.
Python package
The
livekit-plugins-hume
package on PyPI.
Plugin reference
Reference for the Hume TTS plugin.
Hume docs
Hume docs.
Voice AI quickstart
Get started with LiveKit Agents and Hume TTS.
On this page
Overview
Quick reference
Installation
Authentication
Usage
Parameters
Updating utterance options
Additional resources


Content from https://docs.livekit.io/agents/integrations/tts/inworld:

On this page
Overview
Quick reference
Installation
Authentication
Usage
Parameters
Additional resources
Copy page
See more page options
Overview
Inworld
develops AI products for builders of consumer applications, including an accessible, high quality text-to-speech service for developers. With LiveKit's Inworld TTS integration and the Agents framework, you can build engaging and immersive voice AI experiences.
Quick reference
This section includes a brief overview of the Inworld TTS plugin. For more information, see
Additional resources
.
Installation
Install the plugin from PyPI:
pip
install
"livekit-agents[inworld]~=1.0"
Authentication
The Inworld plugin requires Base64
Inworld API key
.
Set
INWORLD_API_KEY
in your
.env
file.
Usage
Use Inworld TTS within an
AgentSession
or as a standalone speech generator. For example,
you can use this TTS in the
Voice AI quickstart
.
from
livekit
.
plugins
import
inworld
session
=
AgentSession
(
tts
=
inworld
.
TTS
(
voice
=
"Hades"
)
# ... llm, stt, etc.
)
Parameters
This section describes some of the available parameters. See the
plugin reference
for a complete list of all available parameters.
model
string
Optional
Default:
"inworld-tts-1"
#
ID of the model to use for generation. See
supported models
.
voice
string
Optional
Default:
"Ashley"
#
ID of the voice to use for generation. Use the
List voices API endpoint
for possible values.
Additional resources
The following resources provide more information about using Inworld with LiveKit Agents.
Python package
The
livekit-plugins-inworld
package on PyPI.
Plugin reference
Reference for the Inworld TTS plugin.
GitHub repo
View the source or contribute to the LiveKit Inworld TTS plugin.
Inworld docs
Inworld TTS docs.
On this page
Overview
Quick reference
Installation
Authentication
Usage
Parameters
Additional resources


Content from https://docs.livekit.io/agents/integrations/tts/lmnt:

On this page
Overview
Quick reference
Installation
Authentication
Usage
Parameters
Additional resources
Copy page
See more page options
Overview
LMNT
provides a fast text-to-speech service optimized for realtime voice AI. With LiveKit's LMNT integration and the Agents framework, you can build high-performance and lifelike voice AI at scale.
Quick reference
This section provides a quick reference for the LMNT TTS plugin. For more information, see
Additional resources
.
Installation
Install the plugin from PyPI:
pip
install
"livekit-agents[lmnt]~=1.0"
Authentication
The LMNT plugin requires an
LMNT API key
.
Set
LMNT_API_KEY
in your
.env
file.
Usage
Use LMNT TTS within an
AgentSession
or as a standalone speech generator. For example,
you can use this TTS in the
Voice AI quickstart
.
from
livekit
.
plugins
import
lmnt
session
=
AgentSession
(
tts
=
lmnt
.
TTS
(
voice
=
"leah"
,
)
# ... llm, stt, etc.
)
Parameters
This section describes some of the parameters you can set when you create an LMNT TTS. See the
plugin reference
for a complete list of all available parameters.
model
string
Optional
Default:
blizzard
#
The model to use for synthesis. Refer to the
LMNT models guide
for possible values.
voice
string
Optional
Default:
leah
#
The voice ID to use. Find or create new voices in the
LMNT voice library
.
language
string
Optional
#
Two-letter ISO 639-1 language code. See the
LMNT API documentation
for supported languages.
temperature
float
Optional
#
Influences how expressive and emotionally varied the speech becomes. Lower values (like 0.3) create more neutral, consistent speaking styles. Higher values (like 1.0) allow for more dynamic emotional range and speaking styles.
top_p
float
Optional
#
Controls the stability of the generated speech. A lower value (like 0.3) produces more consistent, reliable speech. A higher value (like 0.9) gives more flexibility in how words are spoken, but might occasionally produce unusual intonations or speech patterns.
Additional resources
The following resources provide more information about using LMNT with LiveKit Agents.
Python package
The
livekit-plugins-lmnt
package on PyPI.
Plugin reference
Reference for the LMNT TTS plugin.
GitHub repo
View the source or contribute to the LiveKit LMNT TTS plugin.
LMNT docs
LMNT API documentation.
Voice AI quickstart
Get started with LiveKit Agents and LMNT TTS.
On this page
Overview
Quick reference
Installation
Authentication
Usage
Parameters
Additional resources


Content from https://docs.livekit.io/agents/integrations/tts/neuphonic:

On this page
Overview
Quick reference
Installation
Authentication
Usage
Parameters
Additional resources
Copy page
See more page options
Overview
Neuphonic
provides hyper realistic realtime voice synthesis. You can use the Neuphonic TTS plugin for LiveKit Agents to build voice AI applications that sound realistic.
Quick reference
This section includes a brief overview of the Neuphonic TTS plugin. For more information, see
Additional resources
.
Installation
Install the plugin from PyPI:
pip
install
"livekit-agents[neuphonic]~=1.0"
Authentication
The Neuphonic plugin requires a
Neuphonic API key
.
Set
NEUPHONIC_API_TOKEN
in your
.env
file.
Usage
Use Neuphonic TTS within an
AgentSession
or as a standalone speech generator. For example,
you can use this TTS in the
Voice AI quickstart
.
from
livekit
.
plugins
import
neuphonic
session
=
AgentSession
(
tts
=
neuphonic
.
TTS
(
voice_id
=
"fc854436-2dac-4d21-aa69-ae17b54e98eb"
)
,
# ... llm, stt, etc.
)
Parameters
This section describes some of the available parameters. See the
plugin reference
for a complete list of all available parameters.
voice_id
string
Required
#
ID of the voice to use for generation.
speed
float
Optional
Default:
1
#
Speed of generated speech.
model
string
Optional
Default:
neu_hq
#
ID of the model to use for generation.
lang_code
string
Optional
Default:
en
#
Language code for the generated speech.
Additional resources
The following resources provide more information about using Neuphonic with LiveKit Agents.
Python package
The
livekit-plugins-neuphonic
package on PyPI.
Plugin reference
Reference for the Neuphonic TTS plugin.
GitHub repo
View the source or contribute to the LiveKit Neuphonic TTS plugin.
Neuphonic documentation
Neuphonic's full documentation.
Voice AI quickstart
Get started with LiveKit Agents and Neuphonic TTS.
On this page
Overview
Quick reference
Installation
Authentication
Usage
Parameters
Additional resources


Content from https://docs.livekit.io/agents/integrations/tts/playai:

On this page
Overview
Quick reference
Installation
Authentication
Usage
Parameters
Customizing pronunciation
Additional resources
Copy page
See more page options
Overview
PlayHT
provides realistic TTS voice generation. With LiveKit's PlayHT integration and the Agents
framework, you can build voice AI applications with fluent and conversational voices.
To learn more about TTS and generating agent speech, see
Agent speech
.
Quick reference
This section includes a basic usage example and some reference material. For links to more detailed documentation, see
Additional resources
.
Installation
Install the plugin from PyPI:
pip
install
"livekit-agents[playai]~=1.0"
Authentication
The PlayHT plugin requires a
PlayHT API key
.
Set the following environment variables in your
.env
file:
PLAYHT_API_KEY
=
<
playht-api-key
>
PLAYHT_USER_ID
=
<
playht-user-id
>
Usage
Use PlayHT TTS within an
AgentSession
or as a standalone speech generator. For example,
you can use this TTS in the
Voice AI quickstart
.
from
livekit
.
plugins
import
playai
session
=
AgentSession
(
tts
=
playai
.
TTS
(
voice
=
"s3://voice-cloning-zero-shot/a59cb96d-bba8-4e24-81f2-e60b888a0275/charlottenarrativesaad/manifest.json"
,
language
=
"SPANISH"
,
model
=
"play3.0-mini"
,
)
,
# ... llm, stt, etc.
)
Parameters
This section describes some of the available parameters. See the
plugin reference
for a complete list of all available parameters.
voice
string
Optional
Default:
s3://voice-cloning-zero-shot/d9ff78ba-d016-47f6-b0ef-dd630f59414e/female-cs/manifest.json
#
URL of the voice manifest file. For a full list, see
List of pre-built voices
.
model
TTSModel | string
Optional
Default:
Play3.0-mini
#
Name of the TTS model. For a full list, see
Models
.
language
string
Optional
Default:
ENGLISH
#
Language of the text to be spoken. For language support by model, see
Models
.
Customizing pronunciation
PlayHT TTS supports adding custom pronunciations to your speech-to-text conversions. To learn more,
see the
Add Custom Pronunciations to your Audio help article
.
Additional resources
The following resources provide more information about using PlayHT with LiveKit Agents.
Python package
The
livekit-plugins-playai
package on PyPI.
Plugin reference
Reference for the PlayHT TTS plugin.
GitHub repo
View the source or contribute to the LiveKit PlayHT TTS plugin.
PlayHT docs
PlayHT TTS docs.
Voice AI quickstart
Get started with LiveKit Agents and PlayHT TTS.
On this page
Overview
Quick reference
Installation
Authentication
Usage
Parameters
Customizing pronunciation
Additional resources


Content from https://docs.livekit.io/agents/integrations/tts/resemble:

On this page
Overview
Quick reference
Installation
Authentication
Usage
Parameters
Customizing pronunciation
Additional resources
Copy page
See more page options
Overview
Resemble AI
provides natural and human-like text-to-speech. You can use the Resemble AI TTS plugin for LiveKit Agents to build your voice AI applications.
Quick reference
This section includes a brief overview of the Resemble AI TTS plugin. For more information, see
Additional resources
.
Installation
Install the plugin from PyPI:
pip
install
"livekit-agents[resemble]~=1.0"
Authentication
The Resemble AI plugin requires a
Resemble AI API key
.
Set
RESEMBLE_API_KEY
in your
.env
file.
Usage
Use Resemble AI TTS within an
AgentSession
or as a standalone speech generator. For example, you can use this TTS in the
Voice AI quickstart
.
from
livekit
.
plugins
import
resemble
session
=
AgentSession
(
tts
=
resemble
.
TTS
(
voice_uuid
=
"55592656"
,
)
# ... llm, stt, etc.
)
Parameters
This section describes some of the available parameters. See the
plugin reference
for a complete list of all available parameters.
voice_uuid
string
Required
Default:
55592656
#
ID of the voice to use.
Customizing pronunciation
Resemble AI supports custom pronunciation with Speech Synthesis Markup Language (SSML), an XML-based markup language that gives you granular control over speech output. With SSML, you can leverage XML tags to craft audio content that delivers a more natural and engaging listening experience. To learn more, see
SSML
.
Additional resources
The following resources provide more information about using Resemble AI with LiveKit Agents.
Python package
The
livekit-plugins-resemble
package on PyPI.
Plugin reference
Reference for the Resemble AI TTS plugin.
Resemble AI docs
Resemble AI docs.
Voice AI quickstart
Get started with LiveKit Agents and Resemble AI TTS.
On this page
Overview
Quick reference
Installation
Authentication
Usage
Parameters
Customizing pronunciation
Additional resources


Content from https://docs.livekit.io/agents/integrations/tts/rime:

On this page
Overview
Quick reference
Installation
Authentication
Usage
Parameters
Customizing pronunciation
Additional resources
Copy page
See more page options
Overview
Rime
provides text-to-speech synthesis (TTS) optimized for speed and quality. With LiveKit's Rime integration and the Agents framework, you can build voice AI applications that are responsive and sound realistic.
To learn more about TTS and generating agent speech, see
Agent speech
.
Quick reference
This section includes a basic usage example and some reference material. For links to more detailed documentation, see
Additional resources
.
Installation
Install the plugin from PyPI:
pip
install
"livekit-agents[rime]~=1.0"
Authentication
The Rime plugin requires a
Rime API key
.
Set
RIME_API_KEY
in your
.env
file.
Usage
Use Rime TTS within an
AgentSession
or as a standalone speech generator. For example,
you can use this TTS in the
Voice AI quickstart
.
from
livekit
.
plugins
import
rime
session
=
AgentSession
(
tts
=
rime
.
TTS
(
model
=
"mist"
,
speaker
=
"rainforest"
,
speed_alpha
=
0.9
,
reduce_latency
=
True
,
)
,
# ... llm, stt, etc.
)
Parameters
This section describes some of the available parameters. See the
plugin reference
for a complete list of all available parameters.
model
string
Optional
Default:
mist
#
ID of the model to use. To learn more, see
Models
.
speaker
string
Optional
Default:
lagoon
#
ID of the voice to use for speech generation. To learn more, see
Voices
.
audio_format
TTSEncoding
Optional
Default:
pcm
#
Audio format to use. Valid values are:
pcm
and
mp3
.
sample_rate
integer
Optional
Default:
16000
#
Sample rate of the generated audio. Set this rate to best match your application needs.
To learn more, see
Recommendations for reducing response time
.
speed_alpha
float
Optional
Default:
1.0
#
Adjusts the speed of speech. Lower than
1.0
results in faster speech; higher than
1.0
results in slower speech.
reduce_latency
boolean
Optional
Default:
false
#
When set to
true
, turns off text normalization to reduce the amount of time spent preparing input text for TTS inference. This
might result in the mispronunciation of digits and abbreviations.
To learn more, see
Recommendations for reducing response time
.
phonemize_between_brackets
boolean
Optional
Default:
false
#
When set to
true
, allows the use of custom pronunciation strings in text. To learn more, see
Custom pronunciation
.
api_key
string
Optional
Env:
RIME_API_KEY
#
Rime API Key. Required if the environment variable isn't set.
Customizing pronunciation
Rime TTS supports customizing pronunciation. To learn more, see
Custom Pronunciation guide
.
Additional resources
The following resources provide more information about using Rime with LiveKit Agents.
Python package
The
livekit-plugins-rime
package on PyPI.
Plugin reference
Reference for the Rime TTS plugin.
GitHub repo
View the source or contribute to the LiveKit Rime TTS plugin.
Rime docs
Rime TTS docs.
Voice AI quickstart
Get started with LiveKit Agents and Rime TTS.
On this page
Overview
Quick reference
Installation
Authentication
Usage
Parameters
Customizing pronunciation
Additional resources


Content from https://docs.livekit.io/agents/integrations/tts/sarvam:

On this page
Overview
Quick reference
Installation
Authentication
Usage
Parameters
Additional resources
Copy page
See more page options
Overview
Sarvam
provides high-quality text-to-speech technology optimized for Indian languages. With LiveKit's Sarvam integration and the Agents framework, you can build voice AI agents that sound natural in Indian languages.
Quick reference
This section provides a quick reference for the Sarvam TTS plugin. For more information, see
Additional resources
.
Installation
Install the plugin from PyPI:
pip
install
"livekit-agents[sarvam]~=1.0"
Authentication
The Sarvam plugin requires a
Sarvam API key
.
Set
SARVAM_API_KEY
in your
.env
file.
Usage
Use Sarvam TTS within an
AgentSession
or as a standalone speech generator. For example,
you can use this TTS in the
Voice AI quickstart
.
from
livekit
.
plugins
import
sarvam
session
=
AgentSession
(
tts
=
sarvam
.
TTS
(
target_language_code
=
"hi-IN"
,
speaker
=
"anushka"
,
)
# ... llm, stt, etc.
)
Parameters
This section describes some of the available parameters. See the
plugin reference
for a complete list of all available parameters.
target_language_code
string
Required
#
BCP-47 language code for supported Indian languages. For example:
hi-IN
for Hindi,
en-IN
for Indian English. See
documentation
for a complete list of supported languages.
model
string
Optional
Default:
bulbul:v2
#
The Sarvam TTS model to use. Currently only
bulbul:v2
is supported. See
documentation
for a complete list of supported models.
speaker
string
Optional
Default:
anushka
#
Voice to use for synthesis. See
documentation
for a complete list of supported voices.
pitch
float
Optional
Default:
0.0
#
Voice pitch adjustment. Valid range: -20.0 to 20.0.
pace
float
Optional
Default:
1.0
#
Speech rate multiplier. Valid range: 0.5 to 2.0.
loudness
float
Optional
Default:
1.0
#
Volume multiplier. Valid range: 0.5 to 2.0.
Additional resources
The following resources provide more information about using Sarvam with LiveKit Agents.
Python package
The
livekit-plugins-sarvam
package on PyPI.
Plugin reference
Reference for the Sarvam TTS plugin.
GitHub repo
View the source or contribute to the LiveKit Sarvam TTS plugin.
Sarvam docs
Sarvam's full docs site.
Voice AI quickstart
Get started with LiveKit Agents and Sarvam.
Sarvam STT
Guide to the Sarvam STT integration with LiveKit Agents.
On this page
Overview
Quick reference
Installation
Authentication
Usage
Parameters
Additional resources


Content from https://docs.livekit.io/agents/integrations/tts/speechify:

On this page
Overview
Quick reference
Installation
Authentication
Usage
Parameters
Customizing pronunciation
Additional resources
Copy page
See more page options
Overview
Speechify
provides an ultra low latency, human quality, and affordable text to speech API with voice cloning features. You can use the Speechify TTS plugin for LiveKit Agents to build high-quality voice AI applications.
Quick reference
This section includes a brief overview of the Speechify TTS plugin. For more information, see
Additional resources
.
Installation
Install the plugin from PyPI:
pip
install
"livekit-agents[speechify]~=1.0"
Authentication
The Speechify plugin requires a
Speechify API key
.
Set
SPEECHIFY_API_KEY
in your .env file.
Usage
Use Speechify TTS within an AgentSession or as a standalone speech generator. For example, you can use this TTS in the
Voice AI quickstart
.
from
livekit
.
plugins
import
speechify
session
=
AgentSession
(
tts
=
speechify
.
TTS
(
model
=
"simba-english"
,
voice_id
=
"jack"
,
)
# ... llm, stt, etc.
)
Parameters
This section describes some of the available parameters. See the
plugin reference
for a complete list of all available parameters.
voice_id
string
Required
Default:
jack
#
ID of the voice to be used for synthesizing speech. Refer to
list_voices()
method in the
plugin reference
.
model
string
Optional
#
ID of the model to use for generation. Use
simba-english
or
simba-multilingual
To learn more, see:
supported models
.
language
string
Optional
#
Language of input text in ISO-639-1 format. See the
supported languages
.
encoding
string
Optional
Default:
wav_48000
#
Audio encoding to use. Choose between
wav_48000
,
mp3_24000
,
ogg_24000
or
aac_24000
.
loudness_normalization
boolean
Optional
#
Determines whether to normalize the audio loudness to a standard level. When enabled, loudness normalization aligns the audio output to the following standards: Integrated loudness: -14 LUFS True peak: -2 dBTP Loudness range: 7 LU If disabled, the audio loudness will match the original loudness of the selected voice, which may vary significantly and be either too quiet or too loud. Enabling loudness normalization can increase latency due to additional processing required for audio level adjustments.
text_normalization
boolean
Optional
#
Determines whether to normalize the text. If enabled, it will transform numbers, dates, etc. into words. For example, "55" is normalized into "fifty five". This can increase latency due to additional processing required for text normalization.
Customizing pronunciation
Speechify supports custom pronunciation with Speech Synthesis Markup Language (SSML), an XML-based markup language that gives you granular control over speech output. With SSML, you can leverage XML tags to craft audio content that delivers a more natural and engaging listening experience. To learn more, see
SSML
.
Additional resources
The following resources provide more information about using Speechify with LiveKit Agents.
Python package
The
livekit-plugins-speechify
package on PyPI.
Plugin reference
Reference for the Speechify TTS plugin.
Speechify docs
Speechify docs.
Voice AI quickstart
Get started with LiveKit Agents and Speechify TTS.
On this page
Overview
Quick reference
Installation
Authentication
Usage
Parameters
Customizing pronunciation
Additional resources


Content from https://docs.livekit.io/agents/integrations/tts/spitch:

On this page
Overview
Quick reference
Installation
Authentication
Usage
Parameters
Additional resources
Copy page
See more page options
Overview
Spitch
provides AI-powered speech and language solutions optimized for African languages. With LiveKit's Spitch integration and the Agents framework, you can build voice AI agents that produce natural speech in a variety of African languages.
Quick reference
This section provides a quick reference for the Spitch TTS plugin. For more information, see
Additional resources
.
Installation
Install the plugin from PyPI:
pip
install
"livekit-agents[spitch]~=1.0"
Authentication
The Spitch plugin requires a
Spitch API key
.
Set
SPITCH_API_KEY
in your
.env
file.
Usage
Use Spitch TTS within an
AgentSession
or as a standalone speech generator. For example,
you can use this TTS in the
Voice AI quickstart
.
from
livekit
.
plugins
import
spitch
session
=
AgentSession
(
tts
=
spitch
.
TTS
(
language
=
"en"
,
voice
=
"lina"
,
)
# ... llm, stt, etc.
)
Parameters
This section describes some of the available parameters. See the
plugin reference
for a complete list of all available parameters.
language
string
Optional
Default:
en
#
Language short code for the generated speech. For supported values, see
Spitch languages
.
voice
string
Optional
Default:
lina
#
Voice to use for synthesis. For supported values, see
Spitch voices
.
Additional resources
The following resources provide more information about using Spitch with LiveKit Agents.
Python package
The
livekit-plugins-spitch
package on PyPI.
Plugin reference
Reference for the Spitch TTS plugin.
GitHub repo
View the source or contribute to the LiveKit Spitch TTS plugin.
Spitch docs
Spitch's official documentation.
Voice AI quickstart
Get started with LiveKit Agents and Spitch.
Spitch STT
Guide to the Spitch STT integration with LiveKit Agents.
On this page
Overview
Quick reference
Installation
Authentication
Usage
Parameters
Additional resources


Content from https://docs.livekit.io/agents/integrations/avatar/bey:

On this page
Overview
Quick reference
Installation
Authentication
Usage
Parameters
Additional resources
Copy page
See more page options
Overview
Beyond Presence
provides hyper-realistic interactive avatars for conversational video AI agents. You can use the open source Beyond Presence integration for LiveKit Agents to add virtual avatars to your voice AI app.
Quick reference
This section includes a basic usage example and some reference material. For links to more detailed documentation, see
Additional resources
.
Installation
Install the plugin from PyPI:
pip
install
"livekit-agents[bey]~=1.0"
Authentication
The Beyond Presence plugin requires a
Beyond Presence API key
.
Set
BEY_API_KEY
in your
.env
file.
Usage
Use the plugin in an
AgentSession
. For example, you can use this avatar in the
Voice AI quickstart
.
from
livekit
.
plugins
import
bey
session
=
AgentSession
(
# ... stt, llm, tts, etc.
)
avatar
=
bey
.
AvatarSession
(
avatar_id
=
"..."
,
# ID of the Beyond Presence avatar to use
)
# Start the avatar and wait for it to join
await
avatar
.
start
(
session
,
room
=
ctx
.
room
)
# Start your agent session with the user
await
session
.
start
(
room
=
ctx
.
room
,
)
Preview the avatar in the
Agents Playground
or a frontend
starter app
that you build.
Parameters
This section describes some of the available parameters. See the
plugin reference
for a complete list of all available parameters.
avatar_id
string
Optional
Default:
b9be11b8-89fb-4227-8f86-4a881393cbdb
#
ID of the Beyond Presence avatar to use.
avatar_participant_identity
string
Optional
Default:
bey-avatar-agent
#
The identity of the participant to use for the avatar.
avatar_participant_name
string
Optional
Default:
bey-avatar-agent
#
The name of the participant to use for the avatar.
Additional resources
The following resources provide more information about using Beyond Presence with LiveKit Agents.
Python package
The
livekit-plugins-bey
package on PyPI.
Plugin reference
Reference for the Beyond Presence avatar plugin.
GitHub repo
View the source or contribute to the LiveKit Beyond Presence avatar plugin.
Beyond Presence docs
Beyond Presence's full docs site.
Agents Playground
A virtual workbench to test your avatar agent.
Frontend starter apps
Ready-to-use frontend apps with avatar support.
On this page
Overview
Quick reference
Installation
Authentication
Usage
Parameters
Additional resources


Content from https://docs.livekit.io/agents/integrations/avatar/bithuman:

On this page
Overview
Quick reference
Installation
Authentication
Model installation
Usage
Parameters
Additional resources
Copy page
See more page options
Overview
bitHuman
provides realtime virtual avatars that run locally on CPU only for low latency and high quality. You can use the open source bitHuman integration for LiveKit Agents to add virtual avatars to your voice AI app.
Quick reference
This section includes a basic usage example and some reference material. For links to more detailed documentation, see
Additional resources
.
Installation
Install the plugin from PyPI:
pip
install
"livekit-agents[bithuman]~=1.0"
Authentication
The bitHuman plugin requires a
bitHuman API Secret
.
Set
BITHUMAN_API_SECRET
in your
.env
file.
Model installation
The bitHuman integration requires a locally download model. Download a
sample model
or create your own.
Follow the guide below to pass it to the avatar session, or set
BITHUMAN_MODEL_PATH
in your
.env
file.
Usage
Use the plugin in an
AgentSession
. For example, you can use this avatar in the
Voice AI quickstart
.
from
livekit
.
plugins
import
bithuman
session
=
AgentSession
(
# ... stt, llm, tts, etc.
)
avatar
=
bithuman
.
AvatarSession
(
model_path
=
"./albert_einstein.imx"
,
# This example uses a demo model installed in the current directory
)
# Start the avatar and wait for it to join
await
avatar
.
start
(
session
,
room
=
ctx
.
room
)
# Start your agent session with the user
await
session
.
start
(
room
=
ctx
.
room
,
)
Preview the avatar in the
Agents Playground
or a frontend
starter app
that you build.
Parameters
This section describes some of the available parameters. See the
plugin reference
for a complete list of all available parameters.
model_path
string
Required
Env:
BITHUMAN_MODEL_PATH
#
Path to the bitHuman model to use. To learn more, see the
bitHuman docs
.
Additional resources
The following resources provide more information about using bitHuman with LiveKit Agents.
Python package
The
livekit-plugins-bithuman
package on PyPI.
Plugin reference
Reference for the bitHuman avatar plugin.
GitHub repo
View the source or contribute to the LiveKit bitHuman avatar plugin.
bitHuman docs
bitHuman's full API docs site.
Agents Playground
A virtual workbench to test your avatar agent.
Frontend starter apps
Ready-to-use frontend apps with avatar support.
On this page
Overview
Quick reference
Installation
Authentication
Model installation
Usage
Parameters
Additional resources


Content from https://docs.livekit.io/agents/integrations/avatar/hedra:

On this page
Overview
Quick reference
Installation
Authentication
Usage
Avatar setup
Parameters
Additional resources
Copy page
See more page options
Overview
Hedra
provides lifelike generative video with their Character-3 model. You can use the open source Hedra integration for LiveKit Agents to add virtual avatars to your voice AI app.
Quick reference
This section includes a basic usage example and some reference material. For links to more detailed documentation, see
Additional resources
.
Installation
Install the plugin from PyPI:
pip
install
"livekit-agents[hedra]~=1.0"
Authentication
The Hedra plugin requires a
Hedra API key
.
Set
HEDRA_API_KEY
in your
.env
file.
Usage
Use the plugin in an
AgentSession
. For example, you can use this avatar in the
Voice AI quickstart
.
from
livekit
import
agents
from
livekit
.
agents
import
AgentSession
,
RoomOutputOptions
from
livekit
.
plugins
import
hedra
async
def
entrypoint
(
ctx
:
agents
.
JobContext
)
:
session
=
AgentSession
(
# ... stt, llm, tts, etc.
)
avatar
=
hedra
.
AvatarSession
(
avatar_id
=
"..."
,
# ID of the Hedra avatar to use. See "Avatar setup" for details.
)
# Start the avatar and wait for it to join
await
avatar
.
start
(
session
,
room
=
ctx
.
room
)
# Start your agent session with the user
await
session
.
start
(
room
=
ctx
.
room
,
room_output_options
=
RoomOutputOptions
(
# Disable audio output to the room. The avatar plugin publishes audio separately.
audio_enabled
=
False
,
)
,
# ... agent, room_input_options, etc....
)
Preview the avatar in the
Agents Playground
or a frontend
starter app
that you build.
Avatar setup
The Hedra plugin requires a source image asset from which to generate the avatar. Avatars render as 512x512px square videos. Hedra automatically centers and crops around the face within the provided image. Hedra supports humanoid faces, in a range of styles from photorealistic to animated.
You can specify the avatar image by ID or by passing an image directly.
Pass avatar ID
To use an existing avatar, pass the
avatar_id
parameter to the plugin. You can find the ID in the Hedra web studio or upload it using the Hedra API.
Web studio
Generate or upload an image in the
Hedra web studio
. To find the ID to pass as
avatar_id
, download the image from the
library
. The avatar ID is the filename of the downloaded image, minus the extension.
API upload
To upload an image with the Hedra API, first create a new asset:
curl
-X
POST
\
-H
"X-API-Key: <your-api-key>"
\
-H
"Content-Type: application/json"
\
-d
'{"type":"image","name":"<your-avatar-name>"}'
\
https://api.hedra.com/web-app/public/assets
The response includes an asset
id
, which you need for the next step.
Then upload the image:
curl
-X
POST
\
-H
"X-API-Key: <your-api-key>"
\
-H
"Content-Type: multipart/form-data"
\
-F
"file=@<your-local-image-path>"
\
https://api.hedra.com/web-app/public/assets/
<
your-asset-id
>
/upload
You can now use the asset ID in the Hedra plugin as the
avatar_id
.
Pass image directly
To upload a new image directly in the plugin, pass a PIL
Image
object in the
avatar_image
parameter.
from
PIL
import
Image
avatar_image
=
Image
.
open
(
"/path/to/image.jpg"
)
avatar
=
hedra
.
AvatarSession
(
avatar_image
=
avatar_image
,
)
The plugin uploads the image to Hedra and uses it for the avatar session. The image can come from anywhere, including your local filesystem, a remote URL,
uploaded in realtime from your frontend
, or generated by an external API or AI model.
Parameters
This section describes some of the available parameters. See the
plugin reference
for a complete list of all available parameters.
avatar_id
string
Optional
#
ID of the Hedra avatar to use. See
Avatar setup
for details.
avatar_image
string
Optional
#
PIL
Image
object to use for the avatar. See
Image upload
for details.
avatar_participant_name
string
Optional
Default:
hedra-avatar-agent
#
The name of the participant to use for the avatar.
Additional resources
The following resources provide more information about using Hedra with LiveKit Agents.
Python package
The
livekit-plugins-hedra
package on PyPI.
Plugin reference
Reference for the Hedra avatar plugin.
GitHub repo
View the source or contribute to the LiveKit Hedra avatar plugin.
Hedra API docs
Hedra's API docs.
Agents Playground
A virtual workbench to test your avatar agent.
Frontend starter apps
Ready-to-use frontend apps with avatar support.
On this page
Overview
Quick reference
Installation
Authentication
Usage
Avatar setup
Parameters
Additional resources


Content from https://docs.livekit.io/agents/integrations/avatar/tavus:

On this page
Overview
Quick reference
Installation
Authentication
Replica and persona setup
Usage
Parameters
Additional resources
Copy page
See more page options
Overview
Tavus
provides hyper-realistic interactive avatars for conversational video AI agents. You can use the open source Tavus integration for LiveKit Agents to add virtual avatars to your voice AI app.
Tavus demo
A video showcasing an educational AI agent that uses Tavus to create an interactive study partner.
Quick reference
This section includes a basic usage example and some reference material. For links to more detailed documentation, see
Additional resources
.
Installation
Install the plugin from PyPI:
pip
install
"livekit-agents[tavus]~=1.0"
Authentication
The Tavus plugin requires a
Tavus API key
.
Set
TAVUS_API_KEY
in your
.env
file.
Replica and persona setup
The Tavus plugin requires a
Replica
and a
Persona
to start an avatar session.
You can use any replica with the Tavus plugin, but must setup a persona with the following settings for full compatibility with LiveKit Agents:
Set the
pipeline_mode
to
echo
Define a
transport
layer under
layers
, setting the
transport_type
inside to
livekit
.
Here is a simple
curl
command to create a persona with the correct settings using the
Create Persona endpoint
:
curl
--request
POST
\
--url
https://tavusapi.com/v2/personas
\
-H
"Content-Type: application/json"
\
-H
"x-api-key: <api-key>"
\
-d
'{
"layers": {
"transport": {
"transport_type": "livekit"
}
},
"persona_name": "My Persona",
"pipeline_mode": "echo"
}'
Copy your replica ID and persona ID for the following steps.
Usage
Use the plugin in an
AgentSession
. For example, you can use this avatar in the
Voice AI quickstart
.
from
livekit
import
agents
from
livekit
.
agents
import
AgentSession
,
RoomOutputOptions
from
livekit
.
plugins
import
tavus
async
def
entrypoint
(
ctx
:
agents
.
JobContext
)
:
session
=
AgentSession
(
# ... stt, llm, tts, etc.
)
avatar
=
tavus
.
AvatarSession
(
replica_id
=
"..."
,
# ID of the Tavus replica to use
persona_id
=
"..."
,
# ID of the Tavus persona to use (see preceding section for configuration details)
)
# Start the avatar and wait for it to join
await
avatar
.
start
(
session
,
room
=
ctx
.
room
)
# Start your agent session with the user
await
session
.
start
(
room
=
ctx
.
room
,
room_output_options
=
RoomOutputOptions
(
# Disable audio output to the room. The avatar plugin publishes audio separately.
audio_enabled
=
False
,
)
,
# ... agent, room_input_options, etc....
)
Preview the avatar in the
Agents Playground
or a frontend
starter app
that you build.
Parameters
This section describes some of the available parameters. See the
plugin reference
for a complete list of all available parameters.
replica_id
string
Required
#
ID of the Tavus replica to use. See
Replica and persona setup
for details.
persona_id
string
Required
#
ID of the Tavus persona to use. See
Replica and persona setup
for details.
avatar_participant_name
string
Optional
Default:
Tavus-avatar-agent
#
The name of the participant to use for the avatar.
Additional resources
The following resources provide more information about using Tavus with LiveKit Agents.
Python package
The
livekit-plugins-tavus
package on PyPI.
Plugin reference
Reference for the Tavus avatar plugin.
GitHub repo
View the source or contribute to the LiveKit Tavus avatar plugin.
Tavus docs
Tavus's full docs site.
Agents Playground
A virtual workbench to test your avatar agent.
Frontend starter apps
Ready-to-use frontend apps with avatar support.
On this page
Overview
Quick reference
Installation
Authentication
Replica and persona setup
Usage
Parameters
Additional resources


Content from https://docs.livekit.io/home/client/tracks/publish:

On this page
Overview
Device permissions
Mute and unmute
Track permissions
Publishing from backend
Publishing audio tracks
Publishing video tracks
Audio and video synchronization
Copy page
See more page options
Overview
LiveKit includes a simple and consistent method to publish the user's camera and microphone, regardless of the device or browser they are using. In all cases, LiveKit displays the correct indicators when recording is active and acquires the necessary permissions from the user.
// Enables the camera and publishes it to a new video track
room
.
localParticipant
.
setCameraEnabled
(
true
)
;
// Enables the microphone and publishes it to a new audio track
room
.
localParticipant
.
setMicrophoneEnabled
(
true
)
;
Device permissions
In native and mobile apps, you typically need to acquire consent from the user to access the microphone or camera. LiveKit integrates with the system privacy settings to record permission and display the correct indicators when audio or video capture is active.
For web browsers, the user is automatically prompted to grant camera and microphone permissions the first time your app attempts to access them and no additional configuration is required.
Swift
Android
React Native
Flutter
Add these entries to your
Info.plist
:
<
key
>
NSCameraUsageDescription
</
key
>
<
string
>
$(PRODUCT_NAME) uses your camera
</
string
>
<
key
>
NSMicrophoneUsageDescription
</
key
>
<
string
>
$(PRODUCT_NAME) uses your microphone
</
string
>
To enable background audio, you must also add the "Background Modes" capability with "Audio, AirPlay, and Picture in Picture" selected.
Your
Info.plist
should have:
<
key
>
UIBackgroundModes
</
key
>
<
array
>
<
string
>
audio
</
string
>
</
array
>
Mute and unmute
You can mute any track to stop it from sending data to the server. When a track is muted, LiveKit will trigger a
TrackMuted
event on all participants in the room. You can use this event to update your app's UI and reflect the correct state to all users in the room.
Mute/unmute a track using its corresponding
LocalTrackPublication
object.
Track permissions
By default, any published track can be subscribed to by all participants. However, publishers can restrict who can subscribe to their tracks using Track Subscription Permissions:
JavaScript
Swift
Android
Flutter
Python
localParticipant
.
setTrackSubscriptionPermissions
(
false
,
[
{
participantIdentity
:
'allowed-identity'
,
allowAll
:
true
,
}
,
]
)
;
Publishing from backend
You may also publish audio and video tracks from a backend process, which can be consumed just like any camera or microphone track. The
LiveKit Agents
framework makes it easy to add a programmable participant to any room, and publish media such as synthesized speech or video.
LiveKit also includes complete SDKs for server environments in
Go
,
Rust
,
Python
, and
Node.js
.
You can also publish media using the
LiveKit CLI
.
Publishing audio tracks
You can publish audio by creating an
AudioSource
and publishing it as a track.
Audio streams carry raw PCM data at a specified sample rate and channel count. Publishing audio involves splitting the stream into audio frames of a configurable length. An internal buffer holds 50 ms of queued audio to send to the realtime stack. The
capture_frame
method, used to send new frames, is blocking and doesn't return control until the buffer has taken in the entire frame. This allows for easier interruption handling.
In order to publish an audio track, you need to determine the sample rate and number of channels beforehand, as well as the length (number of samples) of each frame. In the following example, the agent transmits a constant 16-bit sine wave at 48kHz in 10 ms long frames:
Python
import
numpy
as
np
from
livekit
import
agents
,
rtc
SAMPLE_RATE
=
48000
NUM_CHANNELS
=
1
# mono audio
AMPLITUDE
=
2
**
8
-
1
SAMPLES_PER_CHANNEL
=
480
# 10 ms at 48kHz
async
def
entrypoint
(
ctx
:
agents
.
JobContext
)
:
source
=
rtc
.
AudioSource
(
SAMPLE_RATE
,
NUM_CHANNELS
)
track
=
rtc
.
LocalAudioTrack
.
create_audio_track
(
"example-track"
,
source
)
# since the agent is a participant, our audio I/O is its "microphone"
options
=
rtc
.
TrackPublishOptions
(
source
=
rtc
.
TrackSource
.
SOURCE_MICROPHONE
)
# ctx.agent is an alias for ctx.room.local_participant
publication
=
await
ctx
.
agent
.
publish_track
(
track
,
options
)
frequency
=
440
async
def
_sinewave
(
)
:
audio_frame
=
rtc
.
AudioFrame
.
create
(
SAMPLE_RATE
,
NUM_CHANNELS
,
SAMPLES_PER_CHANNEL
)
audio_data
=
np
.
frombuffer
(
audio_frame
.
data
,
dtype
=
np
.
int16
)
time
=
np
.
arange
(
SAMPLES_PER_CHANNEL
)
/
SAMPLE_RATE
total_samples
=
0
while
True
:
time
=
(
total_samples
+
np
.
arange
(
SAMPLES_PER_CHANNEL
)
)
/
SAMPLE_RATE
sinewave
=
(
AMPLITUDE
*
np
.
sin
(
2
*
np
.
pi
*
frequency
*
time
)
)
.
astype
(
np
.
int16
)
np
.
copyto
(
audio_data
,
sinewave
)
# send this frame to the track
await
source
.
capture_frame
(
audio_frame
)
total_samples
+=
SAMPLES_PER_CHANNEL
await
_sinewave
(
)
Warning
When streaming finite audio (for example, from a file), make sure the frame length isn't longer than the number of samples left to stream, otherwise the end of the buffer consists of noise.
Audio examples
For audio examples using the LiveKit SDK, see the following in the GitHub repository:
Speedup Output Audio
Use the
TTS node
to speed up audio output.
Echo Agent
Echo user audio back to them.
Sync TTS Transcription
Uses manual subscription, transcription forwarding, and manually publishes audio output.
Publishing video tracks
Agents publish data to their tracks as a continuous live feed. Video streams can transmit data in any of
11 buffer encodings
.
When publishing video tracks, you need to establish the frame rate and buffer encoding of the video beforehand.
In this example, the agent connects to the room and starts publishing a solid color frame at 10 frames per second (FPS). Copy the following code into your
entrypoint
function:
Python
from
livekit
import
rtc
from
livekit
.
agents
import
JobContext
WIDTH
=
640
HEIGHT
=
480
source
=
rtc
.
VideoSource
(
WIDTH
,
HEIGHT
)
track
=
rtc
.
LocalVideoTrack
.
create_video_track
(
"example-track"
,
source
)
options
=
rtc
.
TrackPublishOptions
(
# since the agent is a participant, our video I/O is its "camera"
source
=
rtc
.
TrackSource
.
SOURCE_CAMERA
,
simulcast
=
True
,
# when modifying encoding options, max_framerate and max_bitrate must both be set
video_encoding
=
rtc
.
VideoEncoding
(
max_framerate
=
30
,
max_bitrate
=
3_000_000
,
)
,
video_codec
=
rtc
.
VideoCodec
.
H264
,
)
publication
=
await
ctx
.
agent
.
publish_track
(
track
,
options
)
# this color is encoded as ARGB. when passed to VideoFrame it gets re-encoded.
COLOR
=
[
255
,
255
,
0
,
0
]
;
# FFFF0000 RED
async
def
_draw_color
(
)
:
argb_frame
=
bytearray
(
WIDTH
*
HEIGHT
*
4
)
while
True
:
await
asyncio
.
sleep
(
0.1
)
# 10 fps
argb_frame
[
:
]
=
COLOR
*
WIDTH
*
HEIGHT
frame
=
rtc
.
VideoFrame
(
WIDTH
,
HEIGHT
,
rtc
.
VideoBufferType
.
RGBA
,
argb_frame
)
# send this frame to the track
source
.
capture_frame
(
frame
)
asyncio
.
create_task
(
_draw_color
(
)
)
Note
Although the published frame is static, it's still necessary to stream it continuously for the benefit of participants joining the room after the initial frame is sent.
Unlike audio, video
capture_frame
doesn't keep an internal buffer.
LiveKit can translate between video buffer encodings automatically.
VideoFrame
provides the current video buffer type and a method to convert it to any of the other encodings:
Python
async
def
handle_video
(
track
:
rtc
.
Track
)
:
video_stream
=
rtc
.
VideoStream
(
track
)
async
for
event
in
video_stream
:
video_frame
=
event
.
frame
current_type
=
video_frame
.
type
frame_as_bgra
=
video_frame
.
convert
(
rtc
.
VideoBufferType
.
BGRA
)
# [...]
await
video_stream
.
aclose
(
)
@ctx
.
room
.
on
(
"track_subscribed"
)
def
on_track_subscribed
(
track
:
rtc
.
Track
,
publication
:
rtc
.
TrackPublication
,
participant
:
rtc
.
RemoteParticipant
,
)
:
if
track
.
kind
==
rtc
.
TrackKind
.
KIND_VIDEO
:
asyncio
.
create_task
(
handle_video
(
track
)
)
Audio and video synchronization
Note
AVSynchronizer
is currently only available in Python.
While WebRTC handles A/V sync natively, some scenarios require manual synchronization - for example, when synchronizing generated video with voice output.
The
AVSynchronizer
utility helps maintain synchronization by aligning the first audio and video frames. Subsequent frames are automatically synchronized based on configured video FPS and audio sample rate.
Audio and video synchronization
Examples that demonstrate how to synchronize video and audio streams using the
AVSynchronizer
utility.
On this page
Overview
Device permissions
Mute and unmute
Track permissions
Publishing from backend
Publishing audio tracks
Publishing video tracks
Audio and video synchronization


Content from https://docs.livekit.io/home/client/tracks/screenshare:

On this page
Overview
Sharing browser audio
Testing audio sharing
Copy page
See more page options
Overview
LiveKit supports screen sharing natively across all platforms. Your screen is published as a video track, just like your camera. Some platforms support local audio sharing as well.
The steps are somewhat different for each platform:
JavaScript
Swift
Android
Flutter
Unity (WebGL)
// The browser will prompt the user for access and offer a choice of screen, window, or tab
await
room
.
localParticipant
.
setScreenShareEnabled
(
true
)
;
Sharing browser audio
Note
Audio sharing is only possible in certain browsers. Check browser support on the
MDN compatibility table
.
To share audio from a browser tab, you can use the
createScreenTracks
method with the audio option enabled:
const
tracks
=
await
localParticipant
.
createScreenTracks
(
{
audio
:
true
,
}
)
;
tracks
.
forEach
(
(
track
)
=>
{
localParticipant
.
publishTrack
(
track
)
;
}
)
;
Testing audio sharing
Publisher
When sharing audio, make sure you select a
Browser Tab
(not a Window) and ☑️ Share tab audio, otherwise no audio track will be generated when calling
createScreenTracks
:
Subscriber
On the receiving side, you can use
RoomAudioRenderer
to play all audio tracks of the room automatically,
AudioTrack
or your own custom
<audio>
tag to add the track to the page. If you don't hear any sound, check you're receiving the track from the server:
JavaScript
room
.
getParticipantByIdentity
(
'<participant_id>'
)
.
getTrackPublication
(
'screen_share_audio'
)
;
On this page
Overview
Sharing browser audio
Testing audio sharing


Content from https://docs.livekit.io/home/client/tracks/subscribe:

On this page
Overview
Track subscription
Media playback
Volume control
Active speaker identification
Selective subscription
From frontend
From server API
Adaptive stream
Enabling/disabling tracks
Simulcast controls
Copy page
See more page options
Overview
While connected to a room, a participant can receive and render any tracks published to the room. When
autoSubscribe
is enabled (default), the server automatically delivers new tracks to participants, making them ready for rendering.
Track subscription
Rendering media tracks starts with a subscription to receive the track data from the server.
As mentioned in the guide on
rooms, participants, and tracks
, LiveKit models tracks with two constructs:
TrackPublication
and
Track
. Think of a
TrackPublication
as metadata for a track registered with the server and
Track
as the raw media stream. Track publications are always available to the client, even when the track is not subscribed to.
Track subscription callbacks provide your app with both the
Track
and
TrackPublication
objects.
Subscribed callback will be fired on both
Room
and
RemoteParticipant
objects.
JavaScript
React
Swift
Android
Flutter
Python
Rust
Unity
import
{
connect
,
RoomEvent
}
from
'livekit-client'
;
room
.
on
(
RoomEvent
.
TrackSubscribed
,
handleTrackSubscribed
)
;
function
handleTrackSubscribed
(
track
:
RemoteTrack
,
publication
:
RemoteTrackPublication
,
participant
:
RemoteParticipant
,
)
{
/* Do things with track, publication or participant */
}
Note
This guide is focused on frontend applications. To consume media in your backend, use the
LiveKit Agents framework
or SDKs for
Go
,
Rust
,
Python
, or
Node.js
.
Media playback
Once subscribed to an audio or video track, it's ready to be played in your application
JavaScript
React
React Native
Swift
Android
Flutter
Unity (WebGL)
function
handleTrackSubscribed
(
track
:
RemoteTrack
,
publication
:
RemoteTrackPublication
,
participant
:
RemoteParticipant
,
)
{
// Attach track to a new HTMLVideoElement or HTMLAudioElement
const
element
=
track
.
attach
(
)
;
parentElement
.
appendChild
(
element
)
;
// Or attach to existing element
// track.attach(element)
}
Volume control
Audio tracks support a volume between 0 and 1.0, with a default value of 1.0. You can adjust the volume if necessary be setting the volume property on the track.
JavaScript
Swift
Android
Flutter
track
.
setVolume
(
0.5
)
;
Active speaker identification
LiveKit can automatically detect participants who are actively speaking and send updates when their speaking status changes. Speaker updates are sent for both local and remote participants. These events fire on both Room and Participant objects, allowing you to identify active speakers in your UI.
JavaScript
React
React Native
Swift
Android
Flutter
Unity (WebGL)
room
.
on
(
RoomEvent
.
ActiveSpeakersChanged
,
(
speakers
:
Participant
[
]
)
=>
{
// Speakers contain all of the current active speakers
}
)
;
participant
.
on
(
ParticipantEvent
.
IsSpeakingChanged
,
(
speaking
:
boolean
)
=>
{
console
.
log
(
`
${
participant
.
identity
}
is
${
speaking
?
'now'
:
'no longer'
}
speaking. audio level:
${
participant
.
audioLevel
}
`
,
)
;
}
)
;
Selective subscription
Disable
autoSubscribe
to take manual control over which tracks the participant should subscribe to. This is appropriate for spatial applications and/or applications that require precise control over what each participant receives.
Both LiveKit's SDKs and server APIs have controls for selective subscription. Once configured, only explicitly subscribed tracks are delivered to the participant.
From frontend
JavaScript
Swift
Android
Flutter
Python
Unity (WebGL)
let
room
=
await
room
.
connect
(
url
,
token
,
{
autoSubscribe
:
false
,
}
)
;
room
.
on
(
RoomEvent
.
TrackPublished
,
(
publication
,
participant
)
=>
{
publication
.
setSubscribed
(
true
)
;
}
)
;
// Also subscribe to tracks published before participant joined
room
.
remoteParticipants
.
forEach
(
(
participant
)
=>
{
participant
.
trackPublications
.
forEach
(
(
publication
)
=>
{
publication
.
setSubscribed
(
true
)
;
}
)
;
}
)
;
From server API
These controls are also available with the server APIs.
Node.js
Go
import
{
RoomServiceClient
}
from
'livekit-server-sdk'
;
const
roomServiceClient
=
new
RoomServiceClient
(
'myhost'
,
'api-key'
,
'my secret'
)
;
// Subscribe to new track
roomServiceClient
.
updateSubscriptions
(
'myroom'
,
'receiving-participant-identity'
,
[
'TR_TRACKID'
]
,
true
)
;
// Unsubscribe from existing track
roomServiceClient
.
updateSubscriptions
(
'myroom'
,
'receiving-participant-identity'
,
[
'TR_TRACKID'
]
,
false
)
;
Adaptive stream
In an application, video elements where tracks are rendered could vary in size, and sometimes hidden. It would be extremely wasteful to fetch high-resolution videos but only to render it in a 150x150 box.
Adaptive stream allows a developer to build dynamic video applications without consternation for how interface design or user interaction might impact video quality. It allows us to fetch the minimum bits necessary for high-quality rendering and helps with scaling to very large sessions.
When adaptive stream is enabled, the LiveKit SDK will monitor both size and visibility of the UI elements that the tracks are attached to. Then it'll automatically coordinate with the server to ensure the closest-matching simulcast layer that matches the UI element is sent back. If the element is hidden, the SDK will automatically pause the associated track on the server side until the element becomes visible.
Note
With JS SDK, you must use
Track.attach()
in order for adaptive stream to be effective.
Enabling/disabling tracks
Implementations seeking fine-grained control can enable or disable tracks at their discretion. This could be used to implement subscriber-side mute. (for example, muting a publisher in the room, but only for the current user).
When disabled, the participant will not receive any new data for that track. If a disabled track is subsequently enabled, new data will be received again.
The
disable
action is useful when optimizing for a participant's bandwidth consumption. For example, if a particular user's video track is offscreen, disabling this track will reduce bytes from being sent by the LiveKit server until the track's data is needed again. (this is not needed with adaptive stream)
JavaScript
Swift
Android
Flutter
Unity (WebGL)
import
{
connect
,
RoomEvent
}
from
'livekit-client'
;
room
.
on
(
RoomEvent
.
TrackSubscribed
,
handleTrackSubscribed
)
;
function
handleTrackSubscribed
(
track
:
RemoteTrack
,
publication
:
RemoteTrackPublication
,
participant
:
RemoteParticipant
,
)
{
publication
.
setEnabled
(
false
)
;
}
Note
You may be wondering how
subscribe
and
unsubscribe
differs from
enable
and
disable
. A track must be subscribed to and enabled for data to be received by the participant. If a track has not been subscribed to (or was unsubscribed) or disabled, the participant performing these actions will not receive that track's data.
The difference between these two actions is
negotiation
. Subscribing requires a negotiation handshake with the LiveKit server, while enable/disable does not. Depending on one's use case, this can make enable/disable more efficient, especially when a track may be turned on or off frequently.
Simulcast controls
If a video track has simulcast enabled, a receiving participant may want to manually specify the maximum receivable quality. This would result a quality and bandwidth reduction for the target track. This might come in handy, for instance, when an application's user interface is displaying a small thumbnail for a particular user's video track.
JavaScript
Swift
Android
Flutter
Unity (WebGL)
import
{
connect
,
RoomEvent
}
from
'livekit-client'
;
connect
(
'ws://your_host'
,
token
,
{
audio
:
true
,
video
:
true
,
}
)
.
then
(
(
room
)
=>
{
room
.
on
(
RoomEvent
.
TrackSubscribed
,
handleTrackSubscribed
)
;
}
)
;
function
handleTrackSubscribed
(
track
:
RemoteTrack
,
publication
:
RemoteTrackPublication
,
participant
:
RemoteParticipant
,
)
{
if
(
track
.
kind
===
Track
.
Kind
.
Video
)
{
publication
.
setVideoQuality
(
VideoQuality
.
LOW
)
;
}
}
On this page
Overview
Track subscription
Media playback
Volume control
Active speaker identification
Selective subscription
From frontend
From server API
Adaptive stream
Enabling/disabling tracks
Simulcast controls


Content from https://docs.livekit.io/home/client/tracks/noise-cancellation:

On this page
Overview
Copy page
See more page options
Overview
Your user's microphone is likely to pick up undesirable audio including background noise (like traffic, music, voices, etc) and might
also pick up echoes from their own speakers. In both cases, this noise leads to a poor experience for other participants in a call.
In voice AI apps, this can also interfere with turn detection or degrade the quality of transcriptions, both of which are critical to a
good user experience.
LiveKit includes default outbound noise and echo cancellation based on the underlying open source WebRTC implementations of
echoCancellation
and
noiseSuppression
. You can adjust these settings
with the
AudioCaptureOptions
type in the LiveKit SDKs during connection.
LiveKit Cloud includes
enhanced noise cancellation
for the best possible audio quality, including a background voice cancellation (BVC) model that is optimized for voice AI applications.
To hear the effects of the various noise removal options, play the samples below:
Original
WebRTC noiseSuppression
LiveKit Cloud enhanced noise cancellation
On this page
Overview


Content from https://docs.livekit.io/home/client/tracks/encryption:

On this page
Overview
How E2EE works
Key distribution
Limitations
Implementation guide
Using a custom key provider
Copy page
See more page options
Overview
LiveKit includes built-in support for end-to-end encryption (E2EE) on realtime audio and video tracks. With E2EE enabled, media data remains fully encrypted from sender to receiver, ensuring that no intermediaries (including LiveKit servers) can access or modify the content. This feature is:
Available for both self-hosted and LiveKit Cloud customers at no additional cost.
Ideal for regulated industries and security-critical applications.
Designed to provide an additional layer of protection beyond standard transport encryption.
Note
Security is our highest priority. Learn more about
our comprehensive approach to security
.
How E2EE works
E2EE is enabled at the room level and automatically applied to all media tracks from all participants in that room. You must enable it within the LiveKit SDK for each participant. In many cases you can use a built-in key provider with a single shared key for the whole room. If you require unique keys for each participant, or key rotation during the lifetime of a single room, you can implement your own key provider.
Key distribution
It is your responsibility to securely generate, store, and distribute encryption keys to your application at runtime. LiveKit does not (and cannot) store or transport encryption keys for you.
If using a shared key, you would typically generate it on your server at the same time that you create a room and distribute it securely to participants alongside their access token for the room. When using unique keys per participant, you may need a more sophisticated method for distributing keys as new participants join the room. Remember that the key is needed for both encryption and decryption, so even when using per-participant keys, you must ensure that all participants have all keys.
Limitations
All LiveKit network traffic is encrypted using TLS, but full end-to-end encryption applies only to media tracks and is not applied to realtime data, text, API calls, or other signaling.
Implementation guide
These examples show how to use the built-in key provider with a shared key. If you need to use a custom key provider, see the section below.
JavaScript
iOS
Android
Flutter
React Native
Python
Node.js
// 1. Initialize the external key provider
const
keyProvider
=
new
ExternalE2EEKeyProvider
(
)
;
// 2. Configure room options
const
roomOptions
:
RoomOptions
=
{
e2ee
:
{
keyProvider
:
keyProvider
,
// Required for web implementations
worker
:
new
Worker
(
new
URL
(
'livekit-client/e2ee-worker'
,
import
.
meta
.
url
)
)
,
}
,
}
;
// 3. Create and configure the room
const
room
=
new
Room
(
roomOptions
)
;
// 4. Set your externally distributed encryption key
await
keyProvider
.
setKey
(
yourSecureKey
)
;
// 5. Enable E2EE for all local tracks
await
room
.
setE2EEEnabled
(
true
)
;
// 6. Connect to the room
await
room
.
connect
(
url
,
token
)
;
Example implementation
For a production-ready implementation, refer to our
Meet example app
which demonstrates E2EE in a production-grade application using the
ExternalE2EEKeyProvider
.
Using a custom key provider
If your application requires key rotation during the lifetime of a single room or unique keys per participant (such as when implementing the
MEGOLM
or
MLS
protocol), you'll need to implement your own key provider.  The full details of that are beyond the scope of this guide, but a brief outline for the JS SDK is provided below (the process is similar in the other SDKs as well):
Extend the
BaseKeyProvider
class.
Call
onSetEncryptionKey
with each key/identity pair
Set appropriate ratcheting options (
ratchetSalt
,
ratchetWindowSize
,
failureTolerance
,
keyringSize
).
Implement the
onKeyRatcheted
method to handle key updates.
Call
ratchetKey()
when key rotation is needed.
Pass your custom key provider in the room options, in place of the built-in key provider.
On this page
Overview
How E2EE works
Key distribution
Limitations
Implementation guide
Using a custom key provider


Content from https://docs.livekit.io/home/client/tracks/advanced:

On this page
Video codec support
Video quality presets
Video track configuration
Video simulcast
Dynacast
Hi-fi audio
Audio RED
Copy page
See more page options
Video codec support
LiveKit supports multiple video codecs to suit different application needs:
H.264
VP8
VP9 (including SVC)
AV1 (including SVC)
Scalable Video Coding (SVC) is a feature of newer codecs like VP9 and AV1 that provides the following benefits:
Improves bitrate efficiency by letting higher quality layers leverage information from lower quality layers.
Enables instant layer switching without waiting for keyframes.
Incorporates multiple spatial (resolution) and temporal (frame rate) layers in a single stream.
When using VP9 or AV1, SVC is automatically activated with L3T3_KEY
scalabilityMode
(three spatial and temporal layers).
You can specify which codec to use when connecting to a room. To learn more, see the examples in the following sections.
Video quality presets
LiveKit provides preset resolutions when creating video tracks. These presets include common resolutions and aspect ratios:
h720 (1280x720)
h540 (960x540)
h360 (640x360)
h180 (320x180)
The presets also include recommended bitrates and framerates for optimal quality. You can use these presets or define custom parameters based on your needs.
React
JavaScript
const
localParticipant
=
useLocalParticipant
(
)
;
const
audioTrack
=
await
createLocalAudioTrack
(
)
;
const
audioPublication
=
await
localParticipant
.
publishTrack
(
audioTrack
,
{
red
:
false
,
}
)
;
Video track configuration
LiveKit provides extensive control over video track settings through two categories:
Capture settings: Device selection and capabilities (resolution, framerate, facing mode).
Publish settings: Encoding parameters (bitrate, framerate, simulcast layers).
Here's how to configure these settings:
JavaScript
Swift
// Room defaults
const
room
=
new
Room
(
{
videoCaptureDefaults
:
{
deviceId
:
''
,
facingMode
:
'user'
,
resolution
:
{
width
:
1280
,
height
:
720
,
frameRate
:
30
,
}
,
}
,
publishDefaults
:
{
videoEncoding
:
{
maxBitrate
:
1_500_000
,
maxFramerate
:
30
,
}
,
videoSimulcastLayers
:
[
{
width
:
640
,
height
:
360
,
encoding
:
{
maxBitrate
:
500_000
,
maxFramerate
:
20
,
}
,
}
,
{
width
:
320
,
height
:
180
,
encoding
:
{
maxBitrate
:
150_000
,
maxFramerate
:
15
,
}
,
}
,
]
,
}
,
}
)
;
// Individual track settings
const
videoTrack
=
await
createLocalVideoTrack
(
{
facingMode
:
'user'
,
resolution
:
VideoPresets
.
h720
,
}
)
;
const
publication
=
await
room
.
localParticipant
.
publishTrack
(
videoTrack
)
;
Video simulcast
Simulcast enables publishing multiple versions of the same video track with different bitrate profiles. This allows LiveKit to dynamically forward the most suitable stream based on each recipient's bandwidth and preferred resolution.
LiveKit will automatically select appropriate layers when it detects bandwidth constraints, upgrading to higher resolutions as conditions improve.
Simulcast is enabled by default in all LiveKit SDKs and can be disabled in publish settings if needed.
Dynacast
Dynamic broadcasting (Dynacast) automatically pauses video layer publication when they aren't being consumed by subscribers. For simulcasted video, if subscribers only use medium and low-resolution layers, the high-resolution publication is paused.
To enable this bandwidth optimization:
JavaScript
Swift
Android
Flutter
const
room
=
new
Room
(
{
dynacast
:
true
}
)
;
With SVC codecs (VP9 and AV1), Dynacast can only pause entire streams, not individual layers, due to SVC encoding characteristics.
Hi-fi audio
For high-quality audio streaming, LiveKit provides several configuration options to optimize audio quality.
Recommended hi-fi settings
For high-quality audio, we provide a preset with our recommended settings:
React
JavaScript
const
localParticipant
=
useLocalParticipant
(
)
;
const
audioTrack
=
await
createLocalAudioTrack
(
{
channelCount
:
2
,
echoCancellation
:
false
,
noiseSuppression
:
false
,
}
)
;
const
audioPublication
=
await
localParticipant
.
publishTrack
(
audioTrack
,
{
audioPreset
:
AudioPresets
.
musicHighQualityStereo
,
dtx
:
false
,
red
:
false
,
}
)
;
Maximum quality settings
LiveKit supports audio tracks up to 510kbps stereo - the highest theoretical quality possible. Note that the listener's playback stack may resample the audio, so actual playback quality may be lower than published quality. For comparison, 256kbps AAC-encoded audio is considered high quality for music streaming services like Spotify.
React
JavaScript
const
localParticipant
=
useLocalParticipant
(
)
;
const
audioTrack
=
await
createLocalAudioTrack
(
{
channelCount
:
2
,
echoCancellation
:
false
,
noiseSuppression
:
false
,
}
)
;
const
audioPublication
=
await
localParticipant
.
publishTrack
(
audioTrack
,
{
audioBitrate
:
510000
,
dtx
:
false
,
red
:
false
,
}
)
;
If you configure a high bitrate, we recommend testing under real-world conditions to find what settings work best for your use case.
Audio RED
REDundant Encoding is a technique to improve audio quality by sending multiple copies of the same audio data in different packets. This is useful in lossy networks where packets may be dropped. The receiver can then use the redundant packets to reconstruct the original audio packet.
Redundant encoding increases bandwidth usage in order to achieve higher audio quality. LiveKit recommends enabling this feature because audio glitches are so distracting that the tradeoff is almost always worth it. If your use case prioritizes bandwidth and can tolerate audio glitches, you can disable RED.
Disabling Audio RED when publishing
You can disable Audio RED when publishing new audio tracks:
React
JavaScript
Swift
Android
const
localParticipant
=
useLocalParticipant
(
)
;
const
audioTrack
=
await
createLocalAudioTrack
(
)
;
const
audioPublication
=
await
localParticipant
.
publishTrack
(
audioTrack
,
{
red
:
false
,
}
)
;
On this page
Video codec support
Video quality presets
Video track configuration
Video simulcast
Dynacast
Hi-fi audio
Audio RED


Content from https://docs.livekit.io/home/client/data/text-streams:

On this page
Overview
Sending all at once
Streaming incrementally
Handling incoming streams
Stream properties
Concurrency
Joining mid-stream
No message persistence
Chat components
Copy page
See more page options
Overview
Text streams provide a simple way to send text between participants in realtime, supporting use cases such as chat, streamed LLM responses, and more. Each individual stream is associated with a topic, and you must register a handler to receive incoming streams for that topic. Streams can target specific participants or the entire room.
To send other kinds of data, use
byte streams
instead.
Sending all at once
Use the
sendText
method when the whole string is available up front. The input string is automatically chunked and streamed so there is no limit on string size.
JavaScript
Swift
Python
Rust
Node.js
Go
const
text
=
'Lorem ipsum dolor sit amet...'
;
const
info
=
await
room
.
localParticipant
.
sendText
(
text
,
{
topic
:
'my-topic'
,
}
)
;
console
.
log
(
`
Sent text with stream ID:
${
info
.
id
}
`
)
;
Streaming incrementally
If your text is generated incrementally, use
streamText
to open a stream writer. You must explicitly close the stream when you are done sending data.
JavaScript
Swift
Python
Rust
Node.js
Go
const
streamWriter
=
await
room
.
localParticipant
.
streamText
(
{
topic
:
'my-topic'
,
}
)
;
console
.
log
(
`
Opened text stream with ID:
${
streamWriter
.
info
.
id
}
`
)
;
// In a real app, you would generate this text asynchronously / incrementally as well
const
textChunks
=
[
"Lorem "
,
"ipsum "
,
"dolor "
,
"sit "
,
"amet..."
]
for
(
const
chunk
of
textChunks
)
{
await
streamWriter
.
write
(
chunk
)
}
// The stream must be explicitly closed when done
await
streamWriter
.
close
(
)
;
console
.
log
(
`
Closed text stream with ID:
${
streamWriter
.
info
.
id
}
`
)
;
Handling incoming streams
Whether the data was sent with
sendText
or
streamText
, it is always received as a stream. You must register a handler to receive it.
JavaScript
Swift
Python
Rust
Node.js
Go
room
.
registerTextStreamHandler
(
'my-topic'
,
(
reader
,
participantInfo
)
=>
{
const
info
=
reader
.
info
;
console
.
log
(
`
Received text stream from
${
participantInfo
.
identity
}
\n
`
+
`
Topic:
${
info
.
topic
}
\n
`
+
`
Timestamp:
${
info
.
timestamp
}
\n
`
+
`
ID:
${
info
.
id
}
\n
`
+
`
Size:
${
info
.
size
}
`
// Optional, only available if the stream was sent with `sendText`
)
;
// Option 1: Process the stream incrementally using a for-await loop.
for
await
(
const
chunk
of
reader
)
{
console
.
log
(
`
Next chunk:
${
chunk
}
`
)
;
}
// Option 2: Get the entire text after the stream completes.
const
text
=
await
reader
.
readAll
(
)
;
console
.
log
(
`
Received text:
${
text
}
`
)
;
}
)
;
Stream properties
These are all of the properties available on a text stream, and can be set from the send/stream methods or read from the handler.
Property
Description
Type
id
Unique identifier for this stream.
string
topic
Topic name used to route the stream to the appropriate handler.
string
timestamp
When the stream was created.
number
size
Total expected size in bytes (UTF-8), if known.
number
attributes
Additional attributes as needed for your application.
string dict
destinationIdentities
Identities of the participants to send the stream to. If empty, is sent to all.
array
Concurrency
Multiple streams can be written or read concurrently. If you call
sendText
or
streamText
multiple times on the same topic, the recipient's handler will be invoked multiple times, once for each stream. These invocations will occur in the same order as the streams were opened by the sender, and the stream readers will be closed in the same order in which the streams were closed by the sender.
Joining mid-stream
Participants who join a room after a stream has been initiated will not receive any of it. Only participants connected at the time the stream is opened are eligible to receive it.
No message persistence
LiveKit does not include long-term persistence for text streams. All data is transmitted in real-time between connected participants only. If you need message history, you'll need to implement storage yourself using a database or other persistence layer.
Chat components
LiveKit provides pre-built React components for common text streaming use cases like chat. For details, see the
Chat component
and
useChat hook
.
Note
Streams are a simple and powerful way to send text, but if you need precise control over individual packet behavior, the lower-level
data packets
API may be more appropriate.
On this page
Overview
Sending all at once
Streaming incrementally
Handling incoming streams
Stream properties
Concurrency
Joining mid-stream
No message persistence
Chat components


Content from https://docs.livekit.io/home/client/data/byte-streams:

On this page
Overview
Sending files
Streaming bytes
Handling incoming streams
Stream properties
Concurrency
Joining mid-stream
Chunk sizes
Copy page
See more page options
Overview
Byte streams provide a simple way to send files, images, or other binary data between participants in realtime. Each individual stream is associated with a topic, and you must register a handler to receive incoming streams for that topic. Streams can target specific participants or the entire room.
To send text data, use
text streams
instead.
Sending files
To send a file or an image, use the
sendFile
method. Precise support varies by SDK, as this is integrated with the platform's own file APIs.
JavaScript
Swift
Python
Rust
Node.js
Go
// Send a `File` object
const
file
=
(
$
(
'file'
)
as
HTMLInputElement
)
.
files
?.
[
0
]
!
;
const
info
=
await
room
.
localParticipant
.
sendFile
(
file
,
{
mimeType
:
file
.
type
,
topic
:
'my-topic'
,
// Optional, allows progress to be shown to the user
onProgress
:
(
progress
)
=>
console
.
log
(
'sending file, progress'
,
Math
.
ceil
(
progress
*
100
)
)
,
}
)
;
console
.
log
(
`
Sent file with stream ID:
${
info
.
id
}
`
)
;
Streaming bytes
To stream any kind of binary data, open a stream writer with the
streamBytes
method. You must explicitly close the stream when you are done sending data.
Swift
Python
Rust
Node.js
Go
let
writer
=
try
await
room
.
localParticipant
.
streamBytes
(
for
:
"my-topic"
)
print
(
"Opened byte stream with ID:
\(
writer
.
info
.
id
)
"
)
// Example sending arbitrary binary data
// For sending files, use `sendFile` instead
let
dataChunks
=
[
Data
(
[
0x00
,
0x01
]
)
,
Data
(
[
0x03
,
0x04
]
)
]
for
chunk
in
dataChunks
{
try
await
writer
.
write
(
chunk
)
}
// The stream must be explicitly closed when done
try
await
writer
.
close
(
)
print
(
"Closed byte stream with ID:
\(
writer
.
info
.
id
)
"
)
Handling incoming streams
Whether the data was sent as a file or a stream, it is always received as a stream. You must register a handler to receive it.
JavaScript
Swift
Python
Rust
Node.js
Go
room
.
registerByteStreamHandler
(
'my-topic'
,
(
reader
,
participantInfo
)
=>
{
const
info
=
reader
.
info
;
// Optional, allows you to display progress information if the stream was sent with `sendFile`
reader
.
onProgress
=
(
progress
)
=>
{
console
.
log
(
`
"progress
${
progress
?
(
progress
*
100
)
.
toFixed
(
0
)
:
'undefined'
}
%
`
)
;
}
;
// Option 1: Process the stream incrementally using a for-await loop.
for
await
(
const
chunk
of
reader
)
{
// Collect these however you want.
console
.
log
(
`
Next chunk:
${
chunk
}
`
)
;
}
// Option 2: Get the entire file after the stream completes.
const
result
=
new
Blob
(
await
reader
.
readAll
(
)
,
{
type
:
info
.
mimeType
}
)
;
console
.
log
(
`
File "
${
info
.
name
}
" received from
${
participantInfo
.
identity
}
\n
`
+
`
Topic:
${
info
.
topic
}
\n
`
+
`
Timestamp:
${
info
.
timestamp
}
\n
`
+
`
ID:
${
info
.
id
}
\n
`
+
`
Size:
${
info
.
size
}
`
// Optional, only available if the stream was sent with `sendFile`
)
;
}
)
;
Stream properties
These are all of the properties available on a text stream, and can be set from the send/stream methods or read from the handler.
Property
Description
Type
id
Unique identifier for this stream.
string
topic
Topic name used to route the stream to the appropriate handler.
string
timestamp
When the stream was created.
number
mimeType
The MIME type of the stream data. Auto-detected for files, otherwise defaults to
application/octet-stream
.
string
name
The name of the file being sent.
string
size
Total expected size in bytes, if known.
number
attributes
Additional attributes as needed for your application.
string dict
destinationIdentities
Identities of the participants to send the stream to. If empty, will be sent to all.
array
Concurrency
Multiple streams can be written or read concurrently. If you call
sendFile
or
streamBytes
multiple times on the same topic, the recipient's handler will be invoked multiple times, once for each stream. These invocations will occur in the same order as the streams were opened by the sender, and the stream readers will be closed in the same order in which the streams were closed by the sender.
Joining mid-stream
Participants who join a room after a stream has been initiated will not receive any of it. Only participants connected at the time the stream is opened are eligible to receive it.
Chunk sizes
The processes for writing and reading streams are optimized separately. This means the number and size of chunks sent may not match the number and size of those received. However, the full data received is guaranteed to be complete and in order. Chunks are generally smaller than 15kB.
Note
Streams are a simple and powerful way to send data, but if you need precise control over individual packet behavior, the lower-level
data packets
API may be more appropriate.
On this page
Overview
Sending files
Streaming bytes
Handling incoming streams
Stream properties
Concurrency
Joining mid-stream
Chunk sizes


Content from https://docs.livekit.io/home/client/data/rpc:

On this page
Overview
Method registration
Method invocation
Method names
Payload format
Response timeout
Errors
Copy page
See more page options
Overview
With RPC your application can define methods on one participant that can be invoked remotely by other participants within a room, and may return a response. This feature can be used to request data, coordinate application-specific state, and more. When used to
forward tool calls
from an AI Agent, your LLM can directly access data or manipulate UI in your application's frontend.
Method registration
First register the method at the destination participant with
localParticipant.registerRpcMethod
and provide the method's name and a handler function.  Any number of methods can be registered on a single participant.
JavaScript
Python
Node.js
Rust
Android
Swift
Go
localParticipant
.
registerRpcMethod
(
'greet'
,
async
(
data
:
RpcInvocationData
)
=>
{
console
.
log
(
`
Received greeting from
${
data
.
callerIdentity
}
:
${
data
.
payload
}
`
)
;
return
`
Hello,
${
data
.
callerIdentity
}
!
`
;
}
)
;
Method invocation
Use
localParticipant.performRpc
to invoke the registered RPC method on a remote participant by providing the destination participant's identity, the method name, and the payload. This is an asynchronous operation that returns a string, and may raise an error.
JavaScript
Python
Node.js
Rust
Android
Swift
Go
try
{
const
response
=
await
localParticipant
.
performRpc
(
{
destinationIdentity
:
'recipient-identity'
,
method
:
'greet'
,
payload
:
'Hello from RPC!'
,
}
)
;
console
.
log
(
'RPC response:'
,
response
)
;
}
catch
(
error
)
{
console
.
error
(
'RPC call failed:'
,
error
)
;
}
Method names
Method names can be any string, up to 64 bytes long (UTF-8).
Payload format
RPC requests and responses both support a string payload, with a maximum size of 15KiB (UTF-8). You may use any format that makes sense, such as JSON or base64-encoded data.
Response timeout
performRpc
uses a timeout to hang up automatically if the response takes too long. The default timeout is 10 seconds, but you are free to change it as needed in your
performRpc
call. In general, you should set a timeout that is as short as possible while still satisfying your use case.
The timeout you set is used for the entire duration of the request, including network latency. This means the timeout the handler is provided will be shorter than the overall timeout.
Errors
performRpc
will return certain built-in errors (detailed below), or your own custom errors generated in your remote method handler.
To return a custom error to the caller, handlers should throw an error of the type
RpcError
with the following properties:
code
: A number that indicates the type of error. Codes 1001-1999 are reserved for LiveKit internal errors.
message
: A string that provides a readable description of the error.
data
: An optional string that provides even more context about the error, with the same format and limitations as request/response payloads.
Any other error thrown in a handler will be caught and the caller will receive a generic
1500 Application Error
.
Built-in error types
Code
Name
Description
1400
UNSUPPORTED_METHOD
Method not supported at destination
1401
RECIPIENT_NOT_FOUND
Recipient not found
1402
REQUEST_PAYLOAD_TOO_LARGE
Request payload too large
1403
UNSUPPORTED_SERVER
RPC not supported by server
1404
UNSUPPORTED_VERSION
Unsupported RPC version
1500
APPLICATION_ERROR
Application error in method handler
1501
CONNECTION_TIMEOUT
Connection timeout
1502
RESPONSE_TIMEOUT
Response timeout
1503
RECIPIENT_DISCONNECTED
Recipient disconnected
1504
RESPONSE_PAYLOAD_TOO_LARGE
Response payload too large
1505
SEND_FAILED
Failed to send
On this page
Overview
Method registration
Method invocation
Method names
Payload format
Response timeout
Errors


Content from https://docs.livekit.io/home/client/data/packets:

On this page
Overview
Delivery options
Size limits
Selective delivery
Topic
Usage
Copy page
See more page options
Overview
Use
LocalParticipant.publishData
or
RoomService.SendData
to send individual packets of data to one or more participants in a room.
Note
This is a low-level API meant for advanced control over individual packet behavior. For most use cases, consider using the higher-level
text streams
,
byte streams
, or
RPC
features.
Delivery options
LiveKit offers two forms of packet delivery:
Reliable
: Packets are delivered in order, with automatic retransmission in the case of packet loss. This is preferable for scenarios where delivery is prioritized over latency, such as in-room chat.
Lossy
: Each packet is sent once, with no ordering guarantee. This is ideal for realtime updates where speed of delivery is a priority.
Note
Reliable delivery indicates "best-effort" delivery. It cannot fully guarantee the packet will be delivered in all cases. For instance, a receiver that is temporarily disconnected at the moment the packet is sent will not receive it. Packets are not buffered on the server and only a limited number of retransmissions are attempted.
Size limits
In the
reliable
delivery mode, each packet can be up to 15KiB in size. The protocol limit is 16KiB for the entire data packet, but LiveKit adds various headers to properly route the packets which reduces the space available for user data.
While some platforms might support larger packet sizes without returning an error, LiveKit recommends this 16KiB limit to maximize compatibility across platforms and address limitations of the Stream Control Transmission Protocol (SCTP).  To learn more, see
Understanding message size limits
.
In the
lossy
delivery mode, LiveKit recommends even smaller data packets - just 1300 bytes maximum - to stay within the network Maximum Transmit Unit (MTU) of 1400 bytes. Larger packets are fragmented into multiple packets and if any single packet is lost, the whole packet is lost with it.
Selective delivery
Packets can be sent either to the entire room or to a subset of participants with the
destinationIdentities
parameter on the
publishData
call. To send to the entire room, leave
destinationIdentities
blank.
Topic
You may have different types and purposes of data packets. To easily differentiate, set the
topic
field to any string that makes sense for your application.
For example, in a realtime multiplayer game, you might use different topics for chat messages, character position updates, and environment updates.
Usage
JavaScript
Swift
Kotlin
Flutter
Python
Go
Unity
const
strData
=
JSON
.
stringify
(
{
some
:
"data"
}
)
const
encoder
=
new
TextEncoder
(
)
const
decoder
=
new
TextDecoder
(
)
// publishData takes in a Uint8Array, so we need to convert it
const
data
=
encoder
.
encode
(
strData
)
;
// Publish lossy data to the entire room
room
.
localParticipant
.
publishData
(
data
,
{
reliable
:
false
}
)
// Publish reliable data to a set of participants
room
.
localParticipant
.
publishData
(
data
,
{
reliable
:
true
,
destinationIdentities
:
[
'my-participant-identity'
]
}
)
// Receive data from other participants
room
.
on
(
RoomEvent
.
DataReceived
,
(
payload
:
Uint8Array
,
participant
:
Participant
,
kind
:
DataPacket_Kind
)
=>
{
const
strData
=
decoder
.
decode
(
payload
)
...
}
)
On this page
Overview
Delivery options
Size limits
Selective delivery
Topic
Usage


Content from https://docs.livekit.io/home/client/state/participant-attributes:

On this page
Overview
Deleting attributes
Update frequency
Size limits
Usage from LiveKit SDKs
Usage from server APIs
Copy page
See more page options
Overview
Each LiveKit participant has two fields for application-specific state:
Participant.attributes
: A string key-value store
Participant.metadata
: A single string that can store any data.
These fields are stored and managed by the LiveKit server, and are automatically synchronized to new participants who join the room later.
Initial values can be set in the participant’s
access token
, ensuring the value is immediately available when the participant connects.
While the metadata field is a single string, the attributes field is a key-value store. This allows fine-grained updates to different parts of the state without affecting or transmitting the values of other keys.
Deleting attributes
To delete an attribute key, set its value to an empty string (
''
).
Update frequency
Attributes and metadata are not suitable for high-frequency updates (more than once every few seconds) due to synchronization overhead on the server. If you need to send updates more frequently, consider using
data packets
instead.
Size limits
Metadata and attributes each have a 64 KiB limit. For attributes, this limit includes the combined size of all keys and values.
Usage from LiveKit SDKs
The LiveKit SDKs receive events on attributes and metadata changes for both the local participant and any remote participants in the room. See
Handling events
for more information.
Participants must have the
canUpdateOwnMetadata
permission in their access token to update their own attributes or metadata.
JavaScript
React
Swift
Kotlin
Flutter
Python
// receiving changes
room
.
on
(
RoomEvent
.
ParticipantAttributesChanged
,
(
changed
:
Record
<
string
,
string
>
,
participant
:
Participant
)
=>
{
console
.
log
(
'participant attributes changed'
,
changed
,
'all attributes'
,
participant
.
attributes
,
)
;
}
,
)
;
room
.
on
(
RoomEvent
.
ParticipantMetadataChanged
,
(
oldMetadata
:
string
|
undefined
,
participant
:
Participant
)
=>
{
console
.
log
(
'metadata changed from'
,
oldMetadata
,
participant
.
metadata
)
;
}
,
)
;
// updating local participant
room
.
localParticipant
.
setAttributes
(
{
myKey
:
'myValue'
,
myOtherKey
:
'otherValue'
,
}
)
;
room
.
localParticipant
.
setMetadata
(
JSON
.
stringify
(
{
some
:
'values'
,
}
)
,
)
;
Usage from server APIs
From the server side, you can update attributes or metadata of any participant in the room using the
RoomService.UpdateParticipant
API.
Node.js
Go
Python
Ruby
Java/Kotlin
import
{
RoomServiceClient
}
from
'livekit-server-sdk'
;
const
roomServiceClient
=
new
RoomServiceClient
(
'myhost'
,
'api-key'
,
'my secret'
)
;
roomServiceClient
.
updateParticipant
(
'room'
,
'identity'
,
{
attributes
:
{
myKey
:
'myValue'
,
}
,
metadata
:
'updated metadata'
,
}
)
;
On this page
Overview
Deleting attributes
Update frequency
Size limits
Usage from LiveKit SDKs
Usage from server APIs


Content from https://docs.livekit.io/home/client/state/room-metadata:

On this page
Overview
Copy page
See more page options
Overview
Similar to
Participant metadata
, Rooms also feature a metadata field for application-specific data which is visible to all participants.
Room metadata can only be set using the server APIs, but can be accessed by all participants in the room using the LiveKit SDKs.
To set room metadata, use the
CreateRoom
and
UpdateRoomMetadata
APIs.
To subscribe to updates, you must
handle
the
RoomMetadataChanged
event.
On this page
Overview


Content from https://docs.livekit.io/agents/v0/integrations/openai/realtime:

Try the playground
On this page
Quickstarts
OpenAI Realtime API and LiveKit
How it works
The Agents framework
LiveKit concepts
MultimodalAgent
Agents 1.0 available for Python
This documentation is for v0.x of the LiveKit Agents framework.
See updated documentation here:
OpenAI Realtime API integration guide
.
v1.0 for Node.js is coming soon.
Quickstarts
OpenAI Playground
Experiment with OpenAI's Realtime API in the playground with personalities like the
Snarky Teenager
or
Opera Singer
.
Speech to speech
Use OpenAI's Realtime API in this quickstart guide to create a speech-to-speech agent.
OpenAI
Realtime
OpenAI Realtime API and LiveKit
OpenAI's Realtime API is a WebSocket interface for low-latency audio streaming, best suited for server-to-server use rather than direct consumption by end-user devices.
LiveKit offers Python and Node.js integrations for the API, enabling developers to build realtime conversational AI applications using LiveKit's Agents framework. This framework integrates with LiveKit's SDKs and telephony solutions, allowing you to build applications for any platform.
How it works
WebSocket is not ideal for realtime audio and video over long distances or slower networks. LiveKit bridges this gap by converting the transport to WebRTC and routing data through our global edge network to minimize transmission latency.
With the Agents framework, user audio is first transmitted to LiveKit's edge network via WebRTC and routed to your backend agent over low-latency connections. The agent then uses Agents framework integration to relay audio to OpenAI's model via WebSocket. Similarly, speech from OpenAI is streamed back through WebSocket to the agent and relayed to the user via WebRTC.
The Agents framework
The Agents framework provides everything needed to build conversational applications using OpenAI's Realtime API, including:
Support for
Python
and
Node.js
SDKs for nearly every platform
Inbound and outbound calling (using SIP trunks)
WebRTC transport via LiveKit Cloud or self-host OSS
Worker load balancing and request distribution (see
Agent lifecycle
)
LiveKit concepts
The LiveKit Agents framework uses the following concepts:
Room
: a realtime session with participants. The room acts as bridge between your end user and your agent. Each room has a name and is identified by a unique ID.
Participant
: a user or process (i.e. agent) participating in a room.
Agent
: a programmable AI participant in a room.
Track
: audio, video, text, or data published by a user or agent, and subscribed to by other participants in the room.
MultimodalAgent
The framework includes the
MultimodalAgent
class for building speech-to-speech agents that use the OpenAI Realtime API. To learn more about the differences between speech-to-speech and voice pipeline agents, see
Voice agents comparison
.
Try the playground
On this page
Quickstarts
OpenAI Realtime API and LiveKit
How it works
The Agents framework
LiveKit concepts
MultimodalAgent


Content from https://docs.livekit.io/agents/v0/integrations/openai/customize/vad:

On this page
Modifying the VAD parameters
Server-side VAD
Client-side VAD
Copy page
See more page options
Agents 1.0 available for Python
This documentation is for v0.x of the LiveKit Agents framework.
See updated documentation here:
Realtime API integration guide
.
v1.0 for Node.js is coming soon.
VAD is a technique used to determine when a user has finished speaking, or has ended their turn at speaking, and lets the assistant know to respond. Accurate turn detection is key to maintaining a natural conversational flow and avoiding interruptions or awkward pauses. To learn more about turn detection, see
Turn detection
.
Modifying the VAD parameters
By default, OpenAI's Realtime API handles turn detection using VAD on the server side.
You can disable this to manually handle turn detection.
Server-side VAD
Server-side VAD is enabled by default. This means the API determines when the user has started or stopped speaking, and responds automatically. For server-side VAD, you can fine-tune the behavior by adjusting various parameters to suit your application's needs. Here are the parameters you can adjust:
threshold
: Adjusts the sensitivity of the VAD. A lower threshold makes the VAD more sensitive to speech (detects quieter sounds), while a higher threshold makes it less sensitive. The default value is
0.5
.
prefix_padding_ms
: Minimum duration of speech (in milliseconds) required to start a new speech chunk. This helps prevent very short sounds from triggering speech detection.
silence_duration_ms
: Minimum duration of silence (in milliseconds) at the end of speech before ending the speech segment. This ensures brief pauses do not prematurely end a speech segment.
agent.py
Python
Node.js
model
=
openai
.
realtime
.
RealtimeModel
(
voice
=
"alloy"
,
temperature
=
0.8
,
instructions
=
"You are a helpful assistant"
,
turn_detection
=
openai
.
realtime
.
ServerVadOptions
(
threshold
=
0.6
,
prefix_padding_ms
=
200
,
silence_duration_ms
=
500
)
,
)
agent
=
multimodal
.
MultimodalAgent
(
model
=
model
)
agent
.
start
(
ctx
.
room
)
Client-side VAD
Note
This option is currently only available for Python.
If you want to have more control over audio input, you can turn off VAD and implement manual VAD. This is useful
for push-to-talk interfaces where there is an obvious signal a user has started and stopped speaking. When you turn off
VAD, your have to trigger audio responses explicitly.
Usage
To turn off server-side VAD, update the turn detection parameter:
model
=
openai
.
realtime
.
RealtimeModel
(
voice
=
"alloy"
,
temperature
=
0.8
,
instructions
=
"You are a helpful assistant"
,
turn_detection
=
None
,
)
agent
=
multimodal
.
MultimodalAgent
(
model
=
model
)
agent
.
start
(
ctx
.
room
)
To manually generate speech, use the
generate_reply
method:
# When it's time to generate a new response, call generate_reply
agent
.
generate_reply
(
on_duplicate
=
"cancel_existing"
)
On this page
Modifying the VAD parameters
Server-side VAD
Client-side VAD


Content from https://docs.livekit.io/agents/v0/integrations/openai/customize/parameters:

On this page
Parameters
modalities
instructions
voice
turn_detection
temperature
max_output_tokens
Example Initialization
Copy page
See more page options
Agents 1.0 available for Python
This documentation is for v0.x of the LiveKit Agents framework.
See updated documentation here:
Realtime API integration guide
.
v1.0 for Node.js is coming soon.
The
RealtimeModel
class is used to create a realtime conversational AI session. Below are the key parameters that can be passed when initializing the model, with a focus on the
modalities
,
instructions
,
voice
,
turn_detection
,
temperature
, and
max_output_tokens
options.
Parameters
modalities
Type
:
list[api_proto.Modality]
Default
:
["text", "audio"]
Description
: Specifies the input/output modalities supported by the model. This can be either or both of:
"text"
: The model processes text-based input and generates text responses.
"audio"
: The model processes audio input and can generate audio responses.
Example
:
modalities
=
[
"text"
,
"audio"
]
instructions
Type
:
str | None
Default
:
None
Description
: Custom instructions are the 'system prompt' for the model to follow during the conversation. This can be used to guide the behavior of the model or set specific goals.
Example
:
instructions
=
"Please provide responses that are brief and informative."
voice
Type
:
api_proto.Voice
Default
:
"alloy"
Description
: Determines the voice used for audio responses. Some examples of voices include:
"alloy"
"echo"
"shimmer"
Example
:
voice
=
"alloy"
turn_detection
Type
:
api_proto.TurnDetectionType
Default
:
{"type": "server_vad"}
Description
: Controls how the model detects when a speaker has finished talking, which is critical in realtime interactions.
"server_vad"
: OpenAI uses server side Voice Activity Detection (VAD) to detect when the user has stopped speaking. This can be fine-tuned using the following parameters:
threshold
(optional): Float value to control the sensitivity of speech detection.
prefix_padding_ms
(optional): The amount of time (in milliseconds) to pad before the detected speech.
silence_duration_ms
(optional): The amount of silence (in milliseconds) required to consider the speech finished.
Example
:
turn_detection
=
{
"type"
:
"server_vad"
,
"threshold"
:
0.6
,
"prefix_padding_ms"
:
300
,
"silence_duration_ms"
:
500
}
temperature
Type
:
float
Default
:
0.8
Description
: Controls the randomness of the model's output. Higher values (e.g.,
1.0
and above) make the model's output more diverse and creative, while lower values (e.g.,
0.6
) makes it more focused and deterministic.
Example
:
temperature
=
0.7
max_output_tokens
Type
:
int
Default
:
2048
Description
: Limits the maximum number of tokens in the generated output. This helps control the length of the responses from the model, where one token roughly corresponds to one word.
Example
:
max_output_tokens
=
1500
Example Initialization
Here is a full example of how to initialize the
RealtimeModel
with these parameters:
Python
realtime_model
=
RealtimeModel
(
modalities
=
[
"text"
,
"audio"
]
,
instructions
=
"Give brief, concise answers."
,
voice
=
"alloy"
,
turn_detection
=
openai
.
realtime
.
ServerVadOptions
(
threshold
=
0.6
,
prefix_padding_ms
=
200
,
silence_duration_ms
=
500
,
)
,
temperature
=
0.7
,
max_output_tokens
=
1500
,
)
On this page
Parameters
modalities
instructions
voice
turn_detection
temperature
max_output_tokens
Example Initialization

